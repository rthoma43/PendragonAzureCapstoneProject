{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "pendragon-synapse"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"AzureSqlDatabase1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase1'"
		},
		"AzureSqlDatabase2_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase2'"
		},
		"pendragon-synapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'pendragon-synapse-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:pendragon-synapse.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://pendragon.dfs.core.windows.net/"
		},
		"AzureMLService1_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "57cd2ff8-9306-41d0-9cad-c2052a0a8381"
		},
		"AzureMLService1_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "Spring2023-TeamPendragon"
		},
		"CognitiveService1_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "57cd2ff8-9306-41d0-9cad-c2052a0a8381"
		},
		"PendragonKeyVault_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://pendragonkeys.vault.azure.net/"
		},
		"RestService_Next_Token_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://api.twitter.com"
		},
		"RestService_Twitter_7days_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://api.twitter.com"
		},
		"Twitter_RestService_ Aithusa_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://api.twitter.com"
		},
		"Twitter_RestService_AS_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://api.twitter.com"
		},
		"pendragon-synapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://pendragon.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Archive_Analysis_Tables')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Archive Analysis",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "ArchiveAnalysis",
								"type": "NotebookReference"
							},
							"parameters": {
								"Sent_maintable": {
									"value": {
										"value": "@pipeline().parameters.SentimentOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"TopicWords_maintable": {
									"value": {
										"value": "@pipeline().parameters.TopicWordsOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"NamedEntity_maintable": {
									"value": {
										"value": "@pipeline().parameters.NEROutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"Abstract_maintable": {
									"value": {
										"value": "@pipeline().parameters.SummarizationOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"query": {
									"value": {
										"value": "@pipeline().parameters.queryname",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"queryname": {
						"type": "string",
						"defaultValue": "Test001"
					},
					"TopicWordsOutputTable": {
						"type": "string",
						"defaultValue": "Words.NATO_Topic_Words"
					},
					"SentimentOutputTable": {
						"type": "string",
						"defaultValue": "Sent.NATO_Sentiment"
					},
					"NEROutputTable": {
						"type": "string",
						"defaultValue": "NER.NATO_NER"
					},
					"SummarizationOutputTable": {
						"type": "string",
						"defaultValue": "sum.NATO_Abstractive_Sum"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/ArchiveAnalysis')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GET Loop Next Token RT')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Initial GET Past 7 Days into Temp Folder",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete Previous Output Files",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Delete Previous Merged Output files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"paginationRules": {
									"supportRFC5988": "true"
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "RestResourceTwitter7days",
								"type": "DatasetReference",
								"parameters": {
									"query": {
										"value": "@pipeline().parameters.Query",
										"type": "Expression"
									},
									"tweet_fields": {
										"value": "@pipeline().parameters.TweetFields",
										"type": "Expression"
									},
									"max_results": {
										"value": "@pipeline().parameters.MaxResults",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "GET using Next Token",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Initial GET Past 7 Days into Temp Folder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0,pipeline().parameters.Loops)",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Extract Next Token",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "Get Next Token",
											"type": "DataFlowReference",
											"parameters": {},
											"datasetParameters": {
												"source1": {},
												"source2": {},
												"sink1": {},
												"sink2": {}
											}
										},
										"staging": {},
										"integrationRuntime": {
											"referenceName": "WarmIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"traceLevel": "None",
										"cacheSinks": {
											"firstRowOnly": true
										}
									}
								},
								{
									"name": "GET to Temp",
									"type": "Copy",
									"dependsOn": [
										{
											"activity": "Extract Next Token",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "RestSource",
											"httpRequestTimeout": "00:01:40",
											"requestInterval": "00.00:00:00.010",
											"requestMethod": "GET",
											"paginationRules": {
												"supportRFC5988": "true"
											}
										},
										"sink": {
											"type": "JsonSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "JsonWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "RestResource_Next_Token",
											"type": "DatasetReference",
											"parameters": {
												"query": {
													"value": "@pipeline().parameters.Query",
													"type": "Expression"
												},
												"tweet_fields": {
													"value": "@pipeline().parameters.TweetFields",
													"type": "Expression"
												},
												"max_results": {
													"value": "@pipeline().parameters.MaxResults",
													"type": "Expression"
												},
												"next_token": {
													"value": "@activity('Extract Next Token').output.runStatus.output.sink1.value[0].next_token",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "Twitter_RawJSON_Temp",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							]
						}
					},
					{
						"name": "Delete Previous Output Files",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Raw_Twitter_Outputs",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Delete NATO Temp file",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "GET using Next Token",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Merge JSONs and Convert to CSV",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Delete NATO Temp file",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Merge JSON to CSV",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Delete Previous Merged Output files",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Merged_JSON_folder",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Copy data to Dedicated SQL Pool",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete All Existing Rows in NATO_Tweets1 Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlPoolSink",
								"allowCopyCommand": true,
								"copyCommandSettings": {}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "id",
											"type": "Int64"
										},
										"sink": {
											"name": "id",
											"type": "Int64"
										}
									},
									{
										"source": {
											"name": "created_at",
											"type": "DateTime"
										},
										"sink": {
											"name": "created_at",
											"type": "DateTime"
										}
									},
									{
										"source": {
											"name": "text",
											"type": "String"
										},
										"sink": {
											"name": "text",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "lang",
											"type": "String"
										},
										"sink": {
											"name": "lang",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "retweet_count",
											"type": "Int32"
										},
										"sink": {
											"name": "retweet_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "reply_count",
											"type": "Int32"
										},
										"sink": {
											"name": "reply_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "like_count",
											"type": "Int32"
										},
										"sink": {
											"name": "like_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "quote_count",
											"type": "Int32"
										},
										"sink": {
											"name": "quote_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "impression_count",
											"type": "Int32"
										},
										"sink": {
											"name": "impression_count",
											"type": "Int32"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "Merged_output_csv",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DedicatedSqlPoolTable1",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Delete All Existing Rows in NATO_Tweets1 Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Resume SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": "[dbo].[NATO_Tweets1]",
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Resume SQL Pool",
						"description": "Must be \"Resume\" or \"Pause\"",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Merge JSONs and Convert to CSV",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "PauseResumeSQLPool",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"PauseorResume": "Resume"
							}
						}
					},
					{
						"name": "NLP Analysis",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Copy data to Dedicated SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "NLP_Analysis",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"TopicModelingOutputTable": {
									"value": "@pipeline().parameters.TopicOutputTable",
									"type": "Expression"
								},
								"SentimentOutputTable": {
									"value": "@pipeline().parameters.SentimentOutputTable",
									"type": "Expression"
								},
								"SummarizationOutputTable": {
									"value": "@pipeline().parameters.SummarizationOutputTable",
									"type": "Expression"
								},
								"NEROutputTable": {
									"value": "@pipeline().parameters.NEROutputTable",
									"type": "Expression"
								},
								"TopicModelingWordsOutputTable": {
									"value": "@pipeline().parameters.TopicWordsOutputTable",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Pause SQL Pool",
						"description": "Must be \"Resume\" or \"Pause\"",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Execute Archive Analysis Pipeline",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "PauseResumeSQLPool",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"PauseorResume": "Pause"
							}
						}
					},
					{
						"name": "ArchiveTweets",
						"description": "Adds \"query\" and \"pull_datetime\" columns. query value is defined by the pipeline parameter. Pull_datetime is the current timestamp.",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Copy data to Dedicated SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "ArchiveTweets",
								"type": "NotebookReference"
							},
							"parameters": {
								"query": {
									"value": {
										"value": "@pipeline().parameters.QueryName",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolSmall",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Execute Archive Analysis Pipeline",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "NLP Analysis",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Archive_Analysis_Tables",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"queryname": {
									"value": "@pipeline().parameters.QueryName",
									"type": "Expression"
								},
								"TopicWordsOutputTable": {
									"value": "@pipeline().parameters.TopicWordsOutputTable",
									"type": "Expression"
								},
								"SentimentOutputTable": {
									"value": "@pipeline().parameters.SentimentOutputTable",
									"type": "Expression"
								},
								"NEROutputTable": {
									"value": "@pipeline().parameters.NEROutputTable",
									"type": "Expression"
								},
								"SummarizationOutputTable": {
									"value": "@pipeline().parameters.SummarizationOutputTable",
									"type": "Expression"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"QueryName": {
						"type": "string",
						"defaultValue": "NATO001"
					},
					"Query": {
						"type": "string",
						"defaultValue": "(\"NATO\"OR\"North Atlantic Treaty Organization\"OR%23NATO) lang:en -is:retweet -is:reply"
					},
					"TweetFields": {
						"type": "string",
						"defaultValue": "id,text,created_at,lang,public_metrics"
					},
					"MaxResults": {
						"type": "string",
						"defaultValue": "100"
					},
					"Loops": {
						"type": "int",
						"defaultValue": 3
					},
					"TopicWordsOutputTable": {
						"type": "string",
						"defaultValue": "Words.NATO_Topic_Words"
					},
					"SentimentOutputTable": {
						"type": "string",
						"defaultValue": "Sent.NATO_Sentiment"
					},
					"NEROutputTable": {
						"type": "string",
						"defaultValue": "NER.NATO_NER"
					},
					"SummarizationOutputTable": {
						"type": "string",
						"defaultValue": "sum.NATO_Abstractive_Sum"
					},
					"TopicOutputTable": {
						"type": "string",
						"defaultValue": "Topic.NATO_Topics"
					}
				},
				"variables": {
					"Query": {
						"type": "String"
					},
					"max_results": {
						"type": "String"
					},
					"Tweet Fields": {
						"type": "String"
					},
					"Number of Tweets to Pull": {
						"type": "String"
					},
					"loops": {
						"type": "String"
					},
					"until": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/RestResourceTwitter7days')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_RawJSON_Temp')]",
				"[concat(variables('workspaceId'), '/datasets/Raw_Twitter_Outputs')]",
				"[concat(variables('workspaceId'), '/dataflows/Merge JSON to CSV')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/WarmIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_JSON_folder')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_output_csv')]",
				"[concat(variables('workspaceId'), '/datasets/DedicatedSqlPoolTable1')]",
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]",
				"[concat(variables('workspaceId'), '/pipelines/PauseResumeSQLPool')]",
				"[concat(variables('workspaceId'), '/pipelines/NLP_Analysis')]",
				"[concat(variables('workspaceId'), '/notebooks/ArchiveTweets')]",
				"[concat(variables('workspaceId'), '/bigDataPools/SparkPoolSmall')]",
				"[concat(variables('workspaceId'), '/pipelines/Archive_Analysis_Tables')]",
				"[concat(variables('workspaceId'), '/dataflows/Get Next Token')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Next_Token')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GET Loop Next Token RT_copy1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Initial GET Past 7 Days into Temp Folder",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete Previous Output Files",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Delete Previous Merged Output files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"paginationRules": {
									"supportRFC5988": "true"
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "RestResourceTwitter7days",
								"type": "DatasetReference",
								"parameters": {
									"query": {
										"value": "@pipeline().parameters.Query",
										"type": "Expression"
									},
									"tweet_fields": {
										"value": "@pipeline().parameters.TweetFields",
										"type": "Expression"
									},
									"max_results": {
										"value": "@pipeline().parameters.MaxResults",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Delete Previous Output Files",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Raw_Twitter_Outputs",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Delete NATO Temp file",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Pull data until there is no next_token",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Merge JSONs and Convert to CSV",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Delete NATO Temp file",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Merge JSON to CSV",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Delete Previous Merged Output files",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Merged_JSON_folder",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Copy data to Dedicated SQL Pool",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete All Existing Rows in NATO_Tweets1 Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlPoolSink",
								"allowCopyCommand": true,
								"copyCommandSettings": {}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "id",
											"type": "Int64"
										},
										"sink": {
											"name": "id",
											"type": "Int64"
										}
									},
									{
										"source": {
											"name": "created_at",
											"type": "DateTime"
										},
										"sink": {
											"name": "created_at",
											"type": "DateTime"
										}
									},
									{
										"source": {
											"name": "text",
											"type": "String"
										},
										"sink": {
											"name": "text",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "lang",
											"type": "String"
										},
										"sink": {
											"name": "lang",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "retweet_count",
											"type": "Int32"
										},
										"sink": {
											"name": "retweet_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "reply_count",
											"type": "Int32"
										},
										"sink": {
											"name": "reply_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "like_count",
											"type": "Int32"
										},
										"sink": {
											"name": "like_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "quote_count",
											"type": "Int32"
										},
										"sink": {
											"name": "quote_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "impression_count",
											"type": "Int32"
										},
										"sink": {
											"name": "impression_count",
											"type": "Int32"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "Merged_output_csv",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DedicatedSqlPoolTable1",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Delete All Existing Rows in NATO_Tweets1 Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Resume SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": "[dbo].[NATO_Tweets1]",
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Resume SQL Pool",
						"description": "Must be \"Resume\" or \"Pause\"",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Merge JSONs and Convert to CSV",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "PauseResumeSQLPool",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"PauseorResume": "Resume"
							}
						}
					},
					{
						"name": "NLP Analysis",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Copy data to Dedicated SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "NLP_Analysis",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"TopicModelingOutputTable": {
									"value": "@pipeline().parameters.TopicOutputTable",
									"type": "Expression"
								},
								"SentimentOutputTable": {
									"value": "@pipeline().parameters.SentimentOutputTable",
									"type": "Expression"
								},
								"SummarizationOutputTable": {
									"value": "@pipeline().parameters.SummarizationOutputTable",
									"type": "Expression"
								},
								"NEROutputTable": {
									"value": "@pipeline().parameters.NEROutputTable",
									"type": "Expression"
								},
								"TopicModelingWordsOutputTable": {
									"value": "@pipeline().parameters.TopicWordsOutputTable",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Pause SQL Pool",
						"description": "Must be \"Resume\" or \"Pause\"",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Execute Archive Analysis Pipeline",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "PauseResumeSQLPool",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"PauseorResume": "Pause"
							}
						}
					},
					{
						"name": "ArchiveTweets",
						"description": "Adds \"query\" and \"pull_datetime\" columns. query value is defined by the pipeline parameter. Pull_datetime is the current timestamp.",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Copy data to Dedicated SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "ArchiveTweets",
								"type": "NotebookReference"
							},
							"parameters": {
								"query": {
									"value": {
										"value": "@pipeline().parameters.QueryName",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolSmall",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Execute Archive Analysis Pipeline",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "NLP Analysis",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Archive_Analysis_Tables",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"queryname": {
									"value": "@pipeline().parameters.QueryName",
									"type": "Expression"
								},
								"TopicWordsOutputTable": {
									"value": "@pipeline().parameters.TopicWordsOutputTable",
									"type": "Expression"
								},
								"SentimentOutputTable": {
									"value": "@pipeline().parameters.SentimentOutputTable",
									"type": "Expression"
								},
								"NEROutputTable": {
									"value": "@pipeline().parameters.NEROutputTable",
									"type": "Expression"
								},
								"SummarizationOutputTable": {
									"value": "@pipeline().parameters.SummarizationOutputTable",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Pull data until there is no next_token",
						"type": "Until",
						"dependsOn": [
							{
								"activity": "Initial GET Past 7 Days into Temp Folder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@not(equals(variables('until'), '000'))",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Extract next_token",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "Get Next Token",
											"type": "DataFlowReference",
											"parameters": {},
											"datasetParameters": {
												"source1": {},
												"source2": {},
												"sink1": {},
												"sink2": {}
											}
										},
										"staging": {},
										"integrationRuntime": {
											"referenceName": "WarmIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"traceLevel": "None",
										"cacheSinks": {
											"firstRowOnly": true
										}
									}
								},
								{
									"name": "GET to Temp",
									"type": "Copy",
									"dependsOn": [
										{
											"activity": "Set Until Value",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "RestSource",
											"httpRequestTimeout": "00:01:40",
											"requestInterval": "00.00:00:00.010",
											"requestMethod": "GET",
											"paginationRules": {
												"supportRFC5988": "true"
											}
										},
										"sink": {
											"type": "JsonSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "JsonWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "RestResource_Next_Token",
											"type": "DatasetReference",
											"parameters": {
												"query": {
													"value": "@pipeline().parameters.Query",
													"type": "Expression"
												},
												"tweet_fields": {
													"value": "@pipeline().parameters.TweetFields",
													"type": "Expression"
												},
												"max_results": {
													"value": "@pipeline().parameters.MaxResults",
													"type": "Expression"
												},
												"next_token": {
													"value": "@activity('Extract next_token').output.runStatus.output.sink1.value[0].next_token",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "Twitter_RawJSON_Temp",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								},
								{
									"name": "Set Until Value",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "Extract next_token",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "until",
										"value": {
											"value": "@activity('Extract next_token').output.runStatus.output.sink1.value[0].next_token",
											"type": "Expression"
										}
									}
								}
							],
							"timeout": "0.12:00:00"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"QueryName": {
						"type": "string",
						"defaultValue": "NATOTest001"
					},
					"Query": {
						"type": "string",
						"defaultValue": "(\"NATO\"OR\"North Atlantic Treaty Organization\"OR%23NATO) lang:en -is:retweet -is:reply"
					},
					"TweetFields": {
						"type": "string",
						"defaultValue": "id,text,created_at,lang,public_metrics"
					},
					"MaxResults": {
						"type": "string",
						"defaultValue": "100"
					},
					"TopicWordsOutputTable": {
						"type": "string",
						"defaultValue": "Words.NATO_Topic_Words"
					},
					"SentimentOutputTable": {
						"type": "string",
						"defaultValue": "Sent.NATO_Sentiment"
					},
					"NEROutputTable": {
						"type": "string",
						"defaultValue": "NER.NATO_NER"
					},
					"SummarizationOutputTable": {
						"type": "string",
						"defaultValue": "sum.NATO_Abstractive_Sum"
					},
					"TopicOutputTable": {
						"type": "string",
						"defaultValue": "Topic.NATO_Topics"
					}
				},
				"variables": {
					"until": {
						"type": "String",
						"defaultValue": "abcd"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/RestResourceTwitter7days')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_RawJSON_Temp')]",
				"[concat(variables('workspaceId'), '/datasets/Raw_Twitter_Outputs')]",
				"[concat(variables('workspaceId'), '/dataflows/Merge JSON to CSV')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/WarmIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_JSON_folder')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_output_csv')]",
				"[concat(variables('workspaceId'), '/datasets/DedicatedSqlPoolTable1')]",
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]",
				"[concat(variables('workspaceId'), '/pipelines/PauseResumeSQLPool')]",
				"[concat(variables('workspaceId'), '/pipelines/NLP_Analysis')]",
				"[concat(variables('workspaceId'), '/notebooks/ArchiveTweets')]",
				"[concat(variables('workspaceId'), '/bigDataPools/SparkPoolSmall')]",
				"[concat(variables('workspaceId'), '/pipelines/Archive_Analysis_Tables')]",
				"[concat(variables('workspaceId'), '/dataflows/Get Next Token')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Next_Token')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GET Until Next Token')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Query",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "Query",
							"value": "\"NATO\" lang:en -is:retweet -is:reply"
						}
					},
					{
						"name": "Max Results",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "max_results",
							"value": "100"
						}
					},
					{
						"name": "Tweet Fields",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "Tweet Fields",
							"value": "id,text,created_at,lang,public_metrics"
						}
					},
					{
						"name": "Delete Previous Output Files",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Query",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Max Results",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Tweet Fields",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Raw_Aithusa_Outputs",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Delete Previous Merged Output files",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Query",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Merged_Aithusa_JSON_folder",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Until GET Tweets",
						"type": "Until",
						"dependsOn": [
							{
								"activity": "Set variable1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@variables('stopUntil')",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "If Condition1",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "Extract Next Token Aithusa",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@equals(activity('Extract Next Token Aithusa').output.runStatus.output.sink1.value[0].next_token, '\\n')",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Set stopUntil true",
												"type": "SetVariable",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"variableName": "stopUntil",
													"value": true
												}
											}
										]
									}
								},
								{
									"name": "Extract Next Token Aithusa",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "Get Next Token for Aithusa",
											"type": "DataFlowReference",
											"parameters": {},
											"datasetParameters": {
												"source1": {},
												"source2": {},
												"sink1": {},
												"sink2": {}
											}
										},
										"staging": {},
										"integrationRuntime": {
											"referenceName": "WarmIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"traceLevel": "None",
										"cacheSinks": {
											"firstRowOnly": true
										}
									}
								},
								{
									"name": "If specific date with token",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "Delete Temp File_copy2",
											"dependencyConditions": [
												"Completed"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@variables('specific_date')",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "GET Past 7 Days with token",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "RestSource",
														"httpRequestTimeout": "00:01:40",
														"requestInterval": "00.00:00:00.010",
														"requestMethod": "GET",
														"paginationRules": {
															"supportRFC5988": "true"
														}
													},
													"sink": {
														"type": "JsonSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "JsonWriteSettings"
														}
													},
													"enableStaging": false
												},
												"inputs": [
													{
														"referenceName": "RestResource_Aithusa_7days_NextToken",
														"type": "DatasetReference",
														"parameters": {
															"query": {
																"value": "@variables('Query')",
																"type": "Expression"
															},
															"tweet_fields": {
																"value": "@variables('Tweet Fields')",
																"type": "Expression"
															},
															"max_results": {
																"value": "@variables('max_results')",
																"type": "Expression"
															},
															"next_token": {
																"value": "@activity('Extract Next Token Aithusa').output.runStatus.output.sink1.value[0].next_token",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "json_Aithusa",
														"type": "DatasetReference",
														"parameters": {}
													}
												]
											},
											{
												"name": "Set variable3",
												"type": "SetVariable",
												"dependsOn": [
													{
														"activity": "GET Past 7 Days with token",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"userProperties": [],
												"typeProperties": {
													"variableName": "stopUntil",
													"value": true
												}
											}
										],
										"ifTrueActivities": [
											{
												"name": "GET with specifc date with token",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "RestSource",
														"httpRequestTimeout": "00:01:40",
														"requestInterval": "00.00:00:00.010",
														"requestMethod": "GET",
														"paginationRules": {
															"supportRFC5988": "true"
														}
													},
													"sink": {
														"type": "JsonSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "JsonWriteSettings"
														}
													},
													"enableStaging": false
												},
												"inputs": [
													{
														"referenceName": "RestResource_Aithusa_withToken",
														"type": "DatasetReference",
														"parameters": {
															"Query": {
																"value": "@variables('Query')",
																"type": "Expression"
															},
															"max_results": {
																"value": "@variables('max_results')",
																"type": "Expression"
															},
															"tweet_fields": {
																"value": "@variables('Tweet Fields')",
																"type": "Expression"
															},
															"start_year": {
																"value": "@variables('start_year')",
																"type": "Expression"
															},
															"start_month": {
																"value": "@variables('start_month')",
																"type": "Expression"
															},
															"start_day": {
																"value": "@variables('start_day')",
																"type": "Expression"
															},
															"start_hour": {
																"value": "@variables('start_hour')",
																"type": "Expression"
															},
															"start_minute": {
																"value": "@variables('start_minute')",
																"type": "Expression"
															},
															"start_second": "00",
															"end_year": {
																"value": "@variables('end_year')",
																"type": "Expression"
															},
															"end_month": {
																"value": "@variables('end_month')",
																"type": "Expression"
															},
															"end_day": {
																"value": "@variables('end_day')",
																"type": "Expression"
															},
															"end_hour": {
																"value": "@variables('end_hour')",
																"type": "Expression"
															},
															"end_minute": {
																"value": "@variables('end_minute')",
																"type": "Expression"
															},
															"end_second": "00",
															"next_token": {
																"value": "@activity('Extract Next Token Aithusa').output.runStatus.output.sink1.value[0].next_token",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "json_Aithusa",
														"type": "DatasetReference",
														"parameters": {}
													}
												]
											},
											{
												"name": "Set variable2",
												"type": "SetVariable",
												"dependsOn": [
													{
														"activity": "GET with specifc date with token",
														"dependencyConditions": [
															"Failed"
														]
													}
												],
												"userProperties": [],
												"typeProperties": {
													"variableName": "stopUntil",
													"value": true
												}
											}
										]
									}
								},
								{
									"name": "Delete Temp File_copy2",
									"type": "Delete",
									"dependsOn": [
										{
											"activity": "If Condition1",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataset": {
											"referenceName": "json_Aithusa",
											"type": "DatasetReference",
											"parameters": {}
										},
										"enableLogging": false,
										"storeSettings": {
											"type": "AzureBlobFSReadSettings",
											"recursive": true,
											"enablePartitionDiscovery": false
										}
									}
								},
								{
									"name": "Set variable4",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "Extract Next Token Aithusa",
											"dependencyConditions": [
												"Failed"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "stopUntil",
										"value": true
									}
								}
							],
							"timeout": "0.12:00:00"
						}
					},
					{
						"name": "Set variable1",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "If specific date",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "stopUntil",
							"value": false
						}
					},
					{
						"name": "Delete NATO Temp file",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Until GET Tweets",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "json_Aithusa",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Merge JSONs and Convert to CSV",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Delete NATO Temp file",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Merge JSONs and Convert to CSV for Aithusa",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "End year",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_year",
							"value": "2023"
						}
					},
					{
						"name": "End month",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "End year",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_month",
							"value": "04"
						}
					},
					{
						"name": "End day",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "End month",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_day",
							"value": "07"
						}
					},
					{
						"name": "End hour",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "End day",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_hour",
							"value": "08"
						}
					},
					{
						"name": "End minute",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "End hour",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_minute",
							"value": "00"
						}
					},
					{
						"name": "Start year",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_year",
							"value": "2023"
						}
					},
					{
						"name": "Start month",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start year",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_month",
							"value": "04"
						}
					},
					{
						"name": "Start day",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start month",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_day",
							"value": "07"
						}
					},
					{
						"name": "Start hour",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start day",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_hour",
							"value": "00"
						}
					},
					{
						"name": "Start minute",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start hour",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_minute",
							"value": "00"
						}
					},
					{
						"name": "Use date range",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start minute",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "End minute",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "specific_date",
							"value": true
						}
					},
					{
						"name": "If specific date",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Delete Previous Merged Output files",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Delete Previous Output Files",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Use date range",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@variables('specific_date')",
								"type": "Expression"
							},
							"ifFalseActivities": [
								{
									"name": "Initial GET Past 7 Days",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "RestSource",
											"httpRequestTimeout": "00:01:40",
											"requestInterval": "00.00:00:00.010",
											"requestMethod": "GET",
											"paginationRules": {
												"supportRFC5988": "true"
											}
										},
										"sink": {
											"type": "JsonSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "JsonWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "RestResource_Aithusa_7days",
											"type": "DatasetReference",
											"parameters": {
												"query": {
													"value": "@variables('Query')",
													"type": "Expression"
												},
												"tweet_fields": {
													"value": "@variables('Tweet Fields')",
													"type": "Expression"
												},
												"max_results": {
													"value": "@variables('max_results')",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "json_Aithusa",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							],
							"ifTrueActivities": [
								{
									"name": "Initial GET with specifc date",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "RestSource",
											"httpRequestTimeout": "00:01:40",
											"requestInterval": "00.00:00:00.010",
											"requestMethod": "GET",
											"paginationRules": {
												"supportRFC5988": "true"
											}
										},
										"sink": {
											"type": "JsonSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "JsonWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "RestResource_Aithusa",
											"type": "DatasetReference",
											"parameters": {
												"Query": {
													"value": "@variables('Query')",
													"type": "Expression"
												},
												"max_results": {
													"value": "@variables('max_results')",
													"type": "Expression"
												},
												"tweet_fields": {
													"value": "@variables('Tweet Fields')",
													"type": "Expression"
												},
												"start_year": {
													"value": "@variables('start_year')",
													"type": "Expression"
												},
												"start_month": {
													"value": "@variables('start_month')",
													"type": "Expression"
												},
												"start_day": {
													"value": "@variables('start_day')",
													"type": "Expression"
												},
												"start_hour": {
													"value": "@variables('start_hour')",
													"type": "Expression"
												},
												"start_minute": {
													"value": "@variables('start_minute')",
													"type": "Expression"
												},
												"start_second": "00",
												"end_year": {
													"value": "@variables('end_year')",
													"type": "Expression"
												},
												"end_month": {
													"value": "@variables('end_month')",
													"type": "Expression"
												},
												"end_day": {
													"value": "@variables('end_day')",
													"type": "Expression"
												},
												"end_hour": {
													"value": "@variables('end_hour')",
													"type": "Expression"
												},
												"end_minute": {
													"value": "@variables('end_minute')",
													"type": "Expression"
												},
												"end_second": "00"
											}
										}
									],
									"outputs": [
										{
											"referenceName": "json_Aithusa",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							]
						}
					},
					{
						"name": "Copy data to Dedicated SQL Pool",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete All Existing Rows in NATO_Tweets1 Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlPoolSink",
								"allowCopyCommand": true,
								"copyCommandSettings": {}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "id",
											"type": "Int64"
										},
										"sink": {
											"name": "id",
											"type": "Int64"
										}
									},
									{
										"source": {
											"name": "created_at",
											"type": "DateTime"
										},
										"sink": {
											"name": "created_at",
											"type": "DateTime"
										}
									},
									{
										"source": {
											"name": "text",
											"type": "String"
										},
										"sink": {
											"name": "text",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "lang",
											"type": "String"
										},
										"sink": {
											"name": "lang",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "retweet_count",
											"type": "Int32"
										},
										"sink": {
											"name": "retweet_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "reply_count",
											"type": "Int32"
										},
										"sink": {
											"name": "reply_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "like_count",
											"type": "Int32"
										},
										"sink": {
											"name": "like_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "quote_count",
											"type": "Int32"
										},
										"sink": {
											"name": "quote_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "impression_count",
											"type": "Int32"
										},
										"sink": {
											"name": "impression_count",
											"type": "Int32"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "Merged_output_csv_Aithusa",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DedicatedSqlPoolTable1_Aithusa",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Turn on SQL Pool",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Merge JSONs and Convert to CSV",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "PauseResumeSQLPool",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"PauseorResume": "resume"
							}
						}
					},
					{
						"name": "Turn off SQL Pool",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "NLP Analysis",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "ArchiveTweets",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "PauseResumeSQLPool",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"PauseorResume": "pause"
							}
						}
					},
					{
						"name": "Delete All Existing Rows in NATO_Tweets1 Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Turn on SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": "[dbo].[NATO_Tweets1_Aithusa]",
									"type": "String"
								}
							}
						}
					},
					{
						"name": "NLP Analysis",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Copy data to Dedicated SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "NLP_Analysis",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"TopicModelingOutputTable": {
									"value": "@concat('Topic.',pipeline().parameters.QueryName,'_Topics')",
									"type": "Expression"
								},
								"SentimentOutputTable": {
									"value": "@concat('Sent.',pipeline().parameters.QueryName,'_Sentiment')",
									"type": "Expression"
								},
								"SummarizationOutputTable": {
									"value": "@concat('sum.',pipeline().parameters.QueryName,'_Abstractive_Sum')",
									"type": "Expression"
								},
								"NEROutputTable": {
									"value": "@concat('NER.',pipeline().parameters.QueryName,'_NER')",
									"type": "Expression"
								},
								"TopicModelingWordsOutputTable": {
									"value": "@concat('Words.',pipeline().parameters.QueryName,'_Topic_Words')",
									"type": "Expression"
								},
								"TopicModelingTableSQL": {
									"value": "@concat('[Topic].[',pipeline().parameters.QueryName,'_Topics]')",
									"type": "Expression"
								},
								"TopicModelingWordsTableSQL": {
									"value": "@concat('[Words].[',pipeline().parameters.QueryName,'_Topic_Words]')",
									"type": "Expression"
								},
								"SentimentTableSQL": {
									"value": "@concat('[Sent].[',pipeline().parameters.QueryName,'_Sentiment]')",
									"type": "Expression"
								},
								"SummarizationTableSQL": {
									"value": "@concat('[sum].[',pipeline().parameters.QueryName,'_Abstractive_Sum]')",
									"type": "Expression"
								},
								"NERTableSQL": {
									"value": "@concat('[NER].[',pipeline().parameters.QueryName,'_NER]')",
									"type": "Expression"
								},
								"TableName": {
									"value": "@concat('dbo.NATO_Tweets1_',pipeline().parameters.QueryName)",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "ArchiveTweets",
						"description": "Adds \"query\" and \"pull_datetime\" columns. query value is defined by the pipeline parameter. Pull_datetime is the current timestamp.",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Copy data to Dedicated SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "ArchiveTweets",
								"type": "NotebookReference"
							},
							"parameters": {
								"query": {
									"value": {
										"value": "@pipeline().parameters.QueryName",
										"type": "Expression"
									},
									"type": "string"
								},
								"maintable": {
									"value": "dbo.NATO_Tweets1_Aithusa",
									"type": "string"
								},
								"archivetable": {
									"value": {
										"value": "@pipeline().parameters.TweetArchive",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolSmall",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"QueryName": {
						"type": "string",
						"defaultValue": "Aithusa"
					},
					"TweetArchive": {
						"type": "string",
						"defaultValue": "Aithusa_Archive_Tweets"
					}
				},
				"variables": {
					"Query": {
						"type": "String"
					},
					"max_results": {
						"type": "String"
					},
					"Tweet Fields": {
						"type": "String"
					},
					"Number of Tweets to Pull": {
						"type": "String"
					},
					"next_token": {
						"type": "String"
					},
					"stopUntil": {
						"type": "Boolean"
					},
					"specific_date": {
						"type": "Boolean"
					},
					"end_year": {
						"type": "String"
					},
					"end_month": {
						"type": "String"
					},
					"end_day": {
						"type": "String"
					},
					"end_hour": {
						"type": "String"
					},
					"end_minute": {
						"type": "String"
					},
					"start_year": {
						"type": "String"
					},
					"start_month": {
						"type": "String"
					},
					"start_day": {
						"type": "String"
					},
					"start_hour": {
						"type": "String"
					},
					"start_minute": {
						"type": "String"
					},
					"table": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Raw_Aithusa_Outputs')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_Aithusa_JSON_folder')]",
				"[concat(variables('workspaceId'), '/datasets/json_Aithusa')]",
				"[concat(variables('workspaceId'), '/dataflows/Merge JSONs and Convert to CSV for Aithusa')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/WarmIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_output_csv_Aithusa')]",
				"[concat(variables('workspaceId'), '/datasets/DedicatedSqlPoolTable1_Aithusa')]",
				"[concat(variables('workspaceId'), '/pipelines/PauseResumeSQLPool')]",
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]",
				"[concat(variables('workspaceId'), '/pipelines/NLP_Analysis')]",
				"[concat(variables('workspaceId'), '/notebooks/ArchiveTweets')]",
				"[concat(variables('workspaceId'), '/bigDataPools/SparkPoolSmall')]",
				"[concat(variables('workspaceId'), '/dataflows/Get Next Token for Aithusa')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa_7days')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa_7days_NextToken')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa_withToken')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LookUp_LastModified')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get Metadata1",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Individual_NATO_Folder",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "DelimitedTextReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Metadata1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Metadata1').output.childitems",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Get Metadata2",
									"type": "GetMetadata",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataset": {
											"referenceName": "inputFolder_file_Dyn",
											"type": "DatasetReference",
											"parameters": {
												"FileName": {
													"value": "@item().name",
													"type": "Expression"
												}
											}
										},
										"fieldList": [
											"itemName",
											"lastModified"
										],
										"storeSettings": {
											"type": "AzureBlobFSReadSettings",
											"recursive": true,
											"enablePartitionDiscovery": false
										},
										"formatSettings": {
											"type": "DelimitedTextReadSettings"
										}
									}
								},
								{
									"name": "If Condition1",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "Get Metadata2",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@greater(formatDateTime(activity('Get Metadata2').output.lastModified,'yyyyMMddHHmmss'), formatDateTime(variables('PreviousModifiedDate'),'yyyyMMddHHmmss'))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Set variable1",
												"type": "SetVariable",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"variableName": "latestFileName",
													"value": {
														"value": "@activity('Get Metadata2').output.itemName",
														"type": "Expression"
													}
												}
											}
										]
									}
								},
								{
									"name": "Set variable2",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "If Condition1",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "PreviousModifiedDate",
										"value": {
											"value": "@activity('Get Metadata2').output.lastModified",
											"type": "Expression"
										}
									}
								}
							]
						}
					},
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "ForEach1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".csv"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "inputFolder_file_Dyn",
								"type": "DatasetReference",
								"parameters": {
									"FileName": {
										"value": "@variables('latestFileName')",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Lookup_Output",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Copy data1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Find Last ID",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "None",
							"cacheSinks": {
								"firstRowOnly": true
							}
						}
					},
					{
						"name": "Twitter NATO mentions",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Data flow1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"paginationRules": {
									"supportRFC5988": "true"
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "RestResource_Twitter2",
								"type": "DatasetReference",
								"parameters": {
									"Query": "(\"NATO\"OR\"North Atlantic Treaty Organization\"OR%23NATO) lang:en -is:retweet -is:reply",
									"max_results": "100",
									"tweet_fields": "id,text,created_at,lang,public_metrics",
									"until_id": {
										"value": "@activity('Data flow1').output.runStatus.output.sink1.value[0].id\n",
										"type": "Expression"
									},
									"sort_order": "recency"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Twitter_test_rt_Json",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Data flow2",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Twitter NATO mentions",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Twitter_Parse_JSON_to_CSV",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"latestFileName": {
						"type": "String"
					},
					"PreviousModifiedDate": {
						"type": "String",
						"defaultValue": "1990-01-01T05:12:22Z"
					},
					"last_id": {
						"type": "String"
					},
					"Counter": {
						"type": "String"
					},
					"next_token": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Individual_NATO_Folder')]",
				"[concat(variables('workspaceId'), '/datasets/inputFolder_file_Dyn')]",
				"[concat(variables('workspaceId'), '/datasets/Lookup_Output')]",
				"[concat(variables('workspaceId'), '/dataflows/Find Last ID')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/WarmIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Twitter2')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_test_rt_Json')]",
				"[concat(variables('workspaceId'), '/dataflows/Twitter_Parse_JSON_to_CSV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NLP_Analysis')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Delete all from Topics Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Perform Topic Modeling LDA",
						"description": "Outputs to [Topic].[_Topics]",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from Topics Table",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Delete all from Topic Words Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "TopicModeling_LDA_Sklearn_UsingParms",
								"type": "NotebookReference"
							},
							"parameters": {
								"topictable": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"wordtable": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingWordsOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"maintable": {
									"value": {
										"value": "@pipeline().parameters.TableName",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolSmall",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from NATO Sentiment Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Perform Topic Modeling LDA",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.SentimentTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Perform Sentiment Analysis",
						"description": "Outputs to [Sent].[NATO_Sent]",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from NATO Sentiment Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "CognitiveServices_Sentiment_UsingParms",
								"type": "NotebookReference"
							},
							"parameters": {
								"table": {
									"value": {
										"value": "@pipeline().parameters.SentimentOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"topictable": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingOutputTable",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from NATO Summary Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Perform Sentiment Analysis",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.SummarizationTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Perform Summarization",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from NATO Summary Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "CogntiveServices_Summarization_UsingParms",
								"type": "NotebookReference"
							},
							"parameters": {
								"table": {
									"value": {
										"value": "@pipeline().parameters.SummarizationOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"senttable": {
									"value": {
										"value": "@pipeline().parameters.SentimentOutputTable",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from NATO NER Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Perform Sentiment Analysis",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.NERTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Perform Named Entity Recognition",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from NATO NER Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "CognitiveServices_NamedEntityRecog_UsingParms",
								"type": "NotebookReference"
							},
							"parameters": {
								"table": {
									"value": {
										"value": "@pipeline().parameters.NEROutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"senttable": {
									"value": {
										"value": "@pipeline().parameters.SentimentOutputTable",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolSmall",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from Topic Words Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingWordsTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"TopicModelingOutputTable": {
						"type": "string",
						"defaultValue": "Topic.NATO_Topics"
					},
					"SentimentOutputTable": {
						"type": "string",
						"defaultValue": "Sent.NATO_Sentiment"
					},
					"SummarizationOutputTable": {
						"type": "string",
						"defaultValue": "sum.NATO_Abstractive_Sum"
					},
					"NEROutputTable": {
						"type": "string",
						"defaultValue": "NER.NATO_NER"
					},
					"TopicModelingWordsOutputTable": {
						"type": "string",
						"defaultValue": "Words.NATO_Topic_Words"
					},
					"TopicModelingTableSQL": {
						"type": "string",
						"defaultValue": "[Topic].[NATO_Topics]"
					},
					"TopicModelingWordsTableSQL": {
						"type": "string",
						"defaultValue": "[Words].[NATO_Topic_Words]"
					},
					"SentimentTableSQL": {
						"type": "string",
						"defaultValue": "[Sent].[NATO_Sentiment]"
					},
					"SummarizationTableSQL": {
						"type": "string",
						"defaultValue": "[sum].[NATO_Abstractive_Sum]"
					},
					"NERTableSQL": {
						"type": "string",
						"defaultValue": "[NER].[NATO_NER]"
					},
					"TableName": {
						"type": "string",
						"defaultValue": "dbo.NATO_Tweets1"
					}
				},
				"variables": {
					"TopicModelingOutputTable": {
						"type": "String"
					},
					"SentimentOutputTable": {
						"type": "String"
					},
					"SummarizationOutputTable": {
						"type": "String"
					},
					"NEROutputTable": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]",
				"[concat(variables('workspaceId'), '/notebooks/TopicModeling_LDA_Sklearn_UsingParms')]",
				"[concat(variables('workspaceId'), '/bigDataPools/SparkPoolSmall')]",
				"[concat(variables('workspaceId'), '/notebooks/CognitiveServices_Sentiment_UsingParms')]",
				"[concat(variables('workspaceId'), '/notebooks/CogntiveServices_Summarization_UsingParms')]",
				"[concat(variables('workspaceId'), '/notebooks/CognitiveServices_NamedEntityRecog_UsingParms')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NLP_Analysis_copy1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Delete all from Topics Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Perform Topic Modeling LDA",
						"description": "Outputs to [Topic].[_Topics]",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from Topics Table",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Delete all from Topic Words Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "TopicModeling_LDA_Sklearn_UsingParms",
								"type": "NotebookReference"
							},
							"parameters": {
								"topictable": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"wordtable": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingWordsOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"maintable": {
									"value": {
										"value": "@pipeline().parameters.TableName",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolSmall",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from NATO Sentiment Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Perform Topic Modeling LDA",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.SentimentTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Perform Sentiment Analysis",
						"description": "Outputs to [Sent].[NATO_Sent]",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from NATO Sentiment Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "CognitiveServices_Sentiment_UsingParms",
								"type": "NotebookReference"
							},
							"parameters": {
								"table": {
									"value": {
										"value": "@pipeline().parameters.SentimentOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"topictable": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingOutputTable",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from NATO Summary Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Perform Sentiment Analysis",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.SummarizationTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Perform Summarization",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from NATO Summary Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "CogntiveServices_Summarization_UsingParms",
								"type": "NotebookReference"
							},
							"parameters": {
								"table": {
									"value": {
										"value": "@pipeline().parameters.SummarizationOutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"senttable": {
									"value": {
										"value": "@pipeline().parameters.SentimentOutputTable",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from NATO NER Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Perform Sentiment Analysis",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.NERTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					},
					{
						"name": "Perform Named Entity Recognition",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from NATO NER Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "CognitiveServices_NamedEntityRecog_UsingParms",
								"type": "NotebookReference"
							},
							"parameters": {
								"table": {
									"value": {
										"value": "@pipeline().parameters.NEROutputTable",
										"type": "Expression"
									},
									"type": "string"
								},
								"senttable": {
									"value": {
										"value": "@pipeline().parameters.SentimentOutputTable",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolSmall",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from Topic Words Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[delete_all_rows]",
							"storedProcedureParameters": {
								"table_name": {
									"value": {
										"value": "@pipeline().parameters.TopicModelingWordsTableSQL",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"TopicModelingOutputTable": {
						"type": "string",
						"defaultValue": "Topic.NATO_Topics"
					},
					"SentimentOutputTable": {
						"type": "string",
						"defaultValue": "Sent.NATO_Sentiment"
					},
					"SummarizationOutputTable": {
						"type": "string",
						"defaultValue": "sum.NATO_Abstractive_Sum"
					},
					"NEROutputTable": {
						"type": "string",
						"defaultValue": "NER.NATO_NER"
					},
					"TopicModelingWordsOutputTable": {
						"type": "string",
						"defaultValue": "Words.NATO_Topic_Words"
					},
					"TopicModelingTableSQL": {
						"type": "string",
						"defaultValue": "[Topic].[NATO_Topics]"
					},
					"TopicModelingWordsTableSQL": {
						"type": "string",
						"defaultValue": "[Words].[NATO_Topic_Words]"
					},
					"SentimentTableSQL": {
						"type": "string",
						"defaultValue": "[Sent].[NATO_Sentiment]"
					},
					"SummarizationTableSQL": {
						"type": "string",
						"defaultValue": "[sum].[NATO_Abstractive_Sum]"
					},
					"NERTableSQL": {
						"type": "string",
						"defaultValue": "[NER].[NATO_NER]"
					},
					"TableName": {
						"type": "string",
						"defaultValue": "dbo.NATO_Tweets1"
					}
				},
				"variables": {
					"TopicModelingOutputTable": {
						"type": "String"
					},
					"SentimentOutputTable": {
						"type": "String"
					},
					"SummarizationOutputTable": {
						"type": "String"
					},
					"NEROutputTable": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]",
				"[concat(variables('workspaceId'), '/notebooks/TopicModeling_LDA_Sklearn_UsingParms')]",
				"[concat(variables('workspaceId'), '/bigDataPools/SparkPoolSmall')]",
				"[concat(variables('workspaceId'), '/notebooks/CognitiveServices_Sentiment_UsingParms')]",
				"[concat(variables('workspaceId'), '/notebooks/CogntiveServices_Summarization_UsingParms')]",
				"[concat(variables('workspaceId'), '/notebooks/CognitiveServices_NamedEntityRecog_UsingParms')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PauseResumeSQLPool')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "GET List",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools?api-version=2019-06-01-preview')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://management.azure.com/"
							}
						}
					},
					{
						"name": "Filter_PROD",
						"type": "Filter",
						"dependsOn": [
							{
								"activity": "GET List",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('GET List').output.value",
								"type": "Expression"
							},
							"condition": {
								"value": "@not(endswith(item().name,'prod'))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach_pool",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Filter_PROD",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Filter_PROD').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "CheckState",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',item().name,'?api-version=2019-06-01-preview')",
											"type": "Expression"
										},
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "GET",
										"headers": {},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.azure.com/"
										}
									}
								},
								{
									"name": "State-PauseOrResume",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "CheckState",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@concat(activity('CheckState').output.properties.status,'-',pipeline().parameters.PauseorResume)",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "Paused-Resume",
												"activities": [
													{
														"name": "SQLPoolResume",
														"type": "WebActivity",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"url": {
																"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',activity('CheckState').output.name,'/resume?api-version=2019-06-01-preview')",
																"type": "Expression"
															},
															"connectVia": {
																"referenceName": "AutoResolveIntegrationRuntime",
																"type": "IntegrationRuntimeReference"
															},
															"method": "POST",
															"headers": {},
															"body": "Pause and Resume",
															"authentication": {
																"type": "MSI",
																"resource": "https://management.azure.com/"
															}
														}
													}
												]
											},
											{
												"value": "Online-Pause",
												"activities": [
													{
														"name": "SQLPoolPause",
														"type": "WebActivity",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"url": {
																"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',activity('CheckState').output.name,'/pause?api-version=2019-06-01-preview')",
																"type": "Expression"
															},
															"connectVia": {
																"referenceName": "AutoResolveIntegrationRuntime",
																"type": "IntegrationRuntimeReference"
															},
															"method": "POST",
															"headers": {},
															"body": "Pause and Resume",
															"authentication": {
																"type": "MSI",
																"resource": "https://management.azure.com/"
															}
														}
													}
												]
											}
										]
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"ResourceGroup": {
						"type": "string",
						"defaultValue": "Spring2023-TeamPendragon"
					},
					"SubscriptionID": {
						"type": "string",
						"defaultValue": "57cd2ff8-9306-41d0-9cad-c2052a0a8381"
					},
					"WorkspaceName": {
						"type": "string",
						"defaultValue": "pendragon-synapse"
					},
					"SQLPoolName": {
						"type": "string",
						"defaultValue": "SQLPoolTest"
					},
					"PauseorResume": {
						"type": "string",
						"defaultValue": "Resume"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PauseResumeTest')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Pause or Resume SQL Pool",
						"description": "Enter \"Pause\" or \"Resume\" for the \"PauseorResume\" parameter",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "PauseResumeSQLPool",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"PauseorResume": "Resume"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"activity": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/PauseResumeSQLPool')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Run in Sequence')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Execute Pipeline1",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Delete All Look Up Files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Twitter_GET",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Execute Pipeline1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0,899)",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Execute Pipeline2",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "LookUp_LastModified",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {}
									}
								}
							]
						}
					},
					{
						"name": "Delete All Individual Files",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Twitter_Test_RT_DelimitedText1",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Delete All Look Up Files",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Delete All Individual Files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Lookup_Output",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "ForEach1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "Twitter*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings",
									"copyBehavior": "MergeFiles"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".csv"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "Twitter_Test_RT_DelimitedText1",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "Merged_NATO",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Twitter_GET')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_Test_RT_DelimitedText1')]",
				"[concat(variables('workspaceId'), '/datasets/Lookup_Output')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_NATO')]",
				"[concat(variables('workspaceId'), '/pipelines/LookUp_LastModified')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_GET')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Twitter NATO mentions",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"paginationRules": {
									"supportRFC5988": "true"
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "RestResource_Twitter",
								"type": "DatasetReference",
								"parameters": {
									"Query": "(\"NATO\"OR\"North Atlantic Treaty Organization\"OR%23NATO) lang:en -is:retweet -is:reply",
									"max_results": "100",
									"tweet_fields": "id,text,created_at,lang,public_metrics",
									"start_year": "2023",
									"start_month": "03",
									"start_day": "11",
									"start_hour": "20",
									"start_minute": "00",
									"start_second": "01",
									"end_year": "2023",
									"end_month": "03",
									"end_day": "17",
									"end_hour": "20",
									"end_minute": "00",
									"end_second": "01",
									"sort_order": "recency"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Twitter_test_rt_Json",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Convert JSON to CSV",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Twitter NATO mentions",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Twitter_Parse_JSON_to_CSV",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"filenumber": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/RestResource_Twitter')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_test_rt_Json')]",
				"[concat(variables('workspaceId'), '/dataflows/Twitter_Parse_JSON_to_CSV')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/WarmIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_Loop')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "RestSink",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": 10,
								"requestMethod": "POST",
								"writeBatchSize": 10000,
								"httpCompressionType": "none"
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "AzureSqlTable1",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "RestResource_Aithusa",
								"type": "DatasetReference",
								"parameters": {
									"Query": "NATO",
									"max_results": "10",
									"tweet_fields": "id,text,created_at,lang,public_metrics",
									"start_year": "2023",
									"start_month": "03",
									"start_day": "05",
									"start_hour": "00",
									"start_minute": "00",
									"start_second": "01",
									"end_year": "2023",
									"end_month": "03",
									"end_day": "07",
									"end_hour": "00",
									"end_minute": "00",
									"end_second": "01"
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AzureSqlTable1')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "Twitter",
					"table": "NATO_tweets_test"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataset')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "pendragon-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Twitter20230305023424-00001.csv",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/pendragon-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DedicatedSqlPoolTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "entity",
						"type": "nvarchar"
					},
					{
						"name": "entity_category",
						"type": "nvarchar"
					},
					{
						"name": "entity_subcategory",
						"type": "nvarchar"
					},
					{
						"name": "topic",
						"type": "smallint",
						"precision": 5
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "NATO_Tweets1"
				},
				"sqlPool": {
					"referenceName": "SQLPoolTest",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DedicatedSqlPoolTable1_Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "id",
						"type": "bigint",
						"precision": 19
					},
					{
						"name": "created_at",
						"type": "datetime2",
						"scale": 7
					},
					{
						"name": "text",
						"type": "nvarchar"
					},
					{
						"name": "lang",
						"type": "nvarchar"
					},
					{
						"name": "retweet_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "reply_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "like_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "quote_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "impression_count",
						"type": "int",
						"precision": 10
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "NATO_Tweets1_Aithusa"
				},
				"sqlPool": {
					"referenceName": "SQLPoolTest",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Find_minid_input')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Tweets/Individual_files/Lookup_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Individual_NATO_Folder')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Tweets/Individual_files/NATO_Mentions",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Json1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "pendragon-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "RedditTest1dataset",
						"fileSystem": "pendragon"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/pendragon-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lookup_Output')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Tweets/Individual_files/Lookup_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergedJSON_to_CSV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATO_merged.csv",
						"folderPath": "Raw_Twitter_JSONs/Merged_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergedJSON_to_CSV_Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATO_merged.csv",
						"folderPath": "Twitter_Aithusa/Merged_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_Aithusa_JSON_folder')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Twitter_Aithusa/Merged_Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"lang": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"id": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"created_at": {
										"type": "string"
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_JSON_folder')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Raw_Twitter_JSONs/Merged_Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"lang": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"id": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"created_at": {
										"type": "string"
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_Json')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@concat('Twitter','_NATO_',utcnow(),'.json')",
							"type": "Expression"
						},
						"folderPath": "Raw_Twitter_JSONs/Merged_Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_NATO')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@concat('merged_NATO',utcNow(),'.csv')",
							"type": "Expression"
						},
						"folderPath": "Tweets/Combined_files/NATO_mentions",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_output_csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Raw_Twitter_JSONs/Merged_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"rowDelimiter": "\n",
					"escapeChar": "\"",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_output_csv_Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Twitter_Aithusa/Merged_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"rowDelimiter": "\n",
					"escapeChar": "\"",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw_Aithusa_Outputs')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Twitter_Aithusa/Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw_Aithusa_Outputs_withScheme')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Twitter_Aithusa/Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"id": {
										"type": "string"
									},
									"created_at": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"lang": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw_Twitter_Outputs')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Raw_Twitter_JSONs/Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw_Twitter_Outputs1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Raw_Twitter_JSONs/Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"id": {
										"type": "string"
									},
									"lang": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"text": {
										"type": "string"
									},
									"created_at": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResourceTwitter7days')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "RestService_Twitter_7days",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"query": {
						"type": "string",
						"defaultValue": "NATO"
					},
					"tweet_fields": {
						"type": "string",
						"defaultValue": "id,text,created_at,lang,public_metrics"
					},
					"max_results": {
						"type": "string",
						"defaultValue": "10"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().query,'&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/RestService_Twitter_7days')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_ Aithusa",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"Query": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"start_year": {
						"type": "string"
					},
					"start_month": {
						"type": "string"
					},
					"start_day": {
						"type": "string"
					},
					"start_hour": {
						"type": "string"
					},
					"start_minute": {
						"type": "string"
					},
					"start_second": {
						"type": "string"
					},
					"end_year": {
						"type": "string"
					},
					"end_month": {
						"type": "string"
					},
					"end_day": {
						"type": "string"
					},
					"end_hour": {
						"type": "string"
					},
					"end_minute": {
						"type": "string"
					},
					"end_second": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().Query,'&start_time=',dataset().start_year,'-',dataset().start_month,'-',dataset().start_day,'T',dataset().start_hour,':',dataset().start_minute,':',dataset().start_second,'.000Z','&end_time=',dataset().end_year,'-',dataset().end_month,'-',dataset().end_day,'T',dataset().end_hour,':',dataset().end_minute,':',dataset().end_second,'.000Z','&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_ Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Aithusa_7days')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_ Aithusa",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"query": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().query,'&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_ Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Aithusa_7days_NextToken')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_ Aithusa",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"query": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					},
					"next_token": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().query,'&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results,'&next_token=',dataset().next_token)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_ Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Aithusa_withToken')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_ Aithusa",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"Query": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"start_year": {
						"type": "string"
					},
					"start_month": {
						"type": "string"
					},
					"start_day": {
						"type": "string"
					},
					"start_hour": {
						"type": "string"
					},
					"start_minute": {
						"type": "string"
					},
					"start_second": {
						"type": "string"
					},
					"end_year": {
						"type": "string"
					},
					"end_month": {
						"type": "string"
					},
					"end_day": {
						"type": "string"
					},
					"end_hour": {
						"type": "string"
					},
					"end_minute": {
						"type": "string"
					},
					"end_second": {
						"type": "string"
					},
					"next_token": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().Query,'&start_time=',dataset().start_year,'-',dataset().start_month,'-',dataset().start_day,'T',dataset().start_hour,':',dataset().start_minute,':',dataset().start_second,'.000Z','&end_time=',dataset().end_year,'-',dataset().end_month,'-',dataset().end_day,'T',dataset().end_hour,':',dataset().end_minute,':',dataset().end_second,'.000Z','&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results,'&next_token=',dataset().next_token)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_ Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Next_Token')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "RestService_Next_Token",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"query": {
						"type": "string",
						"defaultValue": "NATO"
					},
					"tweet_fields": {
						"type": "string",
						"defaultValue": "id,created_at,text,lang,public_metrics"
					},
					"max_results": {
						"type": "string",
						"defaultValue": "10"
					},
					"next_token": {
						"type": "string",
						"defaultValue": "b26v89c19zqg8o3fqka107quvh02v90tb138wylvg69od"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().query,'&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results,'&next_token=',dataset().next_token)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/RestService_Next_Token')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Twitter')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_AS",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"Query": {
						"type": "string",
						"defaultValue": "NATO"
					},
					"max_results": {
						"type": "string",
						"defaultValue": "10"
					},
					"tweet_fields": {
						"type": "string",
						"defaultValue": "id,text,created_at,lang,public_metrics"
					},
					"start_year": {
						"type": "string",
						"defaultValue": "2023"
					},
					"start_month": {
						"type": "string",
						"defaultValue": "03"
					},
					"start_day": {
						"type": "string",
						"defaultValue": "05"
					},
					"start_hour": {
						"type": "string",
						"defaultValue": "00"
					},
					"start_minute": {
						"type": "string",
						"defaultValue": "00"
					},
					"start_second": {
						"type": "string",
						"defaultValue": "01"
					},
					"end_year": {
						"type": "string",
						"defaultValue": "2023"
					},
					"end_month": {
						"type": "string",
						"defaultValue": "03"
					},
					"end_day": {
						"type": "string",
						"defaultValue": "07"
					},
					"end_hour": {
						"type": "string",
						"defaultValue": "00"
					},
					"end_minute": {
						"type": "string",
						"defaultValue": "00"
					},
					"end_second": {
						"type": "string",
						"defaultValue": "01"
					},
					"sort_order": {
						"type": "string",
						"defaultValue": "recency"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().Query,'&start_time=',dataset().start_year,'-',dataset().start_month,'-',dataset().start_day,'T',dataset().start_hour,':',dataset().start_minute,':',dataset().start_second,'.000Z','&end_time=',dataset().end_year,'-',dataset().end_month,'-',dataset().end_day,'T',dataset().end_hour,':',dataset().end_minute,':',dataset().end_second,'.000Z','&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results,'&sort_order=',dataset().sort_order)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_AS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Twitter2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_AS",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"Query": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"until_id": {
						"type": "string"
					},
					"sort_order": {
						"type": "string",
						"defaultValue": "recency"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().Query,'&tweet.fields=',dataset().tweet_fields,'&until_id=',dataset().until_id,'&max_results=',dataset().max_results,'&sort_order=',dataset().sort_order)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_AS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlPoolTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "entity",
						"type": "nvarchar"
					},
					{
						"name": "entity_category",
						"type": "nvarchar"
					},
					{
						"name": "entity_subcategory",
						"type": "nvarchar"
					},
					{
						"name": "topic",
						"type": "smallint",
						"precision": 5
					},
					{
						"name": "query",
						"type": "varchar"
					},
					{
						"name": "pull_date_time",
						"type": "datetime2",
						"scale": 7
					}
				],
				"typeProperties": {
					"schema": "archive",
					"table": "NER"
				},
				"sqlPool": {
					"referenceName": "SQLPoolTest",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlPoolTable2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "entity",
						"type": "nvarchar"
					},
					{
						"name": "entity_category",
						"type": "nvarchar"
					},
					{
						"name": "entity_subcategory",
						"type": "nvarchar"
					},
					{
						"name": "topic",
						"type": "smallint",
						"precision": 5
					},
					{
						"name": "query",
						"type": "varchar"
					},
					{
						"name": "pull_date_time",
						"type": "datetime2",
						"scale": 7
					}
				],
				"typeProperties": {
					"schema": "archive",
					"table": "NER"
				},
				"sqlPool": {
					"referenceName": "SQLPoolTest",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlPoolTable_Archive_Sent')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "id",
						"type": "bigint",
						"precision": 19
					},
					{
						"name": "created_at",
						"type": "datetime2",
						"scale": 7
					},
					{
						"name": "text",
						"type": "nvarchar"
					},
					{
						"name": "cleantext",
						"type": "nvarchar"
					},
					{
						"name": "lang",
						"type": "nvarchar"
					},
					{
						"name": "retweet_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "reply_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "like_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "quote_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "impression_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "topic",
						"type": "smallint",
						"precision": 5
					},
					{
						"name": "topic_0_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "topic_1_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "topic_2_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "topic_3_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "topic_4_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "sentiment",
						"type": "nvarchar"
					},
					{
						"name": "negative_score",
						"type": "real",
						"precision": 7
					},
					{
						"name": "positive_score",
						"type": "real",
						"precision": 7
					},
					{
						"name": "neutral_score",
						"type": "real",
						"precision": 7
					},
					{
						"name": "mixed_score",
						"type": "real",
						"precision": 7
					},
					{
						"name": "query",
						"type": "nvarchar"
					},
					{
						"name": "pull_datetime",
						"type": "datetime2",
						"scale": 7
					}
				],
				"typeProperties": {
					"schema": "archive",
					"table": "Sentiment"
				},
				"sqlPool": {
					"referenceName": "SQLPoolTest",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlPoolTable_Sentiment')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "id",
						"type": "bigint",
						"precision": 19
					},
					{
						"name": "created_at",
						"type": "datetime2",
						"scale": 7
					},
					{
						"name": "text",
						"type": "nvarchar"
					},
					{
						"name": "cleantext",
						"type": "nvarchar"
					},
					{
						"name": "lang",
						"type": "nvarchar"
					},
					{
						"name": "retweet_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "reply_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "like_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "quote_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "impression_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "topic",
						"type": "smallint",
						"precision": 5
					},
					{
						"name": "topic_0_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "topic_1_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "topic_2_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "topic_3_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "topic_4_confidence",
						"type": "real",
						"precision": 7
					},
					{
						"name": "sentiment",
						"type": "nvarchar"
					},
					{
						"name": "negative_score",
						"type": "real",
						"precision": 7
					},
					{
						"name": "positive_score",
						"type": "real",
						"precision": 7
					},
					{
						"name": "neutral_score",
						"type": "real",
						"precision": 7
					},
					{
						"name": "mixed_score",
						"type": "real",
						"precision": 7
					},
					{
						"name": "query",
						"type": "nvarchar"
					},
					{
						"name": "pull_datetime",
						"type": "datetime2",
						"scale": 7
					}
				],
				"typeProperties": {
					"schema": "archive",
					"table": "Sentiment"
				},
				"sqlPool": {
					"referenceName": "SQLPoolTest",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TwitterJson1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "pendragon-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Twitter_Test_DynamicURL",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"results": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"label": {
										"type": "null"
									},
									"name": {
										"type": "string"
									},
									"volume": {
										"type": "integer"
									},
									"retweets": {
										"type": "integer"
									},
									"tweets": {
										"type": "integer"
									},
									"reachEstimate": {
										"type": "integer"
									},
									"impressions": {
										"type": "integer"
									}
								}
							}
						},
						"orderBy": {
							"type": "string"
						},
						"orderDirection": {
							"type": "string"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/pendragon-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "pendragon-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Twitter_Test_RT_CSV",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/pendragon-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_RawJSON_Temp')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATO_Temp",
						"folderPath": "Raw_Twitter_JSONs/Temp",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"id": {
										"type": "string"
									},
									"created_at": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"lang": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_Test_RT_DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Tweets/Individual_files/NATO_Mentions",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"nullValue": "NA",
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_test_rt_Json')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATOmentions",
						"folderPath": "Raw_Twitter_JSONs",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"created_at": {
										"type": "string"
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"id": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"text": {
										"type": "string"
									},
									"lang": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/inputFolder_file_Dyn')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"FileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().FileName",
							"type": "Expression"
						},
						"folderPath": "Tweets/Individual_files/NATO_Mentions",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/json_Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATO_Temp",
						"folderPath": "Twitter_Aithusa/Temp",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"lang": {
										"type": "string"
									},
									"created_at": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"id": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureMLService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMLService",
				"typeProperties": {
					"subscriptionId": "[parameters('AzureMLService1_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureMLService1_properties_typeProperties_resourceGroupName')]",
					"mlWorkspaceName": "pendragon-ml",
					"authentication": "MSI"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "linked pipelines connections",
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase2_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CognitiveService",
				"typeProperties": {
					"subscriptionId": "[parameters('CognitiveService1_properties_typeProperties_subscriptionId')]",
					"resourceGroup": "spring2023-teampendragon",
					"csName": "pendragon-language",
					"csKind": "TextAnalytics",
					"csLocation": "eastus",
					"endPoint": "https://pendragon-language.cognitiveservices.azure.com/",
					"csKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "PendragonKeyVault",
							"type": "LinkedServiceReference"
						},
						"secretName": "LangaugeKey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/PendragonKeyVault')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PendragonKeyVault')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('PendragonKeyVault_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspace1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "89f52e0a-3aeb-4050-9251-54de8e97068e",
					"tenantID": "9e857255-df57-4c47-a0c0-0546460380cb"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspacePendragon')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "9155e61a-49a3-482c-957f-e73248b2d271",
					"tenantID": "9e857255-df57-4c47-a0c0-0546460380cb"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestService_Next_Token')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('RestService_Next_Token_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous",
					"authHeaders": {
						"Authorization": {
							"type": "SecureString",
							"value": "**********"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestService_Twitter_7days')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('RestService_Twitter_7days_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous",
					"authHeaders": {
						"Authorization": {
							"type": "SecureString",
							"value": "**********"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_RestService_ Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('Twitter_RestService_ Aithusa_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous",
					"authHeaders": {
						"Authorization": {
							"type": "SecureString",
							"value": "**********"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_RestService_AS')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('Twitter_RestService_AS_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous",
					"authHeaders": {
						"Authorization": {
							"type": "SecureString",
							"value": "**********"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pendragon-synapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('pendragon-synapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pendragon-synapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('pendragon-synapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WarmIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 10,
							"cleanup": false
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Find Last ID')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Find_minid_input",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregate1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          created_at as string,",
						"          text as string,",
						"          lang as string,",
						"          retweet_count as string,",
						"          reply_count as string,",
						"          like_count as string,",
						"          quote_count as string,",
						"          impression_count as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     purgeFiles: true) ~> source1",
						"source1 aggregate(id = min(id)) ~> aggregate1",
						"aggregate1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Find_minid_input')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get Next Token for Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "json_Aithusa",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "json_Aithusa",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "Raw_Aithusa_Outputs",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), edit_history_tweet_ids as string[], lang as string, created_at as string, text as string, id as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source(output(",
						"          data as (public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), edit_history_tweet_ids as string[], lang as string, created_at as string, text as string, id as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source2",
						"source1 derive(next_token = meta.next_token) ~> derivedColumn1",
						"derivedColumn1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1,",
						"     mapColumn(",
						"          next_token",
						"     )) ~> sink1",
						"source2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     filePattern:(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.json')),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2) ~> sink2"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/json_Aithusa')]",
				"[concat(variables('workspaceId'), '/datasets/Raw_Aithusa_Outputs')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get Next Token')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "Raw_Twitter_Outputs",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (edit_history_tweet_ids as string[], id as string, created_at as string, text as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source(output(",
						"          data as (edit_history_tweet_ids as string[], id as string, created_at as string, text as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     purgeFiles: true,",
						"     documentForm: 'documentPerLine') ~> source2",
						"source1 derive(next_token = iif(hasPath('meta.next_token'),meta.next_token,'000')) ~> derivedColumn1",
						"derivedColumn1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1,",
						"     mapColumn(",
						"          next_token",
						"     )) ~> sink1",
						"source2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     filePattern:(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.json')),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2) ~> sink2"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Twitter_RawJSON_Temp')]",
				"[concat(variables('workspaceId'), '/datasets/Raw_Twitter_Outputs')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get next token variable')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "json_Aithusa",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (edit_history_tweet_ids as string[], id as string, created_at as string, text as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source1 derive(next_token = meta.next_token) ~> derivedColumn1",
						"derivedColumn1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1,",
						"     mapColumn(",
						"          next_token",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/json_Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get result_count')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (edit_history_tweet_ids as string[], id as string, created_at as string, text as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source1 derive(next_token = iif(hasPath('meta.next_token'),meta.next_token,'000')) ~> derivedColumn1",
						"derivedColumn1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Twitter_RawJSON_Temp')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merge JSON to CSV')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Raw_Twitter_Outputs1",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "MergedJSON_to_CSV",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "sort1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (id as string, lang as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), edit_history_tweet_ids as string[], text as string, created_at as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'arrayOfDocuments') ~> source1",
						"source1 foldDown(unroll(data, data),",
						"     mapColumn(",
						"          data,",
						"          meta",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 derive(id = data.id,",
						"          created_at = data.created_at,",
						"          text = replace(replace(replace(data.text, '\\n\\n', ' '), '\\n', ' '),'\\\"',''),",
						"          lang = data.lang,",
						"          retweet_count = data.public_metrics.retweet_count,",
						"          reply_count = data.public_metrics.reply_count,",
						"          like_count = data.public_metrics.like_count,",
						"          quote_count = data.public_metrics.quote_count,",
						"          impression_count = data.public_metrics.impression_count) ~> derivedColumn1",
						"derivedColumn1 sort(desc(id, false)) ~> sort1",
						"sort1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          created_at as string,",
						"          text as string,",
						"          lang as string,",
						"          retweet_count as string,",
						"          reply_count as string,",
						"          like_count as string,",
						"          quote_count as string,",
						"          impression_count as string",
						"     ),",
						"     partitionFileNames:[(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.csv'))],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          created_at,",
						"          text,",
						"          lang,",
						"          retweet_count,",
						"          reply_count,",
						"          like_count,",
						"          quote_count,",
						"          impression_count",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Raw_Twitter_Outputs1')]",
				"[concat(variables('workspaceId'), '/datasets/MergedJSON_to_CSV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merge JSONs and Convert to CSV for Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Raw_Aithusa_Outputs_withScheme",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "MergedJSON_to_CSV_Aithusa",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "sort1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (edit_history_tweet_ids as string[], id as string, created_at as string, text as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'arrayOfDocuments') ~> source1",
						"source1 foldDown(unroll(data, data),",
						"     mapColumn(",
						"          data,",
						"          meta",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 derive(id = data.id,",
						"          created_at = data.created_at,",
						"          text = replace(replace(replace(data.text, '\\n\\n', ' '), '\\n', ' '),'\\\"',''),",
						"          lang = data.lang,",
						"          retweet_count = data.public_metrics.retweet_count,",
						"          reply_count = data.public_metrics.reply_count,",
						"          like_count = data.public_metrics.like_count,",
						"          quote_count = data.public_metrics.quote_count,",
						"          impression_count = data.public_metrics.impression_count) ~> derivedColumn1",
						"derivedColumn1 sort(desc(id, false)) ~> sort1",
						"sort1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          created_at as string,",
						"          text as string,",
						"          lang as string,",
						"          retweet_count as string,",
						"          reply_count as string,",
						"          like_count as string,",
						"          quote_count as string,",
						"          impression_count as string",
						"     ),",
						"     partitionFileNames:[(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.csv'))],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          created_at,",
						"          text,",
						"          lang,",
						"          retweet_count,",
						"          reply_count,",
						"          like_count,",
						"          quote_count,",
						"          impression_count",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Raw_Aithusa_Outputs_withScheme')]",
				"[concat(variables('workspaceId'), '/datasets/MergedJSON_to_CSV_Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_Parse_JSON_to_CSV')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Twitter_test_rt_Json",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Twitter_Test_RT_DelimitedText1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "sort1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (created_at as string, edit_history_tweet_ids as string[], id as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), text as string, lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source1 foldDown(unroll(data, data),",
						"     mapColumn(",
						"          data,",
						"          meta",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 derive(id = data.id,",
						"          created_at = data.created_at,",
						"          text = replace(replace(replace(data.text, '\\n\\n', ' '), '\\n', ' '),'\\\"',''),",
						"          lang = data.lang,",
						"          retweet_count = data.public_metrics.retweet_count,",
						"          reply_count = data.public_metrics.reply_count,",
						"          like_count = data.public_metrics.like_count,",
						"          quote_count = data.public_metrics.quote_count,",
						"          impression_count = data.public_metrics.impression_count) ~> derivedColumn1",
						"derivedColumn1 sort(asc(id, true),",
						"     partitionLevel: true) ~> sort1",
						"sort1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          created_at as string,",
						"          text as string,",
						"          lang as string,",
						"          retweet_count as string,",
						"          reply_count as string,",
						"          like_count as string,",
						"          quote_count as string,",
						"          impression_count as string",
						"     ),",
						"     filePattern:(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.csv')),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          created_at,",
						"          text,",
						"          lang,",
						"          retweet_count,",
						"          reply_count,",
						"          like_count,",
						"          quote_count,",
						"          impression_count",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Twitter_test_rt_Json')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_Test_RT_DelimitedText1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PendragonManagedIdentityCredential')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {
					"resourceId": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourcegroups/Spring2023-TeamPendragon/providers/Microsoft.ManagedIdentity/userAssignedIdentities/PendragonManagedIdentity"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Abstract_ArchiveTable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Archive"
				},
				"content": {
					"query": "CREATE TABLE [Archive].[Abstractive_Sum]\n(\n    [abstractive_summary] NVARCHAR(4000) NULL,\n    [topic] SMALLINT NULL,\n    [query] NVARCHAR(20) NULL,\n    [pull_datetime] DATETIME2(7) NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Aithusa_Abstract_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Aithusa"
				},
				"content": {
					"query": "CREATE TABLE [sum].[Aithusa_Abstractive_Sum]\n(\n    [topic] SMALLINT NULL,\n    [abstractive_summary] NVARCHAR(4000) NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Aithusa_NER_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Aithusa"
				},
				"content": {
					"query": "CREATE TABLE [NER].[Aithusa_NER]\n(\n    [entity] NVARCHAR(100) NULL,\n    [entity_category] NVARCHAR(100) NULL,\n    [entity_subcategory] NVARCHAR(100) NULL,\n    [topic] SMALLINT NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Aithusa_Sentiment_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Aithusa"
				},
				"content": {
					"query": "CREATE TABLE [Sent].[Aithusa_Sentiment]\n(\n    [id] bigint  NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[cleantext] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n    [topic] SMALLINT NULL,\n    [topic_0_confidence] REAL NULL,\n    [topic_1_confidence] REAL NULL,\n    [topic_2_confidence] REAL NULL,\n    [topic_3_confidence] REAL NULL,\n    [topic_4_confidence] REAL NULL,\n    [sentiment] NVARCHAR(20) NULL,\n    [negative_score] REAL NULL,\n    [positive_score] REAL NULL,\n    [neutral_score] REAL NULL,\n    [mixed_score] REAL NULL\n)\n\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Aithusa_TopicWords_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Aithusa"
				},
				"content": {
					"query": "CREATE TABLE [Words].[Aithusa_Topic_Words]\n( \n\t[topic] bigint  NULL,\n\t[word_1] NVARCHAR(20)  NULL,\n\t[word_2] NVARCHAR(20)  NULL,\n    [word_3] NVARCHAR(20)  NULL,\n    [word_4] NVARCHAR(20)  NULL,\n    [word_5] NVARCHAR(20)  NULL,\n    [word_6] NVARCHAR(20)  NULL,\n    [word_7] NVARCHAR(20)  NULL,\n    [word_8] NVARCHAR(20)  NULL,\n    [word_9] NVARCHAR(20)  NULL,\n    [word_10] NVARCHAR(20)  NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Aithusa_Topic_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Aithusa"
				},
				"content": {
					"query": "CREATE TABLE [Topic].[Aithusa_Topics]\n( \n\t[id] bigint  NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n\t[cleantext] NVARCHAR(4000)  NULL,\n    [topic_0_confidence] FLOAT NULL,\n    [topic_1_confidence] FLOAT NULL,\n    [topic_2_confidence] FLOAT NULL,\n    [topic_3_confidence] FLOAT NULL,\n    [topic_4_confidence] FLOAT NULL,\n\t[topic] BIGINT NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Archive_Tweets_Aithusa_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Aithusa"
				},
				"content": {
					"query": "CREATE TABLE [archive].[Aithusa_Archive_Tweets]\n( \n\t[id] bigint  NOT NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n    [query] NVARCHAR(20) NOT NULL,\n    [pull_datetime] DATETIME2(7) NOT NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Archive_Tweets_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Archive"
				},
				"content": {
					"query": "CREATE TABLE [archive].[NATO_Archive_Tweets]\n( \n\t[id] bigint  NOT NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n    [query] NVARCHAR(20) NULL,\n    [pull_datetime] DATETIME2(7) NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Abstract_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [sum].[NATO_Abstractive_Sum]\n(\n    [abstractive_summary] NVARCHAR(4000) NULL,\n    [topic] SMALLINT NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_NER_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [NER].[NATO_NER]\n(\n    [entity] NVARCHAR(100) NULL,\n    [entity_category] NVARCHAR(100) NULL,\n    [entity_subcategory] NVARCHAR(100) NULL,\n    [topic] SMALLINT NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Sentiment_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [Sent].[NATO_Sentiment]\n(\n    [id] bigint  NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[cleantext] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n    [topic] SMALLINT NULL,\n    [topic_0_confidence] REAL NULL,\n    [topic_1_confidence] REAL NULL,\n    [topic_2_confidence] REAL NULL,\n    [topic_3_confidence] REAL NULL,\n    [topic_4_confidence] REAL NULL,\n    [sentiment] NVARCHAR(20) NULL,\n    [negative_score] REAL NULL,\n    [positive_score] REAL NULL,\n    [neutral_score] REAL NULL,\n    [mixed_score] REAL NULL\n)\n\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_TopicWords_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [Words].[NATO_Topic_Words]\n( \n\t[topic] bigint  NULL,\n\t[word_1] NVARCHAR(20)  NULL,\n\t[word_2] NVARCHAR(20)  NULL,\n    [word_3] NVARCHAR(20)  NULL,\n    [word_4] NVARCHAR(20)  NULL,\n    [word_5] NVARCHAR(20)  NULL,\n    [word_6] NVARCHAR(20)  NULL,\n    [word_7] NVARCHAR(20)  NULL,\n    [word_8] NVARCHAR(20)  NULL,\n    [word_9] NVARCHAR(20)  NULL,\n    [word_10] NVARCHAR(20)  NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_TopicWords_Table_LEM')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/LEM"
				},
				"content": {
					"query": "CREATE TABLE [Words].[NATO_Topic_Words_LEM]\n( \n\t[topic] bigint  NULL,\n\t[word_1] NVARCHAR(20)  NULL,\n\t[word_2] NVARCHAR(20)  NULL,\n    [word_3] NVARCHAR(20)  NULL,\n    [word_4] NVARCHAR(20)  NULL,\n    [word_5] NVARCHAR(20)  NULL,\n    [word_6] NVARCHAR(20)  NULL,\n    [word_7] NVARCHAR(20)  NULL,\n    [word_8] NVARCHAR(20)  NULL,\n    [word_9] NVARCHAR(20)  NULL,\n    [word_10] NVARCHAR(20)  NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Topic_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [Topic].[NATO_Topics]\n( \n\t[id] bigint  NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n\t[cleantext] NVARCHAR(4000)  NULL,\n    [topic_0_confidence] FLOAT NULL,\n    [topic_1_confidence] FLOAT NULL,\n    [topic_2_confidence] FLOAT NULL,\n    [topic_3_confidence] FLOAT NULL,\n    [topic_4_confidence] FLOAT NULL,\n\t[topic] BIGINT NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Topic_Table_LEM')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [Topic].[NATO_Topics_LEM]\n( \n\t[id] bigint  NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n\t[cleantext] NVARCHAR(4000)  NULL,\n    [topic_0_confidence] FLOAT NULL,\n    [topic_1_confidence] FLOAT NULL,\n    [topic_2_confidence] FLOAT NULL,\n    [topic_3_confidence] FLOAT NULL,\n    [topic_4_confidence] FLOAT NULL,\n\t[topic] BIGINT NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Tweets1_Aithusa_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Aithusa"
				},
				"content": {
					"query": "SET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[NATO_Tweets1_Aithusa]\n( \n\t[id] bigint  NOT NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL\n)\nWITH\n(\n\tDISTRIBUTION = HASH ( [id] ),\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Tweets1_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "SET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[NATO_Tweets1]\n( \n\t[id] bigint  NOT NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL\n)\nWITH\n(\n\tDISTRIBUTION = HASH ( [id] ),\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_testtable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [test].[NATO_test]\n( \n\t[id] bigint  NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[cleantext] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n    [topic] SMALLINT NULL,\n    [topic_0_confidence] REAL NULL,\n    [topic_1_confidence] REAL NULL,\n    [topic_2_confidence] REAL NULL,\n    [topic_3_confidence] REAL NULL,\n    [topic_4_confidence] REAL NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NER_ArchiveTable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Archive"
				},
				"content": {
					"query": "CREATE TABLE [Archive].[NER]\n(\n    [entity] NVARCHAR(100) NULL,\n    [entity_category] NVARCHAR(100) NULL,\n    [entity_subcategory] NVARCHAR(100) NULL,\n    [topic] SMALLINT NULL,\n    [query] NVARCHAR(20) NULL,\n    [pull_datetime] DATETIME2(7) NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Sentiment_ArchiveTable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Archive"
				},
				"content": {
					"query": "CREATE TABLE [Archive].[Sentiment]\n(\n    [id] bigint  NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[cleantext] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n    [topic] SMALLINT NULL,\n    [topic_0_confidence] REAL NULL,\n    [topic_1_confidence] REAL NULL,\n    [topic_2_confidence] REAL NULL,\n    [topic_3_confidence] REAL NULL,\n    [topic_4_confidence] REAL NULL,\n    [sentiment] NVARCHAR(20) NULL,\n    [negative_score] REAL NULL,\n    [positive_score] REAL NULL,\n    [neutral_score] REAL NULL,\n    [mixed_score] REAL NULL,\n    [query] NVARCHAR(20) NULL,\n    [pull_datetime] DATETIME2(7) NULL\n)\n\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Timeline')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Analytics"
				},
				"content": {
					"query": "SELECT DATEADD(second, (DATEPART(second, created_at) / 10) * 10, DATEADD(minute, DATEDIFF(minute, 0, created_at), 0)) AS IntervalStart,\n       COUNT(*) AS Count\nFROM [dbo].[NATO_Tweets1]\nGROUP BY DATEADD(second, (DATEPART(second, created_at) / 10) * 10, DATEADD(minute, DATEDIFF(minute, 0, created_at), 0))\nORDER BY IntervalStart;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_TopicWords_ArchiveTable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables/Archive"
				},
				"content": {
					"query": "CREATE TABLE [Archive].[Topic_Words]\n( \n\t[topic] bigint  NULL,\n\t[word_1] NVARCHAR(20)  NULL,\n\t[word_2] NVARCHAR(20)  NULL,\n    [word_3] NVARCHAR(20)  NULL,\n    [word_4] NVARCHAR(20)  NULL,\n    [word_5] NVARCHAR(20)  NULL,\n    [word_6] NVARCHAR(20)  NULL,\n    [word_7] NVARCHAR(20)  NULL,\n    [word_8] NVARCHAR(20)  NULL,\n    [word_9] NVARCHAR(20)  NULL,\n    [word_10] NVARCHAR(20)  NULL,\n    [query] NVARCHAR(20) NULL,\n    [pull_datetime] DATETIME2(7) NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeleteAllRows')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SQL_Procedures"
				},
				"content": {
					"query": "CREATE PROCEDURE delete_all_rows (@table_name NVARCHAR(MAX))\nAS\nBEGIN\n    DECLARE @sql NVARCHAR(MAX)\n    SET @sql = 'DELETE FROM ' + @table_name\n    EXEC sp_executesql @sql\nEND",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Other"
				},
				"content": {
					"query": "CREATE PROCEDURE tweetsClean\nAS\nBEGIN\n     FROM [dbo].[NATO_Tweets1];\nEND;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Other"
				},
				"content": {
					"query": "DROP TABLE [dbo].[NATO_Tweets1]\nGO\n\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[NATO_Tweets1]\n( \n\t[id] [bigint]  NOT NULL,\n\t[created_at] [datetime2](7)  NULL,\n\t[text] [nvarchar](4000)  NULL,\n\t[lang] [nvarchar](10)  NULL,\n\t[retweet_count] [int]  NULL,\n\t[reply_count] [int]  NULL,\n\t[like_count] [int]  NULL,\n\t[quote_count] [int]  NULL,\n\t[impression_count] [int]  NULL,\n\t[cleanText] [nvarchar](4000)\n)\nWITH\n(\n\tDISTRIBUTION = HASH ( [id] ),\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Other"
				},
				"content": {
					"query": "CREATE PROC [dbo].[cleanTweets]\n\nAS\nBEGIN\nupdate dbo.NATO_Tweets1 set cleanText=text;\nUPDATE dbo.NATO_Tweets1 \nSET text = CASE \n\n             WHEN CHARINDEX('https', text) > 0 \n             THEN LEFT(text, CHARINDEX('https', text) - 1) \n             ELSE text \n           END;\n           delete from dbo.NATO_Tweets1 where text='';\nEND\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM [archive].[NATO_Archive_Tweets]\nORDER BY created_at ASC",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 6')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE SCHEMA [Schema]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ArchiveAnalysis')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archiving"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "50521613-0309-42d3-9d93-3f59d651dec2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Archive Tweets"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"query = \"Test001\"\n",
							"\n",
							"Sent_maintable = \"Sent.NATO_Sentiment\"\n",
							"TopicWords_maintable = \"Words.NATO_Topic_Words\"\n",
							"NamedEntity_maintable = \"NER.NATO_NER\"\n",
							"Abstract_maintable = \"sum.NATO_Abstractive_Sum\"\n",
							"\n",
							"Sent_archivetable = \"Archive.Sentiment\"\n",
							"TopicWords_archivetable = \"Archive.Topic_Words\"\n",
							"NamedEntity_archivetable = \"Archive.NER\"\n",
							"Abstract_archivetable = \"Archive.Abstractive_Sum\"\n",
							"\n",
							"#query = \"Aithusa\"\n",
							"#maintable = \"dbo.NATO_Tweets1_Aithusa\"\n",
							"#archivetable = \"Aithusa_Archive_Tweets\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create array of tables to loop through\r\n",
							"tables = [[Sent_maintable,Sent_archivetable],\r\n",
							"            [TopicWords_maintable,TopicWords_archivetable],\r\n",
							"            [NamedEntity_maintable,NamedEntity_archivetable],\r\n",
							"            [Abstract_maintable,Abstract_archivetable]]"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to set columns to nullable\r\n",
							"def set_df_columns_nullable(spark, df, column_list, nullable=True):\r\n",
							"    for struct_field in df.schema:\r\n",
							"        if struct_field.name in column_list:\r\n",
							"            struct_field.nullable = nullable\r\n",
							"    df_mod = spark.createDataFrame(df.rdd, df.schema)\r\n",
							"    return df_mod"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import lit, current_timestamp, col\r\n",
							"\r\n",
							"for each in tables:\r\n",
							"    print(each[0])\r\n",
							"    print(each[1])\r\n",
							"\r\n",
							"    # Read from a query\r\n",
							"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"    df = (spark.read\r\n",
							"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                        # Defaults to storage path defined in the runtime configurations\r\n",
							"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                        # query from which data will be read\r\n",
							"                        .option(Constants.QUERY, \"select * from \" + each[0])\r\n",
							"                        .synapsesql()\r\n",
							"    )\r\n",
							"\r\n",
							"    # create a new DataFrame with two new columns\r\n",
							"    new_df = df.withColumn('query', lit(query)).withColumn('pull_datetime', current_timestamp())\r\n",
							"\r\n",
							"    new_df = set_df_columns_nullable(spark,new_df,['query','pull_datetime'])\r\n",
							"\r\n",
							"    #new_df = new_df.schema['query'].nullable = True\r\n",
							"    #new_df = new_df.schema['pull_datetime'].nullable = True\r\n",
							"\r\n",
							"\r\n",
							"    # show the new DataFrame\r\n",
							"    #display(new_df)\r\n",
							"    #print(\"Main table schema:\")\r\n",
							"    #print(new_df.printSchema())\r\n",
							"\r\n",
							"    outputtable = \"SQLPoolTest.\" + each[1]\r\n",
							"    #print(\"Target table schema:\")\r\n",
							"    #print(outputtable)\r\n",
							"    #print(spark.read.synapsesql(outputtable).limit(0).printSchema())\r\n",
							"    #print(outputtable)\r\n",
							"\r\n",
							"\r\n",
							"    # Write using AAD Auth to internal table\r\n",
							"    # Add required imports\r\n",
							"\r\n",
							"    # Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"    # Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"    (new_df.write\r\n",
							"    # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"    # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"    .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"    # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							"    .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"    # Choose a save mode that is apt for your use case.\r\n",
							"    # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							"    # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							"    .mode(\"overwrite\")\r\n",
							"    # Required parameter - Three-part table name to which data will be written\r\n",
							"    .synapsesql(outputtable))\r\n",
							"\r\n",
							"    print(each[1],\" Archived\")"
						],
						"outputs": [],
						"execution_count": 30
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ArchiveTweets')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Archiving"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bf1fc20c-bcd9-47b4-91a3-66b7f7d146dd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Archive Tweets"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"query = \"NATO\"\n",
							"maintable = \"dbo.NATO_Tweets1\"\n",
							"archivetable = \"NATO_Archive_Tweets\"\n",
							"\n",
							"#query = \"Aithusa\"\n",
							"#maintable = \"dbo.NATO_Tweets1_Aithusa\"\n",
							"#archivetable = \"Aithusa_Archive_Tweets\""
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Read from a query\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\n",
							"df = (spark.read\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\n",
							"                     # Defaults to storage path defined in the runtime configurations\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\n",
							"                     # query from which data will be read\n",
							"                     .option(Constants.QUERY, \"select * from \" + maintable)\n",
							"                     .synapsesql()\n",
							")"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import lit, current_timestamp\r\n",
							"\r\n",
							"# create a new DataFrame with two new columns\r\n",
							"new_df = df.withColumn('query', lit(query)).withColumn('pull_datetime', current_timestamp())\r\n",
							"\r\n",
							"new_df.schema['query'].nullable = True\r\n",
							"new_df.schema['pull_datetime'].nullable = True\r\n",
							"\r\n",
							"# drop the \"cleanText\" column\r\n",
							"new_df = new_df.drop(\"cleanText\")\r\n",
							"\r\n",
							"# show the new DataFrame\r\n",
							"display(new_df)"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.archive.\"+archivetable"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_df.printSchema()"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.read.synapsesql(outputtable).limit(0).printSchema()"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(new_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": 43
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CleansDataFromFolder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ReadWrite"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1a48c861-d298-446b-aa05-d8340e6d6d91"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"!pip install azure-storage-file-datalake"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# azureml-core of version 1.0.72 or higher is required\r\n",
							"# azureml-dataprep[pandas] of version 1.1.34 or higher is required\r\n",
							"from azureml.core import Workspace, Dataset\r\n",
							"\r\n",
							"subscription_id = '57cd2ff8-9306-41d0-9cad-c2052a0a8381'\r\n",
							"resource_group = 'Spring2023-TeamPendragon'\r\n",
							"workspace_name = 'pendragon-ml'\r\n",
							"\r\n",
							"workspace = Workspace(subscription_id, resource_group, workspace_name)\r\n",
							"\r\n",
							"dataset = Dataset.get_by_name(workspace, name='NATO_Tweets')\r\n",
							"df = dataset.to_pandas_dataframe()\r\n",
							"df\r\n",
							""
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Remove @, #, links, and emoticons"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# remove @s\r\n",
							"df['text'] = df['text'].str.replace(r'@\\w+\\s*', '')\r\n",
							"\r\n",
							"# remove hashtags\r\n",
							"df['text'] = df['text'].str.replace(r'#\\w+\\s*', '')\r\n",
							"\r\n",
							"# Remove links\r\n",
							"import re\r\n",
							"df['text'] = df['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\r\n",
							"\r\n",
							"# drop rows where the 'text' column is an empty string\r\n",
							"df = df.drop(df[df['text'] == ''].index)\r\n",
							"\r\n",
							"# drop rows where the 'lang' column is not 'en'\r\n",
							"df = df.drop(df[df['lang'] != 'en'].index)\r\n",
							"\r\n",
							"# print the resulting dataframe\r\n",
							"#df.head(10)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import re\r\n",
							"\r\n",
							"# define a regex pattern to match emoticons\r\n",
							"emoticon_pattern = re.compile(\"[\"\r\n",
							"                              u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
							"                              u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
							"                              u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
							"                              u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
							"                              \"]+\", flags=re.UNICODE)\r\n",
							"\r\n",
							"# apply the regex pattern to the 'text' column to remove emoticons\r\n",
							"df['text'] = df['text'].str.replace(emoticon_pattern, '')\r\n",
							"\r\n",
							"# print the modified DataFrame\r\n",
							"print(df['text'])"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Add a period to the end of text if one doesn't exist"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# check if each string in 'text' column ends with a period\r\n",
							"no_period_mask = ~df['text'].str.endswith('.')\r\n",
							"\r\n",
							"# add period to the end of the strings that don't end with a period\r\n",
							"df.loc[no_period_mask, 'text'] += '.'\r\n",
							"\r\n",
							"# print the modified DataFrame\r\n",
							"#print(df['text'])"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sort the DataFrame by the 'created_at' column\r\n",
							"df = df.sort_values(by='created_at')\r\n",
							"#df.head(10)"
						],
						"outputs": [],
						"execution_count": 23
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "df68cc03-e221-4aa6-ade5-c677ff7894d9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Sentiment Analysis, Entity Extraction, Key Phrase Extraction, Named Entity Recognition"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"sorted_df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"\r\n",
							"# select the first 20 records\r\n",
							"first_20 = sorted_df.limit(20)\r\n",
							"first_20.show()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import regexp_replace\r\n",
							"\r\n",
							"# Define the regular expression pattern to match words starting with @, &, or https\r\n",
							"regex = r'@\\w+|&\\w+|https\\w+|#\\w+'\r\n",
							"regex = r'\\b[@#&]\\w+|\\bhttps\\w*'\r\n",
							"\r\n",
							"# Apply the regexp_replace function to the text column and create a new column with the filtered text\r\n",
							"df_filtered = first_20.withColumn(\"text\", regexp_replace(col(\"text\"), regex, ''))\r\n",
							"\r\n",
							"# Show the filtered DataFrame\r\n",
							"df_filtered.select(\"text\").show()\r\n",
							"\r\n",
							"first_20 = df_filtered"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(dir(synapse.ml.cognitive))"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sentiment"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"You can send up to 125,000 characters across all documents contained in the asynchronous request. Each tweet is a maximum length of 4,000 characters.\r\n",
							"125,000/4,000 = 31 Tweets"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\r\n",
							"from pyspark.sql.functions import col\r\n",
							"import pyspark.sql.functions as F\r\n",
							"import collections\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 5\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = first_20.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of equal to the batch_size\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = first_20.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"output = []\r\n",
							"\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(batch_size)\r\n",
							"    sent = (TextSentiment()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setOutputCol(\"sentiment\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    sent_batch_output = sent.transform(batch.select(\"text\"))\r\n",
							"    #display(sent_batch_output)\r\n",
							"\r\n",
							"    # Extract the 'sentiment' column and convert it to JSON\r\n",
							"    json_output = sent_batch_output.select(\"sentiment\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'sentiment' field\r\n",
							"    sentiments = [json.loads(x)[\"sentiment\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'sentiment' field for each row\r\n",
							"    for sentiment in sentiments:\r\n",
							"        #print(sentiment)\r\n",
							"        output.append(sentiment['document']['sentiment'])\r\n",
							"\r\n",
							"# Print all sentiments\r\n",
							"print(output)\r\n",
							"\r\n",
							"# Count the occurrences of each value in the array\r\n",
							"counts = collections.Counter(output)\r\n",
							"\r\n",
							"# Get the most common value and its count\r\n",
							"most_common = counts.most_common(1)[0]\r\n",
							"most_common_value = most_common[0]\r\n",
							"most_common_count = most_common[1]\r\n",
							"\r\n",
							"#print(\"Most common value:\", most_common_value)\r\n",
							"#print(\"Count:\", most_common_count)\r\n",
							"\r\n",
							"confidence_scores = {}\r\n",
							"\r\n",
							"total_values = len(output)\r\n",
							"\r\n",
							"for value, count in counts.items():\r\n",
							"    confidence_scores[value] = count / total_values\r\n",
							"\r\n",
							"#print(\"Confidence scores:\", confidence_scores)\r\n",
							"sentiment_output = [{\"Sentiment\": most_common_value},{\"Count\": most_common_count},{\"Confidence Scores\":confidence_scores}]\r\n",
							"print(sentiment_output)\r\n",
							"\r\n",
							"# Add to DataFrame\r\n",
							"\r\n",
							"# Define the schema of the DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"Sentiment\", StringType(), True),\r\n",
							"    StructField(\"Count\", IntegerType(), True),\r\n",
							"    StructField(\"Confidence Score Negative\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Positive\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Neutral\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Mixed\", FloatType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"sentiment_output_df = spark.createDataFrame([(most_common_value, most_common_count, confidence_scores['negative'], confidence_scores['positive'], confidence_scores['neutral'], confidence_scores['mixed'])], schema)\r\n",
							"\r\n",
							"display(sentiment_output_df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dir(pyspark.sql.types)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"Sentiment\",  StringType(), True),\r\n",
							"    StructField(\"Count\", IntegerType(), True),\r\n",
							"    StructField(\"Confidence Score Negative\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Positive\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Neutral\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Mixed\", FloatType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"sentiment_output_df = spark.createDataFrame([(most_common_value, most_common_count, confidence_scores['negative'], confidence_scores['positive'], confidence_scores['neutral'], confidence_scores['mixed'])], schema)\r\n",
							"\r\n",
							"display(sentiment_output_df)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Load results to the SQL Pool NATO_Sentiment table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(sentiment_output_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(\"SQLPoolTest.Sent.NATO_Sentiment\"))"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Entity Extraction"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"import pyspark.sql.functions as F\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 5\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = first_20.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = first_20.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"# define the schema for the output DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"entity_name\", StringType(), True),\r\n",
							"    StructField(\"entity_url\", StringType(), True)\r\n",
							"    ])\r\n",
							"\r\n",
							"# create an empty DataFrame with the defined schema\r\n",
							"output_df = spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(batch_size)\r\n",
							"    entity = (EntityDetector()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setLanguage(\"en\")\r\n",
							"    .setOutputCol(\"replies\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    ent_batch_output = entity.transform(batch.select(\"text\"))\r\n",
							"    #display(ent_batch_output)\r\n",
							"\r\n",
							"    # Extract the 'replies' column and convert it to JSON\r\n",
							"    json_output = ent_batch_output.select(\"replies\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'replies' field\r\n",
							"    ents = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'replies' field for each row\r\n",
							"    for each in ents:\r\n",
							"        #print(each)\r\n",
							"\r\n",
							"        # create a PySpark DataFrame from the JSON string\r\n",
							"        json_string = each\r\n",
							"        df = spark.read.json(sc.parallelize([json_string]))\r\n",
							"\r\n",
							"        # extract the 'entities' array from the 'document' column\r\n",
							"        df = df.selectExpr('explode(document.entities) as entities')\r\n",
							"\r\n",
							"        # select the 'text' and 'category' fields from the exploded 'entities' array\r\n",
							"        df = df.select('entities.name', 'entities.url')\r\n",
							"\r\n",
							"        # rename the columns\r\n",
							"        df = df.withColumnRenamed('text', 'entity_name')\r\n",
							"        df = df.withColumnRenamed('category', 'entity_url')\r\n",
							"\r\n",
							"        # show the resulting DataFrame\r\n",
							"        #df.show()\r\n",
							"\r\n",
							"        # Append the batch results dataframe to the main output dataframe\r\n",
							"        output_df = output_df.union(df)\r\n",
							"display(output_df)"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Key Phrase Extractor"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"keyPhrase = (KeyPhraseExtractor()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setLanguageCol(\"lang\")\r\n",
							"    .setOutputCol(\"replies\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"keyphrase_output = keyPhrase.transform(first_20.select(\"text\"))\r\n",
							"display(keyPhrase.transform(first_20))"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Convert output to a JSON and collect the results into one array"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"\r\n",
							"# Extract the 'sentiment' column and convert it to JSON\r\n",
							"json_output = keyphrase_output.select(\"replies\").toJSON().collect()\r\n",
							"\r\n",
							"# Deserialize the JSON and extract the 'sentiment' field\r\n",
							"keyphrases = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
							"\r\n",
							"kp_output = []\r\n",
							"\r\n",
							"# Print the 'sentiment' field for each row\r\n",
							"for each in keyphrases:\r\n",
							"    #print(each)\r\n",
							"    for keyphrase in each['document']['keyPhrases']:\r\n",
							"        kp_output.append(keyphrase)\r\n",
							"\r\n",
							"print(kp_output)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Create a dataframe for the key phrases and their count"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Create a PySpark DataFrame from the array\r\n",
							"kp_df = spark.createDataFrame([(i, 1) for i in kp_output], ['text', 'count'])\r\n",
							"\r\n",
							"# Group by 'text' column and sum the 'count' column to get the count of each unique text\r\n",
							"kp_df = kp_df.groupBy('text').agg({'count': 'sum'})\r\n",
							"\r\n",
							"# Sort the DataFrame by the 'count' column in descending order\r\n",
							"kp_df = kp_df.sort(col('sum(count)').desc())\r\n",
							"\r\n",
							"# Show the resulting DataFrame\r\n",
							"display(kp_df)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"import pyspark.sql.functions as F\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 5\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = first_20.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = first_20.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"# define the schema for the output DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"entity_name\", StringType(), True),\r\n",
							"    StructField(\"entity_url\", StringType(), True)\r\n",
							"    ])\r\n",
							"\r\n",
							"# create an empty DataFrame with the defined schema\r\n",
							"output_df = spark.createDataFrame([], schema)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Named Entity Recognition"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"import pyspark.sql.functions as F\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = first_20.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
							"num_batches = int(num_rows / 5) + (num_rows % 5 > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = first_20.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"# define the schema for the output DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"entity_text\", StringType(), True),\r\n",
							"    StructField(\"entity_category\", StringType(), True)\r\n",
							"    ])\r\n",
							"\r\n",
							"# create an empty DataFrame with the defined schema\r\n",
							"output_df = spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(5)\r\n",
							"    ner = (NER()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setLanguageCol(\"lang\")\r\n",
							"    .setOutputCol(\"replies\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    ner_batch_output = ner.transform(batch)\r\n",
							"    #display(ner_batch_output)\r\n",
							"\r\n",
							"    # Extract the 'replies' column and convert it to JSON\r\n",
							"    json_output = ner_batch_output.select(\"replies\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'replies' field\r\n",
							"    nes = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'replies' field for each row\r\n",
							"    for each in nes:\r\n",
							"        #print(each)\r\n",
							"\r\n",
							"        # create a PySpark DataFrame from the JSON string\r\n",
							"        json_string = each\r\n",
							"        df = spark.read.json(sc.parallelize([json_string]))\r\n",
							"\r\n",
							"        # extract the 'entities' array from the 'document' column\r\n",
							"        df = df.selectExpr('explode(document.entities) as entities')\r\n",
							"\r\n",
							"        # select the 'text' and 'category' fields from the exploded 'entities' array\r\n",
							"        df = df.select('entities.text', 'entities.category')\r\n",
							"\r\n",
							"        # rename the columns\r\n",
							"        df = df.withColumnRenamed('text', 'entity_text')\r\n",
							"        df = df.withColumnRenamed('category', 'entity_category')\r\n",
							"\r\n",
							"        # show the resulting DataFrame\r\n",
							"        #df.show()\r\n",
							"\r\n",
							"        # Append the batch results dataframe to the main output dataframe\r\n",
							"        output_df = output_df.union(df)\r\n",
							"display(output_df)"
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices_NamedEntityRecog')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "98df85a2-3e3d-45f9-b123-6ac95b9d4d5e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-ai-textanalytics==5.2.0"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"placeholder\""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def QueryDataSQLPool (query):\r\n",
							"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"    # Read from a query\r\n",
							"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"    df = (spark.read\r\n",
							"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                        # Defaults to storage path defined in the runtime configurations\r\n",
							"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                        # query from which data will be read\r\n",
							"                        .option(Constants.QUERY, query)\r\n",
							"                        .synapsesql()\r\n",
							"    )\r\n",
							"    return(df)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Cognitive Services credentials\r\n",
							"key = \"a3028b5f7e7a462d8a3381e49b9c976d\"\r\n",
							"endpoint = \"https://pendragon-language.cognitiveservices.azure.com/\"\r\n",
							"\r\n",
							"from azure.ai.textanalytics import TextAnalyticsClient\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"\r\n",
							"# Authenticate the client using your key and endpoint \r\n",
							"def authenticate_client():\r\n",
							"    ta_credential = AzureKeyCredential(key)\r\n",
							"    text_analytics_client = TextAnalyticsClient(\r\n",
							"            endpoint=endpoint, \r\n",
							"            credential=ta_credential)\r\n",
							"    return text_analytics_client\r\n",
							"\r\n",
							"client = authenticate_client()"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example function for recognizing entities from text\r\n",
							"def entity_recognition(client,doc):\r\n",
							"\r\n",
							"    try:\r\n",
							"        documents = doc\r\n",
							"        result = client.recognize_entities(documents = documents)[0]\r\n",
							"\r\n",
							"        entities =[]\r\n",
							"        categories = []\r\n",
							"        subcategories = []\r\n",
							"        confidencescores = []\r\n",
							"        entity_lengths = []\r\n",
							"        entity_offsets = []\r\n",
							"\r\n",
							"        #print(\"Named Entities:\\n\")\r\n",
							"        for entity in result.entities:\r\n",
							"            entities.append(entity.text)\r\n",
							"            categories.append(entity.category)\r\n",
							"            subcategories.append(entity.subcategory)\r\n",
							"            confidencescores.append(entity.confidence_score)\r\n",
							"            entity_lengths.append(entity.length)\r\n",
							"            entity_offsets.append(entity.offset)\r\n",
							"\r\n",
							"    except Exception as err:\r\n",
							"        print(\"Encountered exception. {}\".format(err))\r\n",
							"    \r\n",
							"    return(entities, categories, subcategories, confidencescores, entity_lengths, entity_offsets)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create function to run NER and output dataframe"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import *\r\n",
							"import pyspark.sql.functions as F\r\n",
							"from pyspark.sql.functions import lit"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to split input to Cogntive Services Extractive Summarization into batches of 125000 characters max\r\n",
							"def split_into_batches(input_string, max_chars_per_batch):\r\n",
							"    current_batch = \"\"\r\n",
							"    result = []\r\n",
							"    for c in input_string:\r\n",
							"        if len(current_batch) + 1 > max_chars_per_batch:\r\n",
							"            result.append(current_batch)\r\n",
							"            current_batch = c\r\n",
							"        else:\r\n",
							"            current_batch += c\r\n",
							"    if current_batch:\r\n",
							"        result.append(current_batch)\r\n",
							"    return result"
						],
						"outputs": [],
						"execution_count": 157
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"from pyspark.sql.functions import pandas_udf, col\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
						],
						"outputs": [],
						"execution_count": 158
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def NamedEntityRecognition(dataframe, topic):\r\n",
							"    if dataframe.count() > 1:\r\n",
							"        text_col = dataframe.select('cleantext')\r\n",
							"        # Collect all tweets into a list\r\n",
							"        text_list = text_col.rdd.map(lambda row: row[0]).collect()\r\n",
							"        # Join all tweets into a single document\r\n",
							"        result = \" \".join([str(text) + \".\" for text in text_list])\r\n",
							"        document = [result]\r\n",
							"        # Extract the string that contains all tweets\r\n",
							"        input_string = document[0]\r\n",
							"\r\n",
							"        batches = split_into_batches(input_string, 5120)\r\n",
							"        num_batches = len(batches)\r\n",
							"        print(num_batches)\r\n",
							"\r\n",
							"        schema = StructType([\r\n",
							"            StructField(\"entity\",  StringType(), True),\r\n",
							"            StructField(\"entity_category\", StringType(), True),\r\n",
							"            StructField(\"entity_subcategory\", StringType(), True),\r\n",
							"            StructField(\"topic\", ShortType(), True)\r\n",
							"        ])\r\n",
							"\r\n",
							"        # create a DataFrame\r\n",
							"        ner_output_df =  spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"        # Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"        for each in batches:\r\n",
							"            doc = [each]\r\n",
							"            entities, categories, subcategories, confidencescores, entity_lengths, entity_offsets = entity_recognition(client, doc)\r\n",
							"            # combine data into a list of rows\r\n",
							"            data = [(a1, a2, a3, topic) for a1, a2, a3 in zip(entities, categories, subcategories)]\r\n",
							"            batch_output_df = spark.createDataFrame([Row(*i) for i in data], schema)\r\n",
							"            #print(batch_output_df.count())\r\n",
							"\r\n",
							"            ner_output_df = ner_output_df.union(batch_output_df)\r\n",
							"        return ner_output_df\r\n",
							"\r\n",
							"    else:\r\n",
							"        return(spark.createDataFrame(['No Data','No Data','No Data', topic], schema)) \r\n",
							""
						],
						"outputs": [],
						"execution_count": 159
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_all = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment\")\r\n",
							"df_all.count()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_all_ner = NamedEntityRecognition(df_all,5)"
						],
						"outputs": [],
						"execution_count": 161
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Topic 0 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic0 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 0\")\r\n",
							"df_topic0.count()"
						],
						"outputs": [],
						"execution_count": 162
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic0_NER = NamedEntityRecognition(dataframe = df_topic0, topic = 0)"
						],
						"outputs": [],
						"execution_count": 163
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 1 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic1 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 1\")\r\n",
							"df_topic1.count()"
						],
						"outputs": [],
						"execution_count": 164
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic1_NER = NamedEntityRecognition(dataframe = df_topic1, topic = 1)"
						],
						"outputs": [],
						"execution_count": 165
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 2 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic2 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 2\")\r\n",
							"df_topic2.count()"
						],
						"outputs": [],
						"execution_count": 166
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic2_NER = NamedEntityRecognition(dataframe = df_topic2, topic = 2)"
						],
						"outputs": [],
						"execution_count": 167
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 3 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic3 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 3\")\r\n",
							"df_topic3.count()"
						],
						"outputs": [],
						"execution_count": 168
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic3_NER = NamedEntityRecognition(dataframe = df_topic3, topic = 3)"
						],
						"outputs": [],
						"execution_count": 169
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 4 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic4 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 4\")\r\n",
							"df_topic4.count()"
						],
						"outputs": [],
						"execution_count": 170
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic4_NER = NamedEntityRecognition(dataframe = df_topic4, topic = 4)"
						],
						"outputs": [],
						"execution_count": 171
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Union all Topic NER dataframes together"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# union the dataframes\r\n",
							"NameEntity_df = topic0_NER.union(topic1_NER).union(topic2_NER).union(topic3_NER).union(topic4_NER)\r\n",
							"display(NameEntity_df)"
						],
						"outputs": [],
						"execution_count": 172
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(NameEntity_df)"
						],
						"outputs": [],
						"execution_count": 173
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [NER].[NATO_NER]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [NER].[NATO_NER]\r\n",
							"(\r\n",
							"    [entity_text] NVARCHAR(100) NULL,\r\n",
							"    [entity_category] NVARCHAR(100) NULL,\r\n",
							"    [topic] INT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.\" + table"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(NameEntity_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": 176
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices_NamedEntityRecog_UsingParms')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4730a36a-fb86-47e3-9ece-ea2d64090a49"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-ai-textanalytics==5.2.0"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"NER.NATO_NER\"\n",
							"senttable = \"Sent.NATO_Sentiment\"\n",
							"\n",
							"#table = \"NER.Aithusa_NER\"\n",
							"#senttable = \"Sent.Aithusa_Sentiment\""
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def QueryDataSQLPool (query):\r\n",
							"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"    # Read from a query\r\n",
							"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"    df = (spark.read\r\n",
							"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                        # Defaults to storage path defined in the runtime configurations\r\n",
							"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                        # query from which data will be read\r\n",
							"                        .option(Constants.QUERY, query)\r\n",
							"                        .synapsesql()\r\n",
							"    )\r\n",
							"    return(df)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Cognitive Services credentials\r\n",
							"key = \"a3028b5f7e7a462d8a3381e49b9c976d\"\r\n",
							"endpoint = \"https://pendragon-language.cognitiveservices.azure.com/\"\r\n",
							"\r\n",
							"from azure.ai.textanalytics import TextAnalyticsClient\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"\r\n",
							"# Authenticate the client using your key and endpoint \r\n",
							"def authenticate_client():\r\n",
							"    ta_credential = AzureKeyCredential(key)\r\n",
							"    text_analytics_client = TextAnalyticsClient(\r\n",
							"            endpoint=endpoint, \r\n",
							"            credential=ta_credential)\r\n",
							"    return text_analytics_client\r\n",
							"\r\n",
							"client = authenticate_client()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example function for recognizing entities from text\r\n",
							"def entity_recognition(client,doc):\r\n",
							"\r\n",
							"    try:\r\n",
							"        documents = doc\r\n",
							"        result = client.recognize_entities(documents = documents)[0]\r\n",
							"\r\n",
							"        entities =[]\r\n",
							"        categories = []\r\n",
							"        subcategories = []\r\n",
							"        confidencescores = []\r\n",
							"        entity_lengths = []\r\n",
							"        entity_offsets = []\r\n",
							"\r\n",
							"        #print(\"Named Entities:\\n\")\r\n",
							"        for entity in result.entities:\r\n",
							"            entities.append(entity.text)\r\n",
							"            categories.append(entity.category)\r\n",
							"            subcategories.append(entity.subcategory)\r\n",
							"            confidencescores.append(entity.confidence_score)\r\n",
							"            entity_lengths.append(entity.length)\r\n",
							"            entity_offsets.append(entity.offset)\r\n",
							"\r\n",
							"    except Exception as err:\r\n",
							"        print(\"Encountered exception. {}\".format(err))\r\n",
							"    \r\n",
							"    return(entities, categories, subcategories, confidencescores, entity_lengths, entity_offsets)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create function to run NER and output dataframe"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import *\r\n",
							"import pyspark.sql.functions as F\r\n",
							"from pyspark.sql.functions import lit"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to split input to Cogntive Services Extractive Summarization into batches of 125000 characters max\r\n",
							"def split_into_batches(input_string, max_chars_per_batch):\r\n",
							"    current_batch = \"\"\r\n",
							"    result = []\r\n",
							"    for c in input_string:\r\n",
							"        if len(current_batch) + 1 > max_chars_per_batch:\r\n",
							"            result.append(current_batch)\r\n",
							"            current_batch = c\r\n",
							"        else:\r\n",
							"            current_batch += c\r\n",
							"    if current_batch:\r\n",
							"        result.append(current_batch)\r\n",
							"    return result"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"from pyspark.sql.functions import pandas_udf, col\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def NamedEntityRecognition(dataframe, topic):\r\n",
							"    if dataframe.count() > 1:\r\n",
							"        text_col = dataframe.select('cleantext_lemmatized')\r\n",
							"        # Collect all tweets into a list\r\n",
							"        text_list = text_col.rdd.map(lambda row: row[0]).collect()\r\n",
							"        # Join all tweets into a single document\r\n",
							"        result = \" \".join([str(text) + \".\" for text in text_list])\r\n",
							"        document = [result]\r\n",
							"        # Extract the string that contains all tweets\r\n",
							"        input_string = document[0]\r\n",
							"\r\n",
							"        batches = split_into_batches(input_string, 5120)\r\n",
							"        num_batches = len(batches)\r\n",
							"        print(num_batches)\r\n",
							"\r\n",
							"        schema = StructType([\r\n",
							"            StructField(\"entity\",  StringType(), True),\r\n",
							"            StructField(\"entity_category\", StringType(), True),\r\n",
							"            StructField(\"entity_subcategory\", StringType(), True),\r\n",
							"            StructField(\"topic\", ShortType(), True)\r\n",
							"        ])\r\n",
							"\r\n",
							"        # create a DataFrame\r\n",
							"        ner_output_df =  spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"        # Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"        for each in batches:\r\n",
							"            doc = [each]\r\n",
							"            entities, categories, subcategories, confidencescores, entity_lengths, entity_offsets = entity_recognition(client, doc)\r\n",
							"            # combine data into a list of rows\r\n",
							"            data = [(a1, a2, a3, topic) for a1, a2, a3 in zip(entities, categories, subcategories)]\r\n",
							"            batch_output_df = spark.createDataFrame([Row(*i) for i in data], schema)\r\n",
							"            #print(batch_output_df.count())\r\n",
							"\r\n",
							"            ner_output_df = ner_output_df.union(batch_output_df)\r\n",
							"        return ner_output_df\r\n",
							"\r\n",
							"    else:\r\n",
							"        return(spark.createDataFrame(['No Data','No Data','No Data', topic], schema)) \r\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import nltk \r\n",
							"nltk.download('punkt')\r\n",
							"from nltk.tokenize import sent_tokenize, word_tokenize"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_all = QueryDataSQLPool(\"select * from \"+ senttable)\r\n",
							"df_all.count()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Lemmatization of remaing words"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from nltk.stem import WordNetLemmatizer\r\n",
							"def word_lemmatizer(text):\r\n",
							"    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\r\n",
							"    return lem_text"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def lemmatize (sparkdf):\r\n",
							"    pandas_df = sparkdf.toPandas()\r\n",
							"    pandas_df['cleantext_lemmatized'] = pandas_df['cleantext'].apply(lambda x: word_tokenize(x))\r\n",
							"    nltk.download('wordnet')\r\n",
							"    pandas_df['cleantext_lemmatized'] = pandas_df['cleantext_lemmatized'].apply(lambda x: word_lemmatizer(x))\r\n",
							"    pandas_df['cleantext_lemmatized']= pandas_df['cleantext_lemmatized'].str.join(\" \")\r\n",
							"    # change back to spark df_all\r\n",
							"    spark_df = spark.createDataFrame(pandas_df)\r\n",
							"    return(spark_df)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_all = lemmatize(df_all)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_all_ner = NamedEntityRecognition(df_all,5)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_all_ner)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Topic 0 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic0 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 0\")\r\n",
							"df_topic0.count()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic0 = lemmatize(df_topic0)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic0_NER = NamedEntityRecognition(dataframe = df_topic0, topic = 0)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 1 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic1 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 1\")\r\n",
							"df_topic1.count()"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic1 = lemmatize(df_topic1)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic1_NER = NamedEntityRecognition(dataframe = df_topic1, topic = 1)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 2 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic2 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 2\")\r\n",
							"df_topic2.count()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic2 = lemmatize(df_topic2)"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic2_NER = NamedEntityRecognition(dataframe = df_topic2, topic = 2)"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 3 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic3 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 3\")\r\n",
							"df_topic3.count()"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic3 = lemmatize(df_topic3)"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic3_NER = NamedEntityRecognition(dataframe = df_topic3, topic = 3)"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 4 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic4 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 4\")\r\n",
							"df_topic4.count()"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic4 = lemmatize(df_topic4)"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic4_NER = NamedEntityRecognition(dataframe = df_topic4, topic = 4)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Union all Topic NER dataframes together"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# union the dataframes\r\n",
							"NameEntity_df = topic0_NER.union(topic1_NER).union(topic2_NER).union(topic3_NER).union(topic4_NER)\r\n",
							"display(NameEntity_df)"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(NameEntity_df)"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [NER].[NATO_NER]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [NER].[NATO_NER]\r\n",
							"(\r\n",
							"    [entity_text] NVARCHAR(100) NULL,\r\n",
							"    [entity_category] NVARCHAR(100) NULL,\r\n",
							"    [topic] INT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.\" + table"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(outputtable)"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(NameEntity_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": 44
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices_NamedEntityRecog_UsingParms_Post_Lem')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP_manual"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2888d788-3952-428f-ac21-d93d123b6fa2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-ai-textanalytics==5.2.0"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"placeholder\"\n",
							"senttable = \"Topic.NATO_Topics_LEM\"\n",
							"\n",
							"#table = \"NER.Aithusa_NER\"\n",
							"#senttable = \"Sent.Aithusa_Sentiment\""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def QueryDataSQLPool (query):\r\n",
							"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"    # Read from a query\r\n",
							"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"    df = (spark.read\r\n",
							"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                        # Defaults to storage path defined in the runtime configurations\r\n",
							"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                        # query from which data will be read\r\n",
							"                        .option(Constants.QUERY, query)\r\n",
							"                        .synapsesql()\r\n",
							"    )\r\n",
							"    return(df)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Cognitive Services credentials\r\n",
							"key = \"a3028b5f7e7a462d8a3381e49b9c976d\"\r\n",
							"endpoint = \"https://pendragon-language.cognitiveservices.azure.com/\"\r\n",
							"\r\n",
							"from azure.ai.textanalytics import TextAnalyticsClient\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"\r\n",
							"# Authenticate the client using your key and endpoint \r\n",
							"def authenticate_client():\r\n",
							"    ta_credential = AzureKeyCredential(key)\r\n",
							"    text_analytics_client = TextAnalyticsClient(\r\n",
							"            endpoint=endpoint, \r\n",
							"            credential=ta_credential)\r\n",
							"    return text_analytics_client\r\n",
							"\r\n",
							"client = authenticate_client()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example function for recognizing entities from text\r\n",
							"def entity_recognition(client,doc):\r\n",
							"\r\n",
							"    try:\r\n",
							"        documents = doc\r\n",
							"        result = client.recognize_entities(documents = documents)[0]\r\n",
							"\r\n",
							"        entities =[]\r\n",
							"        categories = []\r\n",
							"        subcategories = []\r\n",
							"        confidencescores = []\r\n",
							"        entity_lengths = []\r\n",
							"        entity_offsets = []\r\n",
							"\r\n",
							"        #print(\"Named Entities:\\n\")\r\n",
							"        for entity in result.entities:\r\n",
							"            entities.append(entity.text)\r\n",
							"            categories.append(entity.category)\r\n",
							"            subcategories.append(entity.subcategory)\r\n",
							"            confidencescores.append(entity.confidence_score)\r\n",
							"            entity_lengths.append(entity.length)\r\n",
							"            entity_offsets.append(entity.offset)\r\n",
							"\r\n",
							"    except Exception as err:\r\n",
							"        print(\"Encountered exception. {}\".format(err))\r\n",
							"    \r\n",
							"    return(entities, categories, subcategories, confidencescores, entity_lengths, entity_offsets)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create function to run NER and output dataframe"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import *\r\n",
							"import pyspark.sql.functions as F\r\n",
							"from pyspark.sql.functions import lit"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to split input to Cogntive Services Extractive Summarization into batches of 125000 characters max\r\n",
							"def split_into_batches(input_string, max_chars_per_batch):\r\n",
							"    current_batch = \"\"\r\n",
							"    result = []\r\n",
							"    for c in input_string:\r\n",
							"        if len(current_batch) + 1 > max_chars_per_batch:\r\n",
							"            result.append(current_batch)\r\n",
							"            current_batch = c\r\n",
							"        else:\r\n",
							"            current_batch += c\r\n",
							"    if current_batch:\r\n",
							"        result.append(current_batch)\r\n",
							"    return result"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"from pyspark.sql.functions import pandas_udf, col\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"schema = StructType([\n",
							"            StructField(\"entity\",  StringType(), True),\n",
							"            StructField(\"entity_category\", StringType(), True),\n",
							"            StructField(\"entity_subcategory\", StringType(), True),\n",
							"            StructField(\"topic\", ShortType(), True)\n",
							"        ])"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def NamedEntityRecognition(dataframe, topic):\r\n",
							"    if dataframe.count() > 1:\r\n",
							"        text_col = dataframe.select('cleantext')\r\n",
							"        # Collect all tweets into a list\r\n",
							"        text_list = text_col.rdd.map(lambda row: row[0]).collect()\r\n",
							"        # Join all tweets into a single document\r\n",
							"        result = \" \".join([str(text) + \".\" for text in text_list])\r\n",
							"        document = [result]\r\n",
							"        # Extract the string that contains all tweets\r\n",
							"        input_string = document[0]\r\n",
							"\r\n",
							"        batches = split_into_batches(input_string, 5120)\r\n",
							"        num_batches = len(batches)\r\n",
							"        print(num_batches)\r\n",
							"\r\n",
							"        schema = StructType([\r\n",
							"            StructField(\"entity\",  StringType(), True),\r\n",
							"            StructField(\"entity_category\", StringType(), True),\r\n",
							"            StructField(\"entity_subcategory\", StringType(), True),\r\n",
							"            StructField(\"topic\", ShortType(), True)\r\n",
							"        ])\r\n",
							"\r\n",
							"        # create a DataFrame\r\n",
							"        ner_output_df =  spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"        # Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"        for each in batches:\r\n",
							"            doc = [each]\r\n",
							"            entities, categories, subcategories, confidencescores, entity_lengths, entity_offsets = entity_recognition(client, doc)\r\n",
							"            # combine data into a list of rows\r\n",
							"            data = [(a1, a2, a3, topic) for a1, a2, a3 in zip(entities, categories, subcategories)]\r\n",
							"            batch_output_df = spark.createDataFrame([Row(*i) for i in data], schema)\r\n",
							"            #print(batch_output_df.count())\r\n",
							"\r\n",
							"            ner_output_df = ner_output_df.union(batch_output_df)\r\n",
							"        return ner_output_df\r\n",
							"\r\n",
							"    else:\r\n",
							"        return(spark.createDataFrame(['No Data','No Data','No Data', topic], schema)) \r\n",
							""
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_all = QueryDataSQLPool(\"select * from \"+ senttable)\r\n",
							"df_all.count()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_all_ner = NamedEntityRecognition(df_all,5)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Topic 0 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic0 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 0\")\r\n",
							"df_topic0.count()"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic0_NER = NamedEntityRecognition(dataframe = df_topic0, topic = 0)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 1 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic1 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 1\")\r\n",
							"df_topic1.count()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic1_NER = NamedEntityRecognition(dataframe = df_topic1, topic = 1)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 2 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic2 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 2\")\r\n",
							"df_topic2.count()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic2_NER = NamedEntityRecognition(dataframe = df_topic2, topic = 2)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 3 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic3 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 3\")\r\n",
							"df_topic3.count()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic3_NER = NamedEntityRecognition(dataframe = df_topic3, topic = 3)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 4 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic4 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 4\")\r\n",
							"df_topic4.count()"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic4_NER = NamedEntityRecognition(dataframe = df_topic4, topic = 4)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Union all Topic NER dataframes together"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# union the dataframes\r\n",
							"NameEntity_df = topic0_NER.union(topic1_NER).union(topic2_NER).union(topic3_NER).union(topic4_NER)\r\n",
							"display(NameEntity_df)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(NameEntity_df)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [NER].[NATO_NER]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [NER].[NATO_NER]\r\n",
							"(\r\n",
							"    [entity_text] NVARCHAR(100) NULL,\r\n",
							"    [entity_category] NVARCHAR(100) NULL,\r\n",
							"    [topic] INT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.\" + table"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(NameEntity_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": 27
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices_Sentiment')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "20f10e51-3e98-437f-86ca-64d964d10396"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from Topic.NATO_Topics\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"Sent.NATO_Sentiment\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sort by created_at and sample if needed for testing"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"#df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.count()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"filtered_df = df.filter(df['cleantext'].isNull() | (df['cleantext'] == ''))\r\n",
							"filtered_df.count()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"filtered1_df = df.filter(col('cleantext').isNotNull() & (col('cleantext') != ''))\r\n",
							"filtered1_df.count()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Import libraries"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Assign Linked Cognitive Service"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Perform Sentiment Analysis in Batches"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import col\r\n",
							"import pyspark.sql.functions as F\r\n",
							"import collections\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 1000\r\n",
							"print('Batch size: ', batch_size)\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = df.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of equal to the batch_size\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"print('Number of batches: ', num_batches)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = df.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"from pyspark.sql.types import *\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"id\",  LongType(), True),\r\n",
							"    StructField(\"created_at\", TimestampType(), True),\r\n",
							"    StructField(\"text\", StringType(), True),\r\n",
							"    StructField(\"cleantext\", StringType(), True),\r\n",
							"    StructField(\"lang\", StringType(), True),\r\n",
							"    StructField(\"retweet_count\", IntegerType(), True),\r\n",
							"    StructField(\"reply_count\", IntegerType(), True),\r\n",
							"    StructField(\"like_count\", IntegerType(), True),\r\n",
							"    StructField(\"quote_count\", IntegerType(), True),\r\n",
							"    StructField(\"impression_count\", IntegerType(), True),\r\n",
							"    StructField(\"topic\", ShortType(), True),\r\n",
							"    StructField(\"topic_0_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_1_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_2_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_3_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_4_confidence\", FloatType(), True),\r\n",
							"    StructField(\"sentiment\", StringType(), True),\r\n",
							"    StructField(\"negative_score\", FloatType(), True),\r\n",
							"    StructField(\"positive_score\", FloatType(), True),\r\n",
							"    StructField(\"neutral_score\", FloatType(), True),\r\n",
							"    StructField(\"mixed_score\", FloatType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"# create a DataFrame\r\n",
							"sentiment_output_df =  spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"num = 1\r\n",
							"\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(batch_size)\r\n",
							"    print('Length of batch: ', batch.count())\r\n",
							"    sent = (TextSentiment()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setTextCol(\"cleantext\")\r\n",
							"    .setOutputCol(\"sentiment\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    sent_batch_output = sent.transform(batch)\r\n",
							"\r\n",
							"    sent_batch_output = sent_batch_output.filter(col(\"error\").isNull())\r\n",
							"\r\n",
							"    # Create arrays for each original column values\r\n",
							"    id_values = []\r\n",
							"    created_at_values = []\r\n",
							"    text_values = []\r\n",
							"    cleantext_values = []\r\n",
							"    lang_values = []\r\n",
							"    retweet_count_values = []\r\n",
							"    reply_count_values = []\r\n",
							"    like_count_values = []\r\n",
							"    quote_count_values = []\r\n",
							"    impression_count_values = []\r\n",
							"    topicgroup_values = []\r\n",
							"    topic0confidence_values = []\r\n",
							"    topic1confidence_values = []\r\n",
							"    topic2confidence_values = []\r\n",
							"    topic3confidence_values = []\r\n",
							"    topic4confidence_values = []\r\n",
							"\r\n",
							"\r\n",
							"    # collect all rows of the batch DataFrame\r\n",
							"    rows = batch.collect()\r\n",
							"\r\n",
							"    # iterate over rows and extract values from each column\r\n",
							"    for row in rows:\r\n",
							"        id_values.append(row[\"id\"])\r\n",
							"        created_at_values.append(row[\"created_at\"])\r\n",
							"        text_values.append(row[\"text\"])\r\n",
							"        cleantext_values.append(row['cleantext'])\r\n",
							"        lang_values.append(row[\"lang\"])\r\n",
							"        retweet_count_values.append(row[\"retweet_count\"])\r\n",
							"        reply_count_values.append(row[\"reply_count\"])\r\n",
							"        like_count_values.append(row[\"like_count\"])\r\n",
							"        quote_count_values.append(row[\"quote_count\"])\r\n",
							"        impression_count_values.append(row[\"impression_count\"])\r\n",
							"        topicgroup_values.append(row['topic'])\r\n",
							"        topic0confidence_values.append(row['topic_0_confidence'])\r\n",
							"        topic1confidence_values.append(row['topic_1_confidence'])\r\n",
							"        topic2confidence_values.append(row['topic_2_confidence'])\r\n",
							"        topic3confidence_values.append(row['topic_3_confidence'])\r\n",
							"        topic4confidence_values.append(row['topic_4_confidence'])\r\n",
							"\r\n",
							"    # Create arrays for new column values\r\n",
							"    batch_sentiments = []\r\n",
							"    batch_negative_scores = []\r\n",
							"    batch_positive_scores = []\r\n",
							"    batch_neutral_scores = []\r\n",
							"    batch_mixed_scores = []\r\n",
							"\r\n",
							"    # Extract the 'sentiment' column and convert it to JSON\r\n",
							"    json_output = sent_batch_output.select(\"sentiment\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'sentiment' field\r\n",
							"    sentiments = [json.loads(x)[\"sentiment\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'sentiment' field for each row\r\n",
							"    for sentiment in sentiments:\r\n",
							"        if 'document' in sentiment:\r\n",
							"            batch_sentiments.append(sentiment['document']['sentiment'])\r\n",
							"            batch_negative_scores.append(sentiment['document']['confidenceScores'].get(\"negative\", 0.0))\r\n",
							"            batch_positive_scores.append(sentiment['document']['confidenceScores'].get(\"positive\", 0.0))\r\n",
							"            batch_neutral_scores.append(sentiment['document']['confidenceScores'].get(\"neutral\", 0.0))\r\n",
							"            batch_mixed_scores.append(sentiment['document']['confidenceScores'].get(\"mixed\", 0.0))\r\n",
							"        else:\r\n",
							"            batch_sentiments.append(None)\r\n",
							"            batch_negative_scores.append(0.0)\r\n",
							"            batch_positive_scores.append(0.0)\r\n",
							"            batch_neutral_scores.append(0.0)\r\n",
							"            batch_mixed_scores.append(0.0)\r\n",
							"    \r\n",
							"    # create rows\r\n",
							"    rows = [Row(id=id_values[i],\r\n",
							"                created_at=created_at_values[i],\r\n",
							"                text=text_values[i],\r\n",
							"                cleanText = cleantext_values[i],\r\n",
							"                lang=lang_values[i],\r\n",
							"                retweet_count = retweet_count_values[i],\r\n",
							"                reply_count=reply_count_values[i],\r\n",
							"                like_count=like_count_values[i],\r\n",
							"                quote_count=quote_count_values[i],\r\n",
							"                impression_count = impression_count_values[i],\r\n",
							"                topic = topicgroup_values[i],\r\n",
							"                topic_0_confidence = topic0confidence_values[i],\r\n",
							"                topic_1_confidence = topic1confidence_values[i],\r\n",
							"                topic_2_confidence = topic2confidence_values[i],\r\n",
							"                topic_3_confidence = topic3confidence_values[i],\r\n",
							"                topic_4_confidence = topic4confidence_values[i],\r\n",
							"                sentiment = batch_sentiments[i],\r\n",
							"                negative_score = batch_negative_scores[i],\r\n",
							"                positive_score = batch_positive_scores[i],\r\n",
							"                neutral_score = batch_neutral_scores[i],\r\n",
							"                mixed_score = batch_mixed_scores[i] )\r\n",
							"            for i in range(len(id_values))]\r\n",
							"\r\n",
							"    # create batch dataframe\r\n",
							"    batch_df = spark.createDataFrame(rows, schema)\r\n",
							"\r\n",
							"    # Append the batch results dataframe to the main output dataframe\r\n",
							"    sentiment_output_df = sentiment_output_df.union(batch_df)\r\n",
							"    print('Batch', num, 'succeeded!')\r\n",
							"    num = num + 1\r\n",
							"\r\n",
							"#display(sentiment_output_df)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [Sent].[NATO_Sentiment]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [Sent].[NATO_Sentiment]\r\n",
							"(\r\n",
							"    [id] bigint  NULL,\r\n",
							"\t[created_at] DATETIME2(7)  NULL,\r\n",
							"\t[text] NVARCHAR(4000)  NULL,\r\n",
							"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
							"\t[lang] nvarchar(10)  NULL,\r\n",
							"\t[retweet_count] INT  NULL,\r\n",
							"\t[reply_count] INT  NULL,\r\n",
							"\t[like_count] INT  NULL,\r\n",
							"\t[quote_count] INT  NULL,\r\n",
							"\t[impression_count] INT  NULL,\r\n",
							"    [topic] SMALLINT NULL,\r\n",
							"    [topic_0_confidence] REAL NULL,\r\n",
							"    [topic_1_confidence] REAL NULL,\r\n",
							"    [topic_2_confidence] REAL NULL,\r\n",
							"    [topic_3_confidence] REAL NULL,\r\n",
							"    [topic_4_confidence] REAL NULL,\r\n",
							"    [sentiment] NVARCHAR(20) NULL,\r\n",
							"    [negative_score] REAL NULL,\r\n",
							"    [positive_score] REAL NULL,\r\n",
							"    [neutral_score] REAL NULL,\r\n",
							"    [mixed_score] REAL NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.\" + table"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(sentiment_output_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices_Sentiment_UsingParms')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "66c58568-a09e-46e0-8f50-dbddc48fab9a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"Sent.NATO_Sentiment\"\n",
							"topictable = \"Topic.NATO_Topics\"\n",
							"\n",
							"#table = \"Sent.Aithusa_Sentiment\"\n",
							"#topictable = \"Topic.Aithusa_Topics\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Read from a query\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\n",
							"df = (spark.read\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\n",
							"                     # Defaults to storage path defined in the runtime configurations\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\n",
							"                     # query from which data will be read\n",
							"                     .option(Constants.QUERY, \"select * from \"+topictable)\n",
							"                     .synapsesql()\n",
							")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sort by created_at and sample if needed for testing"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"#df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.count()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"filtered_df = df.filter(df['cleantext'].isNull() | (df['cleantext'] == ''))\r\n",
							"filtered_df.count()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"filtered1_df = df.filter(col('cleantext').isNotNull() & (col('cleantext') != ''))\r\n",
							"filtered1_df.count()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Import libraries"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Assign Linked Cognitive Service"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Perform Sentiment Analysis in Batches"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import col\r\n",
							"import pyspark.sql.functions as F\r\n",
							"import collections\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 1000\r\n",
							"print('Batch size: ', batch_size)\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = df.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of equal to the batch_size\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"print('Number of batches: ', num_batches)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = df.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"from pyspark.sql.types import *\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"id\",  LongType(), True),\r\n",
							"    StructField(\"created_at\", TimestampType(), True),\r\n",
							"    StructField(\"text\", StringType(), True),\r\n",
							"    StructField(\"cleantext\", StringType(), True),\r\n",
							"    StructField(\"lang\", StringType(), True),\r\n",
							"    StructField(\"retweet_count\", IntegerType(), True),\r\n",
							"    StructField(\"reply_count\", IntegerType(), True),\r\n",
							"    StructField(\"like_count\", IntegerType(), True),\r\n",
							"    StructField(\"quote_count\", IntegerType(), True),\r\n",
							"    StructField(\"impression_count\", IntegerType(), True),\r\n",
							"    StructField(\"topic\", ShortType(), True),\r\n",
							"    StructField(\"topic_0_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_1_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_2_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_3_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_4_confidence\", FloatType(), True),\r\n",
							"    StructField(\"sentiment\", StringType(), True),\r\n",
							"    StructField(\"negative_score\", FloatType(), True),\r\n",
							"    StructField(\"positive_score\", FloatType(), True),\r\n",
							"    StructField(\"neutral_score\", FloatType(), True),\r\n",
							"    StructField(\"mixed_score\", FloatType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"# create a DataFrame\r\n",
							"sentiment_output_df =  spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"num = 1\r\n",
							"\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(batch_size)\r\n",
							"    print('Length of batch: ', batch.count())\r\n",
							"    sent = (TextSentiment()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setTextCol(\"cleantext\")\r\n",
							"    .setOutputCol(\"sentiment\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    sent_batch_output = sent.transform(batch)\r\n",
							"\r\n",
							"    sent_batch_output = sent_batch_output.filter(col(\"error\").isNull())\r\n",
							"\r\n",
							"    # Create arrays for each original column values\r\n",
							"    id_values = []\r\n",
							"    created_at_values = []\r\n",
							"    text_values = []\r\n",
							"    cleantext_values = []\r\n",
							"    lang_values = []\r\n",
							"    retweet_count_values = []\r\n",
							"    reply_count_values = []\r\n",
							"    like_count_values = []\r\n",
							"    quote_count_values = []\r\n",
							"    impression_count_values = []\r\n",
							"    topicgroup_values = []\r\n",
							"    topic0confidence_values = []\r\n",
							"    topic1confidence_values = []\r\n",
							"    topic2confidence_values = []\r\n",
							"    topic3confidence_values = []\r\n",
							"    topic4confidence_values = []\r\n",
							"\r\n",
							"\r\n",
							"    # collect all rows of the batch DataFrame\r\n",
							"    rows = batch.collect()\r\n",
							"\r\n",
							"    # iterate over rows and extract values from each column\r\n",
							"    for row in rows:\r\n",
							"        id_values.append(row[\"id\"])\r\n",
							"        created_at_values.append(row[\"created_at\"])\r\n",
							"        text_values.append(row[\"text\"])\r\n",
							"        cleantext_values.append(row['cleantext'])\r\n",
							"        lang_values.append(row[\"lang\"])\r\n",
							"        retweet_count_values.append(row[\"retweet_count\"])\r\n",
							"        reply_count_values.append(row[\"reply_count\"])\r\n",
							"        like_count_values.append(row[\"like_count\"])\r\n",
							"        quote_count_values.append(row[\"quote_count\"])\r\n",
							"        impression_count_values.append(row[\"impression_count\"])\r\n",
							"        topicgroup_values.append(row['topic'])\r\n",
							"        topic0confidence_values.append(row['topic_0_confidence'])\r\n",
							"        topic1confidence_values.append(row['topic_1_confidence'])\r\n",
							"        topic2confidence_values.append(row['topic_2_confidence'])\r\n",
							"        topic3confidence_values.append(row['topic_3_confidence'])\r\n",
							"        topic4confidence_values.append(row['topic_4_confidence'])\r\n",
							"\r\n",
							"    # Create arrays for new column values\r\n",
							"    batch_sentiments = []\r\n",
							"    batch_negative_scores = []\r\n",
							"    batch_positive_scores = []\r\n",
							"    batch_neutral_scores = []\r\n",
							"    batch_mixed_scores = []\r\n",
							"\r\n",
							"    # Extract the 'sentiment' column and convert it to JSON\r\n",
							"    json_output = sent_batch_output.select(\"sentiment\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'sentiment' field\r\n",
							"    sentiments = [json.loads(x)[\"sentiment\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'sentiment' field for each row\r\n",
							"    for sentiment in sentiments:\r\n",
							"        if 'document' in sentiment:\r\n",
							"            batch_sentiments.append(sentiment['document']['sentiment'])\r\n",
							"            batch_negative_scores.append(sentiment['document']['confidenceScores'].get(\"negative\", 0.0))\r\n",
							"            batch_positive_scores.append(sentiment['document']['confidenceScores'].get(\"positive\", 0.0))\r\n",
							"            batch_neutral_scores.append(sentiment['document']['confidenceScores'].get(\"neutral\", 0.0))\r\n",
							"            batch_mixed_scores.append(sentiment['document']['confidenceScores'].get(\"mixed\", 0.0))\r\n",
							"        else:\r\n",
							"            batch_sentiments.append(None)\r\n",
							"            batch_negative_scores.append(0.0)\r\n",
							"            batch_positive_scores.append(0.0)\r\n",
							"            batch_neutral_scores.append(0.0)\r\n",
							"            batch_mixed_scores.append(0.0)\r\n",
							"    \r\n",
							"    # create rows\r\n",
							"    rows = [Row(id=id_values[i],\r\n",
							"                created_at=created_at_values[i],\r\n",
							"                text=text_values[i],\r\n",
							"                cleanText = cleantext_values[i],\r\n",
							"                lang=lang_values[i],\r\n",
							"                retweet_count = retweet_count_values[i],\r\n",
							"                reply_count=reply_count_values[i],\r\n",
							"                like_count=like_count_values[i],\r\n",
							"                quote_count=quote_count_values[i],\r\n",
							"                impression_count = impression_count_values[i],\r\n",
							"                topic = topicgroup_values[i],\r\n",
							"                topic_0_confidence = topic0confidence_values[i],\r\n",
							"                topic_1_confidence = topic1confidence_values[i],\r\n",
							"                topic_2_confidence = topic2confidence_values[i],\r\n",
							"                topic_3_confidence = topic3confidence_values[i],\r\n",
							"                topic_4_confidence = topic4confidence_values[i],\r\n",
							"                sentiment = batch_sentiments[i],\r\n",
							"                negative_score = batch_negative_scores[i],\r\n",
							"                positive_score = batch_positive_scores[i],\r\n",
							"                neutral_score = batch_neutral_scores[i],\r\n",
							"                mixed_score = batch_mixed_scores[i] )\r\n",
							"            for i in range(len(id_values))]\r\n",
							"\r\n",
							"    # create batch dataframe\r\n",
							"    batch_df = spark.createDataFrame(rows, schema)\r\n",
							"\r\n",
							"    # Append the batch results dataframe to the main output dataframe\r\n",
							"    sentiment_output_df = sentiment_output_df.union(batch_df)\r\n",
							"    print('Batch', num, 'succeeded!')\r\n",
							"    num = num + 1\r\n",
							"\r\n",
							"#display(sentiment_output_df)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [Sent].[NATO_Sentiment]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [Sent].[NATO_Sentiment]\r\n",
							"(\r\n",
							"    [id] bigint  NULL,\r\n",
							"\t[created_at] DATETIME2(7)  NULL,\r\n",
							"\t[text] NVARCHAR(4000)  NULL,\r\n",
							"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
							"\t[lang] nvarchar(10)  NULL,\r\n",
							"\t[retweet_count] INT  NULL,\r\n",
							"\t[reply_count] INT  NULL,\r\n",
							"\t[like_count] INT  NULL,\r\n",
							"\t[quote_count] INT  NULL,\r\n",
							"\t[impression_count] INT  NULL,\r\n",
							"    [topic] SMALLINT NULL,\r\n",
							"    [topic_0_confidence] REAL NULL,\r\n",
							"    [topic_1_confidence] REAL NULL,\r\n",
							"    [topic_2_confidence] REAL NULL,\r\n",
							"    [topic_3_confidence] REAL NULL,\r\n",
							"    [topic_4_confidence] REAL NULL,\r\n",
							"    [sentiment] NVARCHAR(20) NULL,\r\n",
							"    [negative_score] REAL NULL,\r\n",
							"    [positive_score] REAL NULL,\r\n",
							"    [neutral_score] REAL NULL,\r\n",
							"    [mixed_score] REAL NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.\" + table"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(sentiment_output_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CogntiveService_KeyPhrases')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f50a3a9f-54c4-4ddb-af80-892ddef0b3c5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from Topic.NATO_Topics WHERE topic = 0\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 180
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 181
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import regexp_replace\r\n",
							"\r\n",
							"# Define the regular expression pattern to match words starting with @, &, or https\r\n",
							"regex = r'@\\w+|&\\w+|#\\w+'\r\n",
							"\r\n",
							"# Apply the regexp_replace function to the text column and create a new column with the filtered text\r\n",
							"df = df.withColumn(\"text\", regexp_replace(col(\"text\"), regex, ''))\r\n",
							"df = df.withColumn('text', regexp_replace('text', 'https\\\\S+', ''))\r\n",
							"\r\n",
							"# Show the filtered DataFrame\r\n",
							"df.select(\"text\").show()"
						],
						"outputs": [],
						"execution_count": 183
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 184
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 185
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"import pyspark.sql.functions as F\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 100\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = df.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = df.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"# define the schema for the output DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"entity_text\", StringType(), True),\r\n",
							"    StructField(\"entity_category\", StringType(), True)\r\n",
							"    ])\r\n",
							"\r\n",
							"kp_output = []\r\n",
							"\r\n",
							"# create an empty DataFrame with the defined schema\r\n",
							"output_df = spark.createDataFrame([], schema)\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(batch_size)\r\n",
							"    keyPhrase = (KeyPhraseExtractor()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setLanguageCol(\"lang\")\r\n",
							"    .setOutputCol(\"replies\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    kp_batch_output = keyPhrase.transform(batch)\r\n",
							"    #display(ner_batch_output)\r\n",
							"\r\n",
							"    # Extract the 'replies' column and convert it to JSON\r\n",
							"    json_output = kp_batch_output.select(\"replies\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'sentiment' field\r\n",
							"    keyphrases = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'sentiment' field for each row\r\n",
							"    for each in keyphrases:\r\n",
							"        #print(each)\r\n",
							"        for keyphrase in each['document']['keyPhrases']:\r\n",
							"            kp_output.append(keyphrase)\r\n",
							"\r\n",
							"print(kp_output)"
						],
						"outputs": [],
						"execution_count": 186
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Create a PySpark DataFrame from the array\r\n",
							"kp_df = spark.createDataFrame([(i, 1) for i in kp_output], ['text', 'count'])\r\n",
							"\r\n",
							"# Group by 'text' column and sum the 'count' column to get the count of each unique text\r\n",
							"kp_df = kp_df.groupBy('text').agg({'count': 'sum'})\r\n",
							"\r\n",
							"# Sort the DataFrame by the 'count' column in descending order\r\n",
							"kp_df = kp_df.sort(col('sum(count)').desc())\r\n",
							"\r\n",
							"# Show the resulting DataFrame\r\n",
							"display(kp_df)"
						],
						"outputs": [],
						"execution_count": 187
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CogntiveServices_Summarization')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "427f252a-4c70-420f-b574-167a8ef03b25"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Cognitive Services Extractive Summary for inputs into OpenAI for Abstractive Summary \r\n",
							"## For Each topic\r\n",
							"<hr>"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Install necessary packags and create functions"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-ai-textanalytics==5.3.0b1"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install openai"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"sum.NATO_Abstractive_Sum\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"import re\r\n",
							"from azure.ai.textanalytics import TextAnalyticsClient\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"from azure.ai.textanalytics import (TextAnalyticsClient,ExtractSummaryAction) \r\n",
							"import openai\r\n",
							"import time"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to pull data from the Dedicated SQL Pool\r\n",
							"def QueryDataSQLPool (query):\r\n",
							"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"    # Read from a query\r\n",
							"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"    df = (spark.read\r\n",
							"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                        # Defaults to storage path defined in the runtime configurations\r\n",
							"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                        # query from which data will be read\r\n",
							"                        .option(Constants.QUERY, query)\r\n",
							"                        .synapsesql()\r\n",
							"    )\r\n",
							"    return(df)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Cognitive Services credentials\r\n",
							"key = \"a3028b5f7e7a462d8a3381e49b9c976d\"\r\n",
							"endpoint = \"https://pendragon-language.cognitiveservices.azure.com/\""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Azure OpenAI API credentials \r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://pendragonopenai.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"7c4b192d51f64c09a2ca680590ccae3f\""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to split input to Cogntive Services Extractive Summarization into batches of 125000 characters max\r\n",
							"def split_into_batches(input_string, max_chars_per_batch):\r\n",
							"    current_batch = \"\"\r\n",
							"    result = []\r\n",
							"    for c in input_string:\r\n",
							"        if len(current_batch) + 1 > max_chars_per_batch:\r\n",
							"            result.append(current_batch)\r\n",
							"            current_batch = c\r\n",
							"        else:\r\n",
							"            current_batch += c\r\n",
							"    if current_batch:\r\n",
							"        result.append(current_batch)\r\n",
							"    return result"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to authenticate the client using your key and endpoint \r\n",
							"def authenticate_client():\r\n",
							"    ta_credential = AzureKeyCredential(key)\r\n",
							"    text_analytics_client = TextAnalyticsClient(\r\n",
							"            endpoint=endpoint, \r\n",
							"            credential=ta_credential)\r\n",
							"    return text_analytics_client\r\n",
							"\r\n",
							"client = authenticate_client()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Creat function to apply Exractive Summarization\r\n",
							"def sample_extractive_summarization(client,doc):\r\n",
							"\r\n",
							"    document = doc\r\n",
							"\r\n",
							"    poller = client.begin_analyze_actions(\r\n",
							"        document,\r\n",
							"        actions=[\r\n",
							"            ExtractSummaryAction(max_sentence_count=20)\r\n",
							"        ],\r\n",
							"    )\r\n",
							"    document_results = poller.result()\r\n",
							"    for result in document_results:\r\n",
							"        extract_summary_result = result[0]  # first document, first result\r\n",
							"        #if extract_summary_result.is_error:\r\n",
							"        #    print(\"...Is an error with code '{}' and message '{}'\".format(\r\n",
							"        #        extract_summary_result.code, extract_summary_result.message\r\n",
							"        #    ))\r\n",
							"        #else:\r\n",
							"            #print(\"Summary extracted: \\n{}\".format(\r\n",
							"            #    \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\r\n",
							"            #)\r\n",
							"    return(\" \".join([sentence.text for sentence in extract_summary_result.sentences]))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Extractive_Summary_Batches(input_string, max_chars_per_batch):\r\n",
							"    # Split the string into batches\r\n",
							"    result = split_into_batches(input_string, max_chars_per_batch)\r\n",
							"    print('Number of batches',len(result))\r\n",
							"    b = 1\r\n",
							"    for batch in result:\r\n",
							"        print('Characters in batch ',b, ':', len(batch))\r\n",
							"        b = b + 1\r\n",
							"\r\n",
							"    # For each batch create an extractive summary of 10 sentences\r\n",
							"    ex_summaries = []\r\n",
							"\r\n",
							"    for each in result:\r\n",
							"        #print('Batch', [each])\r\n",
							"        ex_summary = sample_extractive_summarization(client,[each])\r\n",
							"        ex_summaries.append([ex_summary])\r\n",
							"\r\n",
							"    # Prepare output for next batch\r\n",
							"    result = [' '.join(sums) for sums in ex_summaries]\r\n",
							"    document = [result]\r\n",
							"    input_string = ' '.join([string for sublst in document for string in sublst])\r\n",
							"\r\n",
							"    # Extract the string that contains all tweets\r\n",
							"    return input_string"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define dataframe schema\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"topic\", ShortType(), True),\r\n",
							"    StructField(\"abstractive_summary\", StringType(), True)\r\n",
							"    ])"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Cognitive Services Extractive to OpenAI Abstractive Summarization"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Abstractive_Summarization(dataframe, topic):\r\n",
							"    if dataframe.count() > 1:\r\n",
							"        text_col = dataframe.select('cleantext')\r\n",
							"        # Collect all tweets into a list\r\n",
							"        text_list = text_col.rdd.map(lambda row: row[0]).collect()\r\n",
							"        # Join all tweets into a single document\r\n",
							"        result = \" \".join([str(text) + \".\" for text in text_list])\r\n",
							"        document = [result]\r\n",
							"        # Extract the string that contains all tweets\r\n",
							"        input_string = document[0]\r\n",
							"\r\n",
							"        tokens = input_string.split()  # tokenize the text by splitting on whitespace\r\n",
							"        num_tokens = len(tokens)  # count the number of tokens\r\n",
							"        print(\"Number of input tokens: \", num_tokens) \r\n",
							"\r\n",
							"        loops = 1\r\n",
							"        # Run the loop\r\n",
							"        while num_tokens > 1798:\r\n",
							"            print(\"Extractive Summary\", loops)\r\n",
							"            loops = loops + 1\r\n",
							"            # Call the function and pass in the current input value\r\n",
							"            output_value = Extractive_Summary_Batches(input_string, max_chars_per_batch = 125000)\r\n",
							"            \r\n",
							"            # Print the output summary\r\n",
							"            #print(output_value)\r\n",
							"            \r\n",
							"            # Update the input value to be the output value from the previous iteration\r\n",
							"            input_string = output_value\r\n",
							"\r\n",
							"            tokens = input_string.split()  # tokenize the text by splitting on whitespace\r\n",
							"            num_tokens = len(tokens)  # count the number of tokens\r\n",
							"            print(\"Number of Batch Output Tokens: \", num_tokens)\r\n",
							"\r\n",
							"        print(\"Abstractive Summary\")\r\n",
							"\r\n",
							"        # Create one abstractive summary using the collection of abstractive summaries created above\r\n",
							"        p = 'Summarize the main idea of the following text. ' + input_string\r\n",
							"\r\n",
							"        response = openai.Completion.create(\r\n",
							"        engine=\"SummaryDivinci003\",\r\n",
							"        prompt = p,\r\n",
							"        temperature=0.3,\r\n",
							"        max_tokens=250,\r\n",
							"        top_p=1,\r\n",
							"        frequency_penalty=0,\r\n",
							"        presence_penalty=0,\r\n",
							"        best_of=1,\r\n",
							"        stop=None)\r\n",
							"    \r\n",
							"        final_ab_summarization = response['choices'][0]['text']\r\n",
							"        #print(response['choices'][0]['text'])\r\n",
							"        print(final_ab_summarization)\r\n",
							"        print('Character Length:', len(final_ab_summarization))\r\n",
							"\r\n",
							"        # Put the meta abstractive summary into a data frame and return the data frame object \r\n",
							"        return(spark.createDataFrame([(topic, final_ab_summarization)], schema))\r\n",
							"    \r\n",
							"    else:\r\n",
							"        return(spark.createDataFrame([topic, 'No Data'], schema)) "
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Test on whole dataset"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df_all = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment\")\r\n",
							"#df_all.count()"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#topic_all_summary = Abstractive_Summarization(dataframe = df_all, topic=5)\r\n",
							"#display(topic_all_summary)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 0 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic0 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 0\")\r\n",
							"df_topic0.count()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic0_summary = Abstractive_Summarization(dataframe = df_topic0, topic=0)\r\n",
							"display(topic0_summary)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"time.sleep(180)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 1 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic1 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 1\")\r\n",
							"df_topic1.count()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic1_summary = Abstractive_Summarization(dataframe = df_topic1, topic=1)\r\n",
							"display(topic1_summary)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"time.sleep(180)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic2 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 2\")\r\n",
							"df_topic2.count()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic2_summary = Abstractive_Summarization(dataframe = df_topic2, topic=2)\r\n",
							"display(topic2_summary)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"time.sleep(180)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 3 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic3 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 3\")\r\n",
							"df_topic3.count()"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic3_summary = Abstractive_Summarization(dataframe = df_topic3, topic=3)\r\n",
							"display(topic3_summary)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"time.sleep(180)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Topic 4 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic4 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 4\")\r\n",
							"df_topic4.count()"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic4_summary = Abstractive_Summarization(dataframe = df_topic4, topic=4)\r\n",
							"display(topic4_summary)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Union dataframes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# union the dataframes\r\n",
							"Topic_Summary_df = topic0_summary.union(topic1_summary).union(topic2_summary).union(topic3_summary).union(topic4_summary)\r\n",
							"display(Topic_Summary_df)"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [sum].[NATO_Abstractive_Sum]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [sum].[NATO_Abstractive_Sum]\r\n",
							"(\r\n",
							"    [abstractive_summary] NVARCHAR(4000) NULL,\r\n",
							"    [topic] SMALLINT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.\" + table"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(Topic_Summary_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": 28
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CogntiveServices_Summarization_GTP4_UsingParms')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ef54aa6b-781f-48cc-a678-25dc0d5d46d5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Cognitive Services Extractive Summary for inputs into OpenAI for Abstractive Summary \r\n",
							"## For Each topic\r\n",
							"<hr>"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Install necessary packags and create functions"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-ai-textanalytics==5.3.0b1"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install openai"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"sum.NATO_Abstractive_Sum\"\n",
							"senttable = \"Sent.NATO_Sentiment\"\n",
							"\n",
							"#table = \"sum.Aithusa_Abstractive_Sum\"\n",
							"#senttable = \"Sent.Aithusa_Sentiment\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"import re\r\n",
							"from azure.ai.textanalytics import TextAnalyticsClient\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"from azure.ai.textanalytics import (TextAnalyticsClient,ExtractSummaryAction) \r\n",
							"import openai\r\n",
							"import time"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to pull data from the Dedicated SQL Pool\r\n",
							"def QueryDataSQLPool (query):\r\n",
							"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"    # Read from a query\r\n",
							"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"    df = (spark.read\r\n",
							"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                        # Defaults to storage path defined in the runtime configurations\r\n",
							"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                        # query from which data will be read\r\n",
							"                        .option(Constants.QUERY, query)\r\n",
							"                        .synapsesql()\r\n",
							"    )\r\n",
							"    return(df)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Cognitive Services credentials\r\n",
							"key = \"a3028b5f7e7a462d8a3381e49b9c976d\"\r\n",
							"endpoint = \"https://pendragon-language.cognitiveservices.azure.com/\""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Azure OpenAI API credentials \r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://pendragonopenai.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"7c4b192d51f64c09a2ca680590ccae3f\""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to split input to Cogntive Services Extractive Summarization into batches of 125000 characters max\r\n",
							"def split_into_batches(input_string, max_chars_per_batch):\r\n",
							"    current_batch = \"\"\r\n",
							"    result = []\r\n",
							"    for c in input_string:\r\n",
							"        if len(current_batch) + 1 > max_chars_per_batch:\r\n",
							"            result.append(current_batch)\r\n",
							"            current_batch = c\r\n",
							"        else:\r\n",
							"            current_batch += c\r\n",
							"    if current_batch:\r\n",
							"        result.append(current_batch)\r\n",
							"    return result"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to authenticate the client using your key and endpoint \r\n",
							"def authenticate_client():\r\n",
							"    ta_credential = AzureKeyCredential(key)\r\n",
							"    text_analytics_client = TextAnalyticsClient(\r\n",
							"            endpoint=endpoint, \r\n",
							"            credential=ta_credential)\r\n",
							"    return text_analytics_client\r\n",
							"\r\n",
							"client = authenticate_client()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Creat function to apply Exractive Summarization\r\n",
							"def sample_extractive_summarization(client,doc):\r\n",
							"\r\n",
							"    document = doc\r\n",
							"\r\n",
							"    poller = client.begin_analyze_actions(\r\n",
							"        document,\r\n",
							"        actions=[\r\n",
							"            ExtractSummaryAction(max_sentence_count=20)\r\n",
							"        ],\r\n",
							"    )\r\n",
							"    document_results = poller.result()\r\n",
							"    for result in document_results:\r\n",
							"        extract_summary_result = result[0]  # first document, first result\r\n",
							"        #if extract_summary_result.is_error:\r\n",
							"        #    print(\"...Is an error with code '{}' and message '{}'\".format(\r\n",
							"        #        extract_summary_result.code, extract_summary_result.message\r\n",
							"        #    ))\r\n",
							"        #else:\r\n",
							"            #print(\"Summary extracted: \\n{}\".format(\r\n",
							"            #    \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\r\n",
							"            #)\r\n",
							"    return(\" \".join([sentence.text for sentence in extract_summary_result.sentences]))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Extractive_Summary_Batches(input_string, max_chars_per_batch):\r\n",
							"    # Split the string into batches\r\n",
							"    result = split_into_batches(input_string, max_chars_per_batch)\r\n",
							"    print('Number of batches',len(result))\r\n",
							"    b = 1\r\n",
							"    for batch in result:\r\n",
							"        print('Characters in batch ',b, ':', len(batch))\r\n",
							"        b = b + 1\r\n",
							"\r\n",
							"    # For each batch create an extractive summary of 10 sentences\r\n",
							"    ex_summaries = []\r\n",
							"\r\n",
							"    for each in result:\r\n",
							"        #print('Batch', [each])\r\n",
							"        ex_summary = sample_extractive_summarization(client,[each])\r\n",
							"        ex_summaries.append([ex_summary])\r\n",
							"\r\n",
							"    # Prepare output for next batch\r\n",
							"    result = [' '.join(sums) for sums in ex_summaries]\r\n",
							"    document = [result]\r\n",
							"    input_string = ' '.join([string for sublst in document for string in sublst])\r\n",
							"\r\n",
							"    # Extract the string that contains all tweets\r\n",
							"    return input_string"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define dataframe schema\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"topic\", ShortType(), True),\r\n",
							"    StructField(\"abstractive_summary\", StringType(), True)\r\n",
							"    ])"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Cognitive Services Extractive to OpenAI Abstractive Summarization"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Abstractive_Summarization(dataframe, topic):\r\n",
							"    if dataframe.count() > 1:\r\n",
							"        text_col = dataframe.select('cleantext')\r\n",
							"        # Collect all tweets into a list\r\n",
							"        text_list = text_col.rdd.map(lambda row: row[0]).collect()\r\n",
							"        # Join all tweets into a single document\r\n",
							"        result = \" \".join([str(text) + \".\" for text in text_list])\r\n",
							"        document = [result]\r\n",
							"        # Extract the string that contains all tweets\r\n",
							"        input_string = document[0]\r\n",
							"\r\n",
							"        tokens = input_string.split()  # tokenize the text by splitting on whitespace\r\n",
							"        num_tokens = len(tokens)  # count the number of tokens\r\n",
							"        print(\"Number of input tokens: \", num_tokens) \r\n",
							"\r\n",
							"        loops = 1\r\n",
							"        # Run the loop\r\n",
							"        while num_tokens > 31750:\r\n",
							"            print(\"Extractive Summary\", loops)\r\n",
							"            loops = loops + 1\r\n",
							"            # Call the function and pass in the current input value\r\n",
							"            output_value = Extractive_Summary_Batches(input_string, max_chars_per_batch = 125000)\r\n",
							"            \r\n",
							"            # Print the output summary\r\n",
							"            #print(output_value)\r\n",
							"            \r\n",
							"            # Update the input value to be the output value from the previous iteration\r\n",
							"            input_string = output_value\r\n",
							"\r\n",
							"            tokens = input_string.split()  # tokenize the text by splitting on whitespace\r\n",
							"            num_tokens = len(tokens)  # count the number of tokens\r\n",
							"            print(\"Number of Batch Output Tokens: \", num_tokens)\r\n",
							"\r\n",
							"        print(\"Abstractive Summary\")\r\n",
							"\r\n",
							"        # Create one abstractive summary using the collection of abstractive summaries created above\r\n",
							"        p = 'Summarize the main idea of the following text. ' + input_string\r\n",
							"        #print(p)\r\n",
							"\r\n",
							"        response = openai.Completion.create(\r\n",
							"        engine=\"Summary_GTP4_32k\",\r\n",
							"        prompt = p,\r\n",
							"        temperature=0.3,\r\n",
							"        max_tokens=250,\r\n",
							"        top_p=1,\r\n",
							"        frequency_penalty=0,\r\n",
							"        presence_penalty=0,\r\n",
							"        best_of=1,\r\n",
							"        stop=None)\r\n",
							"        print(response)\r\n",
							"        final_ab_summarization = response['choices'][0]['text']\r\n",
							"        #print(response['choices'][0]['text'])\r\n",
							"        print(final_ab_summarization)\r\n",
							"        print('Character Length:', len(final_ab_summarization))\r\n",
							"\r\n",
							"        # Put the meta abstractive summary into a data frame and return the data frame object \r\n",
							"        return(spark.createDataFrame([(topic, final_ab_summarization)], schema))\r\n",
							"    \r\n",
							"    else:\r\n",
							"        return(spark.createDataFrame([topic, 'No Data'], schema)) "
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Test on whole dataset"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_all = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment\")\r\n",
							"df_all.count()"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic_all_summary = Abstractive_Summarization(dataframe = df_all, topic=5)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic_all_summary = Abstractive_Summarization(dataframe = df_all, topic=5)\r\n",
							"display(topic_all_summary)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 0 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic0 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 0\")\r\n",
							"df_topic0.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic0_summary = Abstractive_Summarization(dataframe = df_topic0, topic=0)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"\r\n",
							"#topic0_summary = Abstractive_Summarization(dataframe = df_topic0, topic=0)\r\n",
							"display(topic0_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 1 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic1 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 1\")\r\n",
							"df_topic1.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic1_summary = Abstractive_Summarization(dataframe = df_topic1, topic=1)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic1_summary = Abstractive_Summarization(dataframe = df_topic1, topic=1)\r\n",
							"display(topic1_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic2 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 2\")\r\n",
							"df_topic2.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic2_summary = Abstractive_Summarization(dataframe = df_topic2, topic=2)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic2_summary = Abstractive_Summarization(dataframe = df_topic2, topic=2)\r\n",
							"display(topic2_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 3 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic3 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 3\")\r\n",
							"df_topic3.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic3_summary = Abstractive_Summarization(dataframe = df_topic3, topic=3)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic3_summary = Abstractive_Summarization(dataframe = df_topic3, topic=3)\r\n",
							"display(topic3_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Topic 4 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic4 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 4\")\r\n",
							"df_topic4.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic4_summary = Abstractive_Summarization(dataframe = df_topic4, topic=4)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic4_summary = Abstractive_Summarization(dataframe = df_topic4, topic=4)\r\n",
							"display(topic4_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Union dataframes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# union the dataframes\r\n",
							"Topic_Summary_df = topic0_summary.union(topic1_summary).union(topic2_summary).union(topic3_summary).union(topic4_summary).union(topic_all_summary)\r\n",
							"display(Topic_Summary_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [sum].[NATO_Abstractive_Sum]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [sum].[NATO_Abstractive_Sum]\r\n",
							"(\r\n",
							"    [abstractive_summary] NVARCHAR(4000) NULL,\r\n",
							"    [topic] SMALLINT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.\" + table"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(Topic_Summary_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CogntiveServices_Summarization_UsingParms')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "193ed7d1-eab4-4ddb-8454-52a0fc1b04ab"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Cognitive Services Extractive Summary for inputs into OpenAI for Abstractive Summary \r\n",
							"## For Each topic\r\n",
							"<hr>"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Install necessary packags and create functions"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-ai-textanalytics==5.3.0b1"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install openai"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"sum.NATO_Abstractive_Sum\"\n",
							"senttable = \"Sent.NATO_Sentiment\"\n",
							"\n",
							"#table = \"sum.Aithusa_Abstractive_Sum\"\n",
							"#senttable = \"Sent.Aithusa_Sentiment\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"import re\r\n",
							"from azure.ai.textanalytics import TextAnalyticsClient\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"from azure.ai.textanalytics import (TextAnalyticsClient,ExtractSummaryAction) \r\n",
							"import openai\r\n",
							"import time"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to pull data from the Dedicated SQL Pool\r\n",
							"def QueryDataSQLPool (query):\r\n",
							"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"    # Read from a query\r\n",
							"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"    df = (spark.read\r\n",
							"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                        # Defaults to storage path defined in the runtime configurations\r\n",
							"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                        # query from which data will be read\r\n",
							"                        .option(Constants.QUERY, query)\r\n",
							"                        .synapsesql()\r\n",
							"    )\r\n",
							"    return(df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Cognitive Services credentials\r\n",
							"key = \"a3028b5f7e7a462d8a3381e49b9c976d\"\r\n",
							"endpoint = \"https://pendragon-language.cognitiveservices.azure.com/\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Azure OpenAI API credentials \r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://pendragonopenai.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"7c4b192d51f64c09a2ca680590ccae3f\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to split input to Cogntive Services Extractive Summarization into batches of 125000 characters max\r\n",
							"def split_into_batches(input_string, max_chars_per_batch):\r\n",
							"    current_batch = \"\"\r\n",
							"    result = []\r\n",
							"    for c in input_string:\r\n",
							"        if len(current_batch) + 1 > max_chars_per_batch:\r\n",
							"            result.append(current_batch)\r\n",
							"            current_batch = c\r\n",
							"        else:\r\n",
							"            current_batch += c\r\n",
							"    if current_batch:\r\n",
							"        result.append(current_batch)\r\n",
							"    return result"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to authenticate the client using your key and endpoint \r\n",
							"def authenticate_client():\r\n",
							"    ta_credential = AzureKeyCredential(key)\r\n",
							"    text_analytics_client = TextAnalyticsClient(\r\n",
							"            endpoint=endpoint, \r\n",
							"            credential=ta_credential)\r\n",
							"    return text_analytics_client\r\n",
							"\r\n",
							"client = authenticate_client()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Creat function to apply Exractive Summarization\r\n",
							"def sample_extractive_summarization(client,doc):\r\n",
							"\r\n",
							"    document = doc\r\n",
							"\r\n",
							"    poller = client.begin_analyze_actions(\r\n",
							"        document,\r\n",
							"        actions=[\r\n",
							"            ExtractSummaryAction(max_sentence_count=20)\r\n",
							"        ],\r\n",
							"    )\r\n",
							"    document_results = poller.result()\r\n",
							"    for result in document_results:\r\n",
							"        extract_summary_result = result[0]  # first document, first result\r\n",
							"        #if extract_summary_result.is_error:\r\n",
							"        #    print(\"...Is an error with code '{}' and message '{}'\".format(\r\n",
							"        #        extract_summary_result.code, extract_summary_result.message\r\n",
							"        #    ))\r\n",
							"        #else:\r\n",
							"            #print(\"Summary extracted: \\n{}\".format(\r\n",
							"            #    \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\r\n",
							"            #)\r\n",
							"    return(\" \".join([sentence.text for sentence in extract_summary_result.sentences]))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Extractive_Summary_Batches(input_string, max_chars_per_batch):\r\n",
							"    # Split the string into batches\r\n",
							"    result = split_into_batches(input_string, max_chars_per_batch)\r\n",
							"    print('Number of batches',len(result))\r\n",
							"    b = 1\r\n",
							"    for batch in result:\r\n",
							"        print('Characters in batch ',b, ':', len(batch))\r\n",
							"        b = b + 1\r\n",
							"\r\n",
							"    # For each batch create an extractive summary of 10 sentences\r\n",
							"    ex_summaries = []\r\n",
							"\r\n",
							"    for each in result:\r\n",
							"        #print('Batch', [each])\r\n",
							"        ex_summary = sample_extractive_summarization(client,[each])\r\n",
							"        ex_summaries.append([ex_summary])\r\n",
							"\r\n",
							"    # Prepare output for next batch\r\n",
							"    result = [' '.join(sums) for sums in ex_summaries]\r\n",
							"    document = [result]\r\n",
							"    input_string = ' '.join([string for sublst in document for string in sublst])\r\n",
							"\r\n",
							"    # Extract the string that contains all tweets\r\n",
							"    return input_string"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define dataframe schema\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"topic\", ShortType(), True),\r\n",
							"    StructField(\"abstractive_summary\", StringType(), True)\r\n",
							"    ])"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Cognitive Services Extractive to OpenAI Abstractive Summarization"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Abstractive_Summarization(dataframe, topic):\r\n",
							"    if dataframe.count() > 1:\r\n",
							"        text_col = dataframe.select('cleantext')\r\n",
							"        # Collect all tweets into a list\r\n",
							"        text_list = text_col.rdd.map(lambda row: row[0]).collect()\r\n",
							"        # Join all tweets into a single document\r\n",
							"        result = \" \".join([str(text) + \".\" for text in text_list])\r\n",
							"        document = [result]\r\n",
							"        # Extract the string that contains all tweets\r\n",
							"        input_string = document[0]\r\n",
							"\r\n",
							"        tokens = input_string.split()  # tokenize the text by splitting on whitespace\r\n",
							"        num_tokens = len(tokens)  # count the number of tokens\r\n",
							"        print(\"Number of input tokens: \", num_tokens) \r\n",
							"\r\n",
							"        loops = 1\r\n",
							"        # Run the loop\r\n",
							"        while num_tokens > 1798:\r\n",
							"            print(\"Extractive Summary\", loops)\r\n",
							"            loops = loops + 1\r\n",
							"            # Call the function and pass in the current input value\r\n",
							"            output_value = Extractive_Summary_Batches(input_string, max_chars_per_batch = 125000)\r\n",
							"            \r\n",
							"            # Print the output summary\r\n",
							"            #print(output_value)\r\n",
							"            \r\n",
							"            # Update the input value to be the output value from the previous iteration\r\n",
							"            input_string = output_value\r\n",
							"\r\n",
							"            tokens = input_string.split()  # tokenize the text by splitting on whitespace\r\n",
							"            num_tokens = len(tokens)  # count the number of tokens\r\n",
							"            print(\"Number of Batch Output Tokens: \", num_tokens)\r\n",
							"\r\n",
							"        print(\"Abstractive Summary\")\r\n",
							"\r\n",
							"        # Create one abstractive summary using the collection of abstractive summaries created above\r\n",
							"        p = 'Summarize the main idea of the following text. ' + input_string\r\n",
							"        #print(p)\r\n",
							"\r\n",
							"        response = openai.Completion.create(\r\n",
							"        engine=\"SummaryDivinci003\",\r\n",
							"        prompt = p,\r\n",
							"        temperature=0.3,\r\n",
							"        max_tokens=250,\r\n",
							"        top_p=1,\r\n",
							"        frequency_penalty=0,\r\n",
							"        presence_penalty=0,\r\n",
							"        best_of=1,\r\n",
							"        stop=None)\r\n",
							"        print(response)\r\n",
							"        final_ab_summarization = response['choices'][0]['text']\r\n",
							"        #print(response['choices'][0]['text'])\r\n",
							"        print(final_ab_summarization)\r\n",
							"        print('Character Length:', len(final_ab_summarization))\r\n",
							"\r\n",
							"        # Put the meta abstractive summary into a data frame and return the data frame object \r\n",
							"        return(spark.createDataFrame([(topic, final_ab_summarization)], schema))\r\n",
							"    \r\n",
							"    else:\r\n",
							"        return(spark.createDataFrame([topic, 'No Data'], schema)) "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Test on whole dataset"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_all = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment\")\r\n",
							"df_all.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic_all_summary = Abstractive_Summarization(dataframe = df_all, topic=5)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic_all_summary = Abstractive_Summarization(dataframe = df_all, topic=5)\r\n",
							"display(topic_all_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 0 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic0 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 0\")\r\n",
							"df_topic0.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic0_summary = Abstractive_Summarization(dataframe = df_topic0, topic=0)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"\r\n",
							"#topic0_summary = Abstractive_Summarization(dataframe = df_topic0, topic=0)\r\n",
							"display(topic0_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 1 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic1 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 1\")\r\n",
							"df_topic1.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic1_summary = Abstractive_Summarization(dataframe = df_topic1, topic=1)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic1_summary = Abstractive_Summarization(dataframe = df_topic1, topic=1)\r\n",
							"display(topic1_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic2 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 2\")\r\n",
							"df_topic2.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic2_summary = Abstractive_Summarization(dataframe = df_topic2, topic=2)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic2_summary = Abstractive_Summarization(dataframe = df_topic2, topic=2)\r\n",
							"display(topic2_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 3 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic3 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 3\")\r\n",
							"df_topic3.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic3_summary = Abstractive_Summarization(dataframe = df_topic3, topic=3)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic3_summary = Abstractive_Summarization(dataframe = df_topic3, topic=3)\r\n",
							"display(topic3_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Topic 4 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic4 = QueryDataSQLPool(\"select * from \" + senttable + \" where topic = 4\")\r\n",
							"df_topic4.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import time\r\n",
							"\r\n",
							"max_retries = 3\r\n",
							"retry_count = 0\r\n",
							"\r\n",
							"while retry_count < max_retries:\r\n",
							"    try:\r\n",
							"        # Perform the task here\r\n",
							"        topic4_summary = Abstractive_Summarization(dataframe = df_topic4, topic=4)\r\n",
							"        print(\"Task successfully completed\")\r\n",
							"        break  # Exit the loop if the task was completed successfully\r\n",
							"    except Exception as e:\r\n",
							"        print(f\"Error occurred: {str(e)}\")\r\n",
							"        retry_count += 1\r\n",
							"        if retry_count < max_retries:\r\n",
							"            print(f\"Retrying in 1 minute... (attempt {retry_count+1}/{max_retries})\")\r\n",
							"            time.sleep(60)  # Wait for 1 minute before trying again\r\n",
							"        else:\r\n",
							"            print(f\"Max retries ({max_retries}) reached. Exiting...\")\r\n",
							"#topic4_summary = Abstractive_Summarization(dataframe = df_topic4, topic=4)\r\n",
							"display(topic4_summary)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Union dataframes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# union the dataframes\r\n",
							"Topic_Summary_df = topic0_summary.union(topic1_summary).union(topic2_summary).union(topic3_summary).union(topic4_summary).union(topic_all_summary)\r\n",
							"display(Topic_Summary_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [sum].[NATO_Abstractive_Sum]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [sum].[NATO_Abstractive_Sum]\r\n",
							"(\r\n",
							"    [abstractive_summary] NVARCHAR(4000) NULL,\r\n",
							"    [topic] SMALLINT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.\" + table"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(Topic_Summary_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NLP_manual_post_sent')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP_manual"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "badca07a-3f42-47c7-bbfd-4cc483276296"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reading SQL table into a dataframe. \n",
							"\n",
							"# Add required imports\n",
							"import com.microsoft.spark.sqlanalytics\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\n",
							"from pyspark.sql.functions import col\n",
							"\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\n",
							"\n",
							"# Read from a query\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\n",
							"df = (spark.read\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\n",
							"                     # Defaults to storage path defined in the runtime configurations\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\n",
							"                     # query from which data will be read\n",
							"                     .option(Constants.QUERY, \"select * from Sent.NATO_Sentiment\")\n",
							"                     .synapsesql())"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.show()"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.groupBy(\"topic\").count().show()"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyspark.sql.functions as F\n",
							"\n",
							"df2 = df.groupBy('topic').agg(F.collect_list('cleantext').alias('cleantext'))"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df2.show()"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df2.sort(df2.topic.asc()).show()"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install transformers"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install wordcloud"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\n",
							"import numpy as np\n",
							"import pandas as pd\n",
							"from os import path\n",
							"from PIL import Image\n",
							"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
							""
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import matplotlib.pyplot as plt\n",
							"%matplotlib inline"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col, concat_ws\n",
							"df3 = df2.withColumn(\"cleantext\",\n",
							"   concat_ws(\",\",col(\"cleantext\")))\n",
							"df3.printSchema()\n",
							"df3.show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df3_sorted = df3.sort(df3.topic.asc())\n",
							"df3_sorted.show()\n",
							""
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df3_sorted.dtypes\n",
							""
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Each topic\n",
							"topic0text = df3_sorted.collect()[0]['cleantext']\n",
							"topic1text = df3_sorted.collect()[1]['cleantext']\n",
							"topic2text = df3_sorted.collect()[2]['cleantext']\n",
							"topic3text = df3_sorted.collect()[3]['cleantext']\n",
							"topic4text = df3_sorted.collect()[4]['cleantext']\n",
							""
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stopwords = set(STOPWORDS)\n",
							"# Create and generate a word cloud image:\n",
							"wordcloud_topic0 = WordCloud(max_font_size=80, max_words=1000, background_color='white', stopwords=stopwords,\n",
							"               min_font_size=10, colormap='Dark2').generate(str(topic0text))\n",
							"\n",
							"# Display the generated image:\n",
							"plt.imshow(wordcloud_topic0, interpolation='bilinear')\n",
							"plt.axis(\"off\")\n",
							"plt.show()\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create and generate a word cloud image:\n",
							"wordcloud_topic1 = WordCloud(max_font_size=80, max_words=1000, background_color='white', stopwords=stopwords,\n",
							"               min_font_size=10, colormap='Dark2').generate(topic1text)\n",
							"\n",
							"# Display the generated image:\n",
							"plt.imshow(wordcloud_topic1, interpolation='bilinear')\n",
							"plt.axis(\"off\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 73
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create and generate a word cloud image:\n",
							"wordcloud_topic2 = WordCloud(max_font_size=80, max_words=1000, background_color='white', stopwords=stopwords,\n",
							"               min_font_size=10, colormap='Dark2').generate(topic2text)\n",
							"\n",
							"# Display the generated image:\n",
							"plt.imshow(wordcloud_topic2, interpolation='bilinear')\n",
							"plt.axis(\"off\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create and generate a word cloud image:\n",
							"wordcloud_topic3 = WordCloud(max_font_size=80, max_words=1000, background_color='white', stopwords=stopwords,\n",
							"               min_font_size=10, colormap='Dark2').generate(topic3text)\n",
							"\n",
							"# Display the generated image:\n",
							"plt.imshow(wordcloud_topic3, interpolation='bilinear')\n",
							"plt.axis(\"off\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 75
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create and generate a word cloud image:\n",
							"wordcloud_topic4 = WordCloud(max_font_size=80, max_words=1000, background_color='white', stopwords=stopwords,\n",
							"               min_font_size=10, colormap='Dark2').generate(topic4text)\n",
							"\n",
							"# Display the generated image:\n",
							"plt.imshow(wordcloud_topic4, interpolation='bilinear')\n",
							"plt.axis(\"off\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 76
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from transformers import pipeline\n",
							""
						],
						"outputs": [],
						"execution_count": 77
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"summarizer = pipeline(\"summarization\")"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#def my_string(value):\n",
							"#    firstString = ''\n",
							"#    secondString = ''\n",
							"#    thirdString = ''\n",
							"#    fourthString = ''\n",
							"#    for i in range(0,len(value)):\n",
							"#        if i <= len(value)//4-1:\n",
							"#            firstString = firstString + value[i]\n",
							"#        if i <= len(value)//3-1:\n",
							"#            secondString = secondString + value[i]\n",
							"#        if i <= len(value)//2-1:\n",
							"#            thirdString = thirdString + value[i]\n",
							"#        else:\n",
							"#            fourthString = fourthString + value[i]\n",
							"#    return firstString, secondString, thirdString, fourthString\n",
							"\n",
							"#split_string = my_string(topic0text)"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#print(split_string)\n",
							"\n",
							"#fourth1 = split_string[0]\n",
							"#fourth2 = split_string[1]\n",
							"#fourth3 = split_string[2]\n",
							"#fourth4 = split_string[3]\n",
							""
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"summary_text1 = summarizer(third1, max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
							"summary_text2 = summarizer(third2, max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
							"summary_text3 = summarizer(third3, max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
							"\n",
							"abstractive_topic0 = summary_text1 + summary_text2 + summary_text3"
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"summary_text = summarizer(topic0text, max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
							"print(summary_text)"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install spacy"
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import spacy\n",
							"from spacy.lang.en.stop_words import STOP_WORDS\n",
							"from string import punctuation\n",
							"from heapq import nlargest"
						],
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Importing requirements\n",
							"from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration"
						],
						"outputs": [],
						"execution_count": 70
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install transformers==2.8.0\n",
							"!pip install torch==1.4.0"
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
							"model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
							"tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
							"device = torch.device('cpu')\n",
							""
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install transformers==2.9.0 \n",
							"!pip install pytorch_lightning==0.7.5"
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install sentencepiece==0.1.91"
						],
						"outputs": [],
						"execution_count": 77
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"text = \"summarize:\" + topic0text\n",
							"text"
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"input_ids=tokenizer.encode(text, return_tensors='pt', max_length=512)"
						],
						"outputs": [],
						"execution_count": 75
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import nltk\n",
							"from nltk.tokenize import word_tokenize\n",
							"from nltk.tag import pos_tag"
						],
						"outputs": [],
						"execution_count": 85
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def preprocess(sent):\n",
							"    sent = nltk.word_tokenize(sent)\n",
							"    sent = nltk.pos_tag(sent)\n",
							"    return sent"
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sent = preprocess(topic0text)\n",
							"sent"
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#import nltk\n",
							"#nltk.download('punkt')\n",
							"nltk.download('averaged_perceptron_tagger')"
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pattern = 'NP: {<DT>?<JJ>*<NN>}'"
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cp = nltk.RegexpParser(pattern)\n",
							"cs = cp.parse(sent)\n",
							"print(cs)"
						],
						"outputs": [],
						"execution_count": 93
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import spacy\n",
							"from spacy import displacy\n",
							"from collections import Counter\n",
							"import en_core_web_sm\n",
							"nlp = spacy.load(\"en_core_web_sm\")"
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"nlp = spacy.load(\"en_core_web_sm\")"
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"python -m spacy download en_core_web_sm"
						],
						"outputs": [],
						"execution_count": 99
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OpenAI')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "OpenAI"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d3b6bdd8-d976-46ee-8196-36d7bdeb1643"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sort dataframe by date column in ascending order\r\n",
							"sorted_df = df.orderBy(col(\"impression_count\").desc())\r\n",
							"\r\n",
							"# show sorted dataframe\r\n",
							"sorted_df.show()"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# select the first 20 records\r\n",
							"first_20 = sorted_df.limit(20)\r\n",
							"first_20.show()"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import collect_list, concat_ws\r\n",
							"\r\n",
							"# concatenate strings in \"text\" column into a single string\r\n",
							"concatenated_string = first_20.agg(concat_ws(\"\", collect_list(\"text\"))).collect()[0][0]\r\n",
							"\r\n",
							"print(concatenated_string)"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark import SparkContext\r\n",
							"sc = SparkContext.getOrCreate()\r\n",
							"\r\n",
							"input_string = concatenated_string\r\n",
							"\r\n",
							"def remove_emoji(text):\r\n",
							"    return text.encode('ascii', 'ignore').decode('ascii')\r\n",
							"result_string = remove_emoji(input_string)\r\n",
							"\r\n",
							"words_rdd = sc.parallelize(result_string.split())\r\n",
							"filtered_words_rdd = words_rdd.filter(lambda word: not word.startswith(\"#\"))\r\n",
							"result_string = \" \".join(filtered_words_rdd.collect())\r\n",
							"\r\n",
							"words_rdd = sc.parallelize(result_string.split())\r\n",
							"filtered_words_rdd = words_rdd.filter(lambda word: not word.startswith(\"@\"))\r\n",
							"result_string = \" \".join(filtered_words_rdd.collect())\r\n",
							"\r\n",
							"words_rdd = sc.parallelize(result_string.split())\r\n",
							"filtered_words_rdd = words_rdd.filter(lambda word: not word.startswith(\"&\"))\r\n",
							"result_string = \" \".join(filtered_words_rdd.collect())\r\n",
							"\r\n",
							"words_rdd = sc.parallelize(result_string.split())\r\n",
							"filtered_words_rdd = words_rdd.filter(lambda word: not word.startswith(\"https\"))\r\n",
							"result_string = \" \".join(filtered_words_rdd.collect())\r\n",
							"\r\n",
							"print(result_string)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#!pip install openai\r\n",
							"import openai\r\n",
							"\r\n",
							"\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\""
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p = 'Provide a summary of the text below that captures the main idea. ' + result_string"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Note: The openai-python library support for Azure OpenAI is in preview.\r\n",
							"import os\r\n",
							"import openai\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\"\r\n",
							"\r\n",
							"# query = input(\"Enter your query in natural language: \")\r\n",
							"\r\n",
							"response = openai.Completion.create(\r\n",
							"  engine=\"SumTest\",\r\n",
							"  prompt = p,\r\n",
							"  temperature=0.3,\r\n",
							"  max_tokens=250,\r\n",
							"  top_p=1,\r\n",
							"  frequency_penalty=0,\r\n",
							"  presence_penalty=0,\r\n",
							"  best_of=1,\r\n",
							"  stop=None)\r\n",
							"\r\n",
							"response"
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p = 'Perform key phrase extraction on the following text. ' + result_string"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Note: The openai-python library support for Azure OpenAI is in preview.\r\n",
							"import os\r\n",
							"import openai\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\"\r\n",
							"\r\n",
							"# query = input(\"Enter your query in natural language: \")\r\n",
							"\r\n",
							"response = openai.Completion.create(\r\n",
							"  engine=\"SumTest\",\r\n",
							"  prompt = p,\r\n",
							"  temperature=0.3,\r\n",
							"  max_tokens=250,\r\n",
							"  top_p=1,\r\n",
							"  frequency_penalty=0,\r\n",
							"  presence_penalty=0,\r\n",
							"  best_of=1,\r\n",
							"  stop=None)\r\n",
							"\r\n",
							"response"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p = 'Perform Named Entity Recognition on the following text. ' + result_string"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Note: The openai-python library support for Azure OpenAI is in preview.\r\n",
							"import os\r\n",
							"import openai\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\"\r\n",
							"\r\n",
							"# query = input(\"Enter your query in natural language: \")\r\n",
							"\r\n",
							"response = openai.Completion.create(\r\n",
							"  engine=\"SumTest\",\r\n",
							"  prompt = p,\r\n",
							"  temperature=0.3,\r\n",
							"  max_tokens=250,\r\n",
							"  top_p=1,\r\n",
							"  frequency_penalty=0,\r\n",
							"  presence_penalty=0,\r\n",
							"  best_of=1,\r\n",
							"  stop=None)\r\n",
							"\r\n",
							"response"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p = 'What is the overall sentiment of the following text as a percentage. ' + result_string"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Note: The openai-python library support for Azure OpenAI is in preview.\r\n",
							"import os\r\n",
							"import openai\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\"\r\n",
							"\r\n",
							"# query = input(\"Enter your query in natural language: \")\r\n",
							"\r\n",
							"response = openai.Completion.create(\r\n",
							"  engine=\"SumTest\",\r\n",
							"  prompt = p,\r\n",
							"  temperature=0.3,\r\n",
							"  max_tokens=250,\r\n",
							"  top_p=1,\r\n",
							"  frequency_penalty=0,\r\n",
							"  presence_penalty=0,\r\n",
							"  best_of=1,\r\n",
							"  stop=None)\r\n",
							"\r\n",
							"response"
						],
						"outputs": [],
						"execution_count": 65
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ReadWriteSQLPool')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ReadWrite"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "edb745b2-f362-423c-b40e-3a7b156d10bf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Read from a query using Azure AD based authentication\r\n",
							"\r\n",
							"https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export?tabs=python%2Cpython1%2Cscala2%2Cscala3%2Cscala4%2Cscala5"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"dfToReadFromQueryAsOption = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")\r\n",
							"\r\n",
							"dfToReadFromQueryAsArgument = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .synapsesql(\"select * from dbo.NATO_Tweets1\")\r\n",
							")\r\n",
							"\r\n",
							"# Show contents of the dataframe\r\n",
							"dfToReadFromQueryAsOption.show()\r\n",
							"dfToReadFromQueryAsArgument.show()"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = dfToReadFromQueryAsArgument"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Text Analytics\r\n",
							"https://learn.microsoft.com/en-us/azure/synapse-analytics/machine-learning/tutorial-text-analytics-use-mmlspark"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a dataframe that's tied to it's column names\r\n",
							"df = spark.createDataFrame([\r\n",
							"  (\"Hello World\",),\r\n",
							"  (\"Bonjour tout le monde\",),\r\n",
							"  (\"La carretera estaba atascada. Haba mucho trfico el da de ayer.\",),\r\n",
							"  (\"\",),\r\n",
							"  (\"\",),\r\n",
							"  (\":) :( :D\",)\r\n",
							"], [\"text\",])\r\n",
							"\r\n",
							"# Run the Text Analytics service with options\r\n",
							"language = (LanguageDetector()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setTextCol(\"text\")\r\n",
							"    .setOutputCol(\"language\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"# Show the results of your text query in a table format\r\n",
							"display(language.transform(df))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Remove special characters"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import regexp_replace\r\n",
							"\r\n",
							"# remove special characters from text column\r\n",
							"df = df.withColumn(\"text\", regexp_replace(df[\"text\"], \"[^a-zA-Z0-9\\\\s]\", \"\"))\r\n",
							"\r\n",
							"df_text = df.select('text')\r\n",
							"\r\n",
							"# show resulting dataframe\r\n",
							"df_text.show()"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Write back to SQLPoolTest dbo.NATO_Tweets1 table using Azure AD based authentication"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(\"SQLPoolTest.dbo.NATO_Tweets1\"))"
						],
						"outputs": [],
						"execution_count": 44
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TopicModeling_LDA')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "be52c08f-542b-480b-ac0e-075ba2153003"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Perform Topic Modeling using Spark Machine Learning Clustering LDA Model\r\n",
							"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.LDA.html"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"placeholder\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sort by created_at and sample if needed"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"#df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Clean data\r\n",
							"Remove special characters, URLs, hashtags, mentions, emojis, and stopwords"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import re\r\n",
							"\r\n",
							"# Define regular expressions for special characters, URLs, hashtags, mentions, and emojis\r\n",
							"special_chars_regex = r'[^\\w\\s]'\r\n",
							"url_regex = r'https?://\\S+'\r\n",
							"mentions_hashtag_regex = r'@\\w+|#\\w+'\r\n",
							"emoji_regex = r'[^\\w\\s\\ufe0f]+'\r\n",
							"extra_spaces_regex = r'\\s{2,}'\r\n",
							"stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\r\n",
							"                 \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\",\r\n",
							"                 \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\r\n",
							"                 \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\r\n",
							"                 \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\r\n",
							"                 \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\r\n",
							"                 \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\",\r\n",
							"                 \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\r\n",
							"                 \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\r\n",
							"                 \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\r\n",
							"                 \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\",\r\n",
							"                 \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\",\r\n",
							"                 \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\",\r\n",
							"                 \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\",\r\n",
							"                 \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\",\r\n",
							"                 \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\",\r\n",
							"                 \"just\", \"don\", \"should\", \"now\"]\r\n",
							"\r\n",
							"def clean_text(text):\r\n",
							"    # Convert text to lowercase\r\n",
							"    text = text.lower()\r\n",
							"    # Remove URLs\r\n",
							"    text = re.sub(url_regex, '', text)\r\n",
							"    # Remove mentions and hashtags\r\n",
							"    text = re.sub(mentions_hashtag_regex, '', text)\r\n",
							"    # Remove special characters\r\n",
							"    text = re.sub(special_chars_regex, '', text)\r\n",
							"    # Remove emojis\r\n",
							"    text = re.sub(emoji_regex, '', text)\r\n",
							"    # Remove empty spaces equal to or bigger than 2 spaces\r\n",
							"    text = re.sub(extra_spaces_regex, ' ', text)\r\n",
							"    text = ' '.join([word for word in text.split() if word.lower() not in stopwords])\r\n",
							"    # Remove empty spaces\r\n",
							"    text = ' '.join(text.split())\r\n",
							"    return text\r\n",
							"\r\n",
							"# Apply the function to the \"text\" column of the DataFrame\r\n",
							"from pyspark.sql.functions import udf\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"\r\n",
							"clean_text_udf = udf(clean_text, StringType())\r\n",
							"\r\n",
							"df = df.withColumn(\"cleanText\", clean_text_udf(\"text\"))\r\n",
							"\r\n",
							"#display(df)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Topic Modeling\r\n",
							"## Pyspark ML Clustering LDA (Latent Dirichlet Allocation)\r\n",
							"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.LDA.html\r\n",
							"## 5 Topics"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.ml.clustering import LDA\r\n",
							"from pyspark.ml.feature import Tokenizer, CountVectorizer\r\n",
							"\r\n",
							"tokenizer = Tokenizer(inputCol=\"cleanText\", outputCol=\"tokens\")\r\n",
							"df_tokens = tokenizer.transform(df)\r\n",
							"\r\n",
							"# create document-term matrix\r\n",
							"vectorizer = CountVectorizer(inputCol='tokens', outputCol='features')\r\n",
							"model = vectorizer.fit(df_tokens)\r\n",
							"df_features = model.transform(df_tokens)\r\n",
							"\r\n",
							"# fit LDA model\r\n",
							"num_topics = 5\r\n",
							"max_iterations = 10\r\n",
							"lda = LDA(k=num_topics, maxIter=max_iterations)\r\n",
							"lda_model = lda.fit(df_features)\r\n",
							"\r\n",
							"# get topic distribution for each document\r\n",
							"df_topics = lda_model.transform(df_features).select('id', 'topicDistribution')\r\n",
							"\r\n",
							"# join with original DataFrame\r\n",
							"df_result = df.join(df_topics, 'id')\r\n",
							"\r\n",
							"#display(df_result)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Parse topicDistribution output column for the topic group and confidence values"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"import numpy as np\r\n",
							"\r\n",
							"# Extract the 'replies' column and convert it to JSON\r\n",
							"json_output = df_result.select(\"topicDistribution\").toJSON().collect()\r\n",
							"\r\n",
							"# Deserialize the JSON and extract the 'replies' field\r\n",
							"topics = [json.loads(x)[\"topicDistribution\"] for x in json_output]\r\n",
							"\r\n",
							"topicgroup = []\r\n",
							"topic0confidence = []\r\n",
							"topic1confidence = []\r\n",
							"topic2confidence = []\r\n",
							"topic3confidence = []\r\n",
							"topic4confidence = []\r\n",
							"\r\n",
							"# Print the 'replies' field for each row\r\n",
							"for topic in topics:\r\n",
							"    #print(topic)\r\n",
							"    \r\n",
							"    # convert JSON array to numpy array\r\n",
							"    arr = np.array(topic['values'])\r\n",
							"    topic0confidence.append(arr[0])\r\n",
							"    topic1confidence.append(arr[1])\r\n",
							"    topic2confidence.append(arr[2])\r\n",
							"    topic3confidence.append(arr[3])\r\n",
							"    topic4confidence.append(arr[4])\r\n",
							"\r\n",
							"    # find index of maximum value\r\n",
							"    idx = np.argmax(arr)\r\n",
							"    #print(idx)\r\n",
							"    topicgroup.append(idx)\r\n",
							"\r\n",
							"#print(topicgroup)\r\n",
							"#print(topic0confidence)\r\n",
							"#print(topic1confidence)\r\n",
							"#print(topic2confidence)\r\n",
							"#print(topic3confidence)\r\n",
							"#print(topic4confidence)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Collect the original dataframe column values"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# assume df is the DataFrame to extract values from\r\n",
							"id_values = []\r\n",
							"created_at_values = []\r\n",
							"text_values = []\r\n",
							"cleantext_values = []\r\n",
							"lang_values = []\r\n",
							"retweet_count_values = []\r\n",
							"reply_count_values = []\r\n",
							"like_count_values = []\r\n",
							"quote_count_values = []\r\n",
							"impression_count_values = []\r\n",
							"\r\n",
							"# collect all rows of the DataFrame\r\n",
							"rows = df.collect()\r\n",
							"\r\n",
							"# iterate over rows and extract values from each column\r\n",
							"for row in rows:\r\n",
							"    id_values.append(row[\"id\"])\r\n",
							"    created_at_values.append(row[\"created_at\"])\r\n",
							"    text_values.append(row[\"text\"])\r\n",
							"    lang_values.append(row[\"lang\"])\r\n",
							"    retweet_count_values.append(row[\"retweet_count\"])\r\n",
							"    reply_count_values.append(row[\"reply_count\"])\r\n",
							"    like_count_values.append(row[\"like_count\"])\r\n",
							"    quote_count_values.append(row[\"quote_count\"])\r\n",
							"    impression_count_values.append(row[\"impression_count\"])\r\n",
							"    cleantext_values.append(row['cleanText'])"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Convert new values to proper int of float data types"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert nump.int64 to int and numpy.float64 to float\r\n",
							"topicgroup = [int(i) for i in topicgroup]\r\n",
							"topic0confidence = [float(i) for i in topic0confidence]\r\n",
							"topic1confidence = [float(i) for i in topic1confidence]\r\n",
							"topic2confidence = [float(i) for i in topic2confidence]\r\n",
							"topic3confidence = [float(i) for i in topic3confidence]\r\n",
							"topic4confidence = [float(i) for i in topic4confidence]"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create a final output dataframe by mapping the values to their proper schema "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"id\",  LongType(), True),\r\n",
							"    StructField(\"created_at\", TimestampType(), True),\r\n",
							"    StructField(\"text\", StringType(), True),\r\n",
							"    StructField(\"cleantext\", StringType(), True),\r\n",
							"    StructField(\"lang\", StringType(), True),\r\n",
							"    StructField(\"retweet_count\", IntegerType(), True),\r\n",
							"    StructField(\"reply_count\", IntegerType(), True),\r\n",
							"    StructField(\"like_count\", IntegerType(), True),\r\n",
							"    StructField(\"quote_count\", IntegerType(), True),\r\n",
							"    StructField(\"impression_count\", IntegerType(), True),\r\n",
							"    StructField(\"topic\", ShortType(), True),\r\n",
							"    StructField(\"topic_0_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_1_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_2_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_3_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_4_confidence\", FloatType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"# create rows\r\n",
							"rows = [Row(id=id_values[i],\r\n",
							"            created_at=created_at_values[i],\r\n",
							"            text=text_values[i],\r\n",
							"            cleanText = cleantext_values[i],\r\n",
							"            lang=lang_values[i],\r\n",
							"            retweet_count = retweet_count_values[i],\r\n",
							"            reply_count=reply_count_values[i],\r\n",
							"            like_count=like_count_values[i],\r\n",
							"            quote_count=quote_count_values[i],\r\n",
							"            impression_count = impression_count_values[i],\r\n",
							"            topic = topicgroup[i],\r\n",
							"            topic_0_confidence = topic0confidence[i],\r\n",
							"            topic_1_confidence = topic1confidence[i],\r\n",
							"            topic_2_confidence = topic2confidence[i],\r\n",
							"            topic_3_confidence = topic3confidence[i],\r\n",
							"            topic_4_confidence = topic4confidence[i])\r\n",
							"        for i in range(len(id_values))]\r\n",
							"\r\n",
							"# create a DataFrame\r\n",
							"final_topic_df = spark.createDataFrame(rows, schema)\r\n",
							"\r\n",
							"# show the DataFrame\r\n",
							"display(final_topic_df)"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [Topic].[NATO_Topics]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [Topic].[NATO_Topics]\r\n",
							"( \r\n",
							"\t[id] bigint  NULL,\r\n",
							"\t[created_at] DATETIME2(7)  NULL,\r\n",
							"\t[text] NVARCHAR(4000)  NULL,\r\n",
							"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
							"\t[lang] nvarchar(10)  NULL,\r\n",
							"\t[retweet_count] INT  NULL,\r\n",
							"\t[reply_count] INT  NULL,\r\n",
							"\t[like_count] INT  NULL,\r\n",
							"\t[quote_count] INT  NULL,\r\n",
							"\t[impression_count] INT  NULL,\r\n",
							"    [topic] SMALLINT NULL,\r\n",
							"    [topic_0_confidence] REAL NULL,\r\n",
							"    [topic_1_confidence] REAL NULL,\r\n",
							"    [topic_2_confidence] REAL NULL,\r\n",
							"    [topic_3_confidence] REAL NULL,\r\n",
							"    [topic_4_confidence] REAL NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtable = \"SQLPoolTest.\" + table"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(final_topic_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtable))"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TopicModeling_LDA_Sklearn')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a89582bc-1263-4640-ba93-d17ee8960c25"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Perform Topic Modeling using Spark Machine Learning Clustering LDA Model\r\n",
							"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.LDA.html"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"topictable = \"Topic.NATO_Topics\"\r\n",
							"wordtable = \"Words.NATO_Topic_Words\""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.show()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sort by created_at and sample if needed"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"#df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Clean data\r\n",
							"Remove special characters, URLs, hashtags, mentions, emojis, and stopwords"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import re\r\n",
							"from pyspark.sql.functions import udf\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"\r\n",
							"# Define regular expressions for special characters, URLs, hashtags, mentions, and emojis\r\n",
							"special_chars_regex = r'[^\\w\\s]'\r\n",
							"url_regex = r'https?://\\S+'\r\n",
							"mentions_hashtag_regex = r'@\\w+|#\\w+'\r\n",
							"emoji_regex = r'[^\\w\\s\\ufe0f]+'\r\n",
							"extra_spaces_regex = r'\\s{2,}'\r\n",
							"\r\n",
							"def clean_text(text):\r\n",
							"    # Convert text to lowercase\r\n",
							"    text = text.lower()\r\n",
							"    # Remove URLs\r\n",
							"    text = re.sub(url_regex, ' ', text)\r\n",
							"    # Remove mentions and hashtags\r\n",
							"    text = re.sub(mentions_hashtag_regex, ' ', text)\r\n",
							"    # Remove special characters\r\n",
							"    text = re.sub(special_chars_regex, ' ', text)\r\n",
							"    # Remove 'amp'\r\n",
							"    text = re.sub(\"amp\", ' ', text)\r\n",
							"    # Remove emojis\r\n",
							"    text = re.sub(emoji_regex, ' ', text)\r\n",
							"    # Remove empty spaces equal to or bigger than 2 spaces\r\n",
							"    text = re.sub(extra_spaces_regex, ' ', text)\r\n",
							"    # Remove empty spaces\r\n",
							"    text = ' '.join(text.split())\r\n",
							"    return text\r\n",
							"\r\n",
							"# Apply the function to the \"text\" column of the DataFrame\r\n",
							"\r\n",
							"clean_text_udf = udf(clean_text, StringType())\r\n",
							"\r\n",
							"df = df.withColumn(\"cleanText\", clean_text_udf(\"text\"))\r\n",
							"\r\n",
							"#display(df)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import split\r\n",
							"from pyspark.ml.feature import StopWordsRemover\r\n",
							"\r\n",
							"# split the \"text\" column into an array of words and create a new column \"words\"\r\n",
							"df = df.withColumn(\"words\", split(df[\"cleanText\"], \" \"))\r\n",
							"\r\n",
							"# Remove stopwords from the words array\r\n",
							"remover = StopWordsRemover(inputCol=\"words\", outputCol=\"removed\")\r\n",
							"df_removed = remover.transform(df)\r\n",
							"\r\n",
							"# concatenate the words in the \"words\" column into a single string and create a new column \"text_concat\"\r\n",
							"df_removed = df_removed.withColumn(\"cleanText\", concat_ws(\" \", df_removed[\"removed\"]))\r\n",
							"\r\n",
							"# drop the \"words\" column\r\n",
							"df_removed = df_removed.drop(\"words\")\r\n",
							"df_removed = df_removed.drop(\"removed\")\r\n",
							"\r\n",
							"display(df_removed)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Topic Modeling\r\n",
							"## Sklearn LDA (Latent Dirichlet Allocation)\r\n",
							"## 5 Topics"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from sklearn.decomposition import LatentDirichletAllocation\r\n",
							"from sklearn.feature_extraction.text import CountVectorizer\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"# Convert Spark DataFrame to Pandas DataFrame\r\n",
							"pandas_df = df_removed.toPandas()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Tokenize and vectorize the text data\r\n",
							"count_vectorizer = CountVectorizer()\r\n",
							"count_data = count_vectorizer.fit_transform(pandas_df[\"cleanText\"])\r\n",
							"\r\n",
							"# Use LatentDirichletAllocation on Pandas DataFrame\r\n",
							"lda = LatentDirichletAllocation(n_components=5, random_state=42)\r\n",
							"lda.fit(count_data)\r\n",
							"lda_output = lda.transform(count_data)\r\n",
							"\r\n",
							"# Convert output to Pandas DataFrame and merge with original data\r\n",
							"lda_output_df = pd.DataFrame(lda_output, columns=[\"topic_0_confidence\", \"topic_1_confidence\",\"topic_2_confidence\",\"topic_3_confidence\",\"topic_4_confidence\"])\r\n",
							"pandas_df = pd.concat([pandas_df, lda_output_df], axis=1)\r\n",
							"\r\n",
							"pandas_df['topic'] = lda_output.argmax(axis=1)\r\n",
							"\r\n",
							"# Convert Pandas DataFrame back to Spark DataFrame\r\n",
							"spark_df = spark.createDataFrame(pandas_df)\r\n",
							"\r\n",
							"# Renaming the \"cleanText\" column to \"cleantext\"\r\n",
							"spark_df = spark_df.withColumnRenamed(\"cleanText\", \"cleantext\")\r\n",
							"\r\n",
							"# Display Spark DataFrame\r\n",
							"display(spark_df)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Top 10 Word for each Topic"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# View the highest probability words for topics\r\n",
							"feature_names = count_vectorizer.get_feature_names_out()\r\n",
							"\r\n",
							"# Get the list of topic-term distributions from the LatentDirichletAllocation object\r\n",
							"topic_term_distributions = lda.components_\r\n",
							"\r\n",
							"# Get the top N most common words for each topic\r\n",
							"N = 10\r\n",
							"top_N_words = [[feature_names[i] for i in topic.argsort()[:-N - 1:-1]] for topic in topic_term_distributions]\r\n",
							"\r\n",
							"print(top_N_words)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Create Pandas Dataframe for the top 10 words for each topic\r\n",
							"df = pd.DataFrame(top_N_words, columns=['word_'+str(i+1) for i in range(len(top_N_words[0]))])\r\n",
							"df.index.name = 'topic'\r\n",
							"\r\n",
							"# reset index and move Topic Number to a column\r\n",
							"df.reset_index(inplace=True)\r\n",
							"df.rename(columns={'index':'topic'}, inplace=True)\r\n",
							"\r\n",
							"# convert pandas dataframe to spark dataframe\r\n",
							"df_words = spark.createDataFrame(df)\r\n",
							"\r\n",
							"# display the spark dataframe\r\n",
							"display(df_words)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark_df.printSchema"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [Topic].[NATO_Topics]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [Topic].[NATO_Topics]\r\n",
							"( \r\n",
							"\t[id] bigint  NULL,\r\n",
							"\t[created_at] DATETIME2(7)  NULL,\r\n",
							"\t[text] NVARCHAR(4000)  NULL,\r\n",
							"\t[lang] nvarchar(10)  NULL,\r\n",
							"\t[retweet_count] INT  NULL,\r\n",
							"\t[reply_count] INT  NULL,\r\n",
							"\t[like_count] INT  NULL,\r\n",
							"\t[quote_count] INT  NULL,\r\n",
							"\t[impression_count] INT  NULL,\r\n",
							"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
							"    [topic_0_confidence] FLOAT NULL,\r\n",
							"    [topic_1_confidence] FLOAT NULL,\r\n",
							"    [topic_2_confidence] FLOAT NULL,\r\n",
							"    [topic_3_confidence] FLOAT NULL,\r\n",
							"    [topic_4_confidence] FLOAT NULL,\r\n",
							"\t[topic] BIGINT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO\r\n",
							"\r\n",
							"CREATE TABLE [Words].[NATO_Topic_Words]\r\n",
							"( \r\n",
							"\t[topic] bigint  NULL,\r\n",
							"\t[word_1] NVARCHAR(20)  NULL,\r\n",
							"\t[word_2] NVARCHAR(20)  NULL,\r\n",
							"    [word_3] NVARCHAR(20)  NULL,\r\n",
							"    [word_4] NVARCHAR(20)  NULL,\r\n",
							"    [word_5] NVARCHAR(20)  NULL,\r\n",
							"    [word_6] NVARCHAR(20)  NULL,\r\n",
							"    [word_7] NVARCHAR(20)  NULL,\r\n",
							"    [word_8] NVARCHAR(20)  NULL,\r\n",
							"    [word_9] NVARCHAR(20)  NULL,\r\n",
							"    [word_10] NVARCHAR(20)  NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtopictable = \"SQLPoolTest.\" + topictable\r\n",
							"outputwordtable = \"SQLPoolTest.\" + wordtable"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(spark_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtopictable))"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_words.printSchema"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(df_words.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputwordtable))"
						],
						"outputs": [],
						"execution_count": 21
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TopicModeling_LDA_Sklearn_Lemmatize')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP_manual"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "48951c69-5840-403e-a9c2-db381beabf85"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Perform Topic Modeling using Spark Machine Learning Clustering LDA Model\r\n",
							"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.LDA.html"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"topictable = \"Topic.NATO_Topics_NER\"\r\n",
							"wordtable = \"Words.NATO_Topic_Words_NER\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sort by created_at and sample if needed"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"#df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Clean data\r\n",
							"Remove special characters, URLs, hashtags, mentions, emojis, and stopwords"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import re\r\n",
							"from pyspark.sql.functions import udf\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"\r\n",
							"# Define regular expressions for special characters, URLs, hashtags, mentions, and emojis\r\n",
							"special_chars_regex = r'[^\\w\\s]'\r\n",
							"url_regex = r'https?://\\S+'\r\n",
							"mentions_hashtag_regex = r'@\\w+|#\\w+'\r\n",
							"emoji_regex = r'[^\\w\\s\\ufe0f]+'\r\n",
							"extra_spaces_regex = r'\\s{2,}'\r\n",
							"\r\n",
							"def clean_text(text):\r\n",
							"    # Convert text to lowercase\r\n",
							"    text = text.lower()\r\n",
							"    # Remove URLs\r\n",
							"    text = re.sub(url_regex, '', text)\r\n",
							"    # Remove mentions and hashtags\r\n",
							"    text = re.sub(mentions_hashtag_regex, '', text)\r\n",
							"    # Remove special characters\r\n",
							"    text = re.sub(special_chars_regex, '', text)\r\n",
							"    # Remove 'amp'\r\n",
							"    text = re.sub(\"amp\", '', text)\r\n",
							"    # Remove emojis\r\n",
							"    text = re.sub(emoji_regex, '', text)\r\n",
							"    # Remove empty spaces equal to or bigger than 2 spaces\r\n",
							"    text = re.sub(extra_spaces_regex, ' ', text)\r\n",
							"    # Remove empty spaces\r\n",
							"    text = ' '.join(text.split())\r\n",
							"    return text\r\n",
							"\r\n",
							"# Apply the function to the \"text\" column of the DataFrame\r\n",
							"\r\n",
							"clean_text_udf = udf(clean_text, StringType())\r\n",
							"\r\n",
							"df = df.withColumn(\"cleanText\", clean_text_udf(\"text\"))\r\n",
							"\r\n",
							"#display(df)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import split\r\n",
							"from pyspark.ml.feature import StopWordsRemover\r\n",
							"\r\n",
							"# split the \"text\" column into an array of words and create a new column \"words\"\r\n",
							"df = df.withColumn(\"words\", split(df[\"cleanText\"], \" \"))\r\n",
							"\r\n",
							"# Remove stopwords from the words array\r\n",
							"remover = StopWordsRemover(inputCol=\"words\", outputCol=\"removed\")\r\n",
							"df_removed = remover.transform(df)\r\n",
							"\r\n",
							"# concatenate the words in the \"words\" column into a single string and create a new column \"text_concat\"\r\n",
							"df_removed = df_removed.withColumn(\"cleanText\", concat_ws(\" \", df_removed[\"removed\"]))\r\n",
							"\r\n",
							"# drop the \"words\" column\r\n",
							"df_removed = df_removed.drop(\"words\")\r\n",
							"df_removed = df_removed.drop(\"removed\")\r\n",
							"\r\n",
							"display(df_removed)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Lemmatization of remaing words. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import nltk \n",
							"nltk.download('punkt')\n",
							"from nltk.tokenize import sent_tokenize, word_tokenize"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pandas_df = df_removed.toPandas()"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pandas_df['clean_text_tokens'] = pandas_df['cleanText'].apply(lambda x: word_tokenize(x))\n",
							"pandas_df.head()"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"nltk.download('wordnet')\n",
							"from nltk.stem import WordNetLemmatizer\n",
							"def word_lemmatizer(text):\n",
							"    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
							"    return lem_text\n",
							"\n",
							"pandas_df['clean_text_tokens'] = pandas_df['clean_text_tokens'].apply(lambda x: word_lemmatizer(x))\n",
							"pandas_df.head()"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pandas_df['clean_text_tokens']= pandas_df['clean_text_tokens'].str.join(\" \")\n",
							"pandas_df.head()"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Topic Modeling\r\n",
							"## Sklearn LDA (Latent Dirichlet Allocation)\r\n",
							"## 5 Topics"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from sklearn.decomposition import LatentDirichletAllocation\r\n",
							"from sklearn.feature_extraction.text import CountVectorizer\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"# Convert Spark DataFrame to Pandas DataFrame\r\n",
							"#pandas_df = df_removed.toPandas()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Tokenize and vectorize the text data\r\n",
							"count_vectorizer = CountVectorizer()\r\n",
							"count_data = count_vectorizer.fit_transform(pandas_df['clean_text_tokens'])\r\n",
							"\r\n",
							"# Use LatentDirichletAllocation on Pandas DataFrame\r\n",
							"lda = LatentDirichletAllocation(n_components=5, random_state=42)\r\n",
							"lda.fit(count_data)\r\n",
							"lda_output = lda.transform(count_data)\r\n",
							"\r\n",
							"# Convert output to Pandas DataFrame and merge with original data\r\n",
							"lda_output_df = pd.DataFrame(lda_output, columns=[\"topic_0_confidence\", \"topic_1_confidence\",\"topic_2_confidence\",\"topic_3_confidence\",\"topic_4_confidence\"])\r\n",
							"pandas_df = pd.concat([pandas_df, lda_output_df], axis=1)\r\n",
							"\r\n",
							"pandas_df['topic'] = lda_output.argmax(axis=1)\r\n",
							"\r\n",
							"# Convert Pandas DataFrame back to Spark DataFrame\r\n",
							"spark_df = spark.createDataFrame(pandas_df)\r\n",
							"\r\n",
							"# Renaming the \"cleanText\" column to \"cleantext\"\r\n",
							"spark_df = spark_df.withColumnRenamed(\"cleanText\", \"cleantext\")\r\n",
							"\r\n",
							"# Display Spark DataFrame\r\n",
							"display(spark_df)"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Top 10 Word for each Topic"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# View the highest probability words for topics\r\n",
							"feature_names = count_vectorizer.get_feature_names_out()\r\n",
							"\r\n",
							"# Get the list of topic-term distributions from the LatentDirichletAllocation object\r\n",
							"topic_term_distributions = lda.components_\r\n",
							"\r\n",
							"# Get the top N most common words for each topic\r\n",
							"N = 10\r\n",
							"top_N_words = [[feature_names[i] for i in topic.argsort()[:-N - 1:-1]] for topic in topic_term_distributions]\r\n",
							"\r\n",
							"print(top_N_words)"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Create Pandas Dataframe for the top 10 words for each topic\r\n",
							"df = pd.DataFrame(top_N_words, columns=['word_'+str(i+1) for i in range(len(top_N_words[0]))])\r\n",
							"df.index.name = 'topic'\r\n",
							"\r\n",
							"# reset index and move Topic Number to a column\r\n",
							"df.reset_index(inplace=True)\r\n",
							"df.rename(columns={'index':'topic'}, inplace=True)\r\n",
							"\r\n",
							"# convert pandas dataframe to spark dataframe\r\n",
							"df_words = spark.createDataFrame(df)\r\n",
							"\r\n",
							"# display the spark dataframe\r\n",
							"display(df_words)"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark_df.printSchema"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [Topic].[NATO_Topics]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [Topic].[NATO_Topics_NER]\r\n",
							"( \r\n",
							"\t[id] bigint  NULL,\r\n",
							"\t[created_at] DATETIME2(7)  NULL,\r\n",
							"\t[text] NVARCHAR(4000)  NULL,\r\n",
							"\t[lang] nvarchar(10)  NULL,\r\n",
							"\t[retweet_count] INT  NULL,\r\n",
							"\t[reply_count] INT  NULL,\r\n",
							"\t[like_count] INT  NULL,\r\n",
							"\t[quote_count] INT  NULL,\r\n",
							"\t[impression_count] INT  NULL,\r\n",
							"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
							"    [topic_0_confidence] FLOAT NULL,\r\n",
							"    [topic_1_confidence] FLOAT NULL,\r\n",
							"    [topic_2_confidence] FLOAT NULL,\r\n",
							"    [topic_3_confidence] FLOAT NULL,\r\n",
							"    [topic_4_confidence] FLOAT NULL,\r\n",
							"\t[topic] BIGINT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO\r\n",
							"\r\n",
							"CREATE TABLE [Words].[NATO_Topic_Words_NER]\r\n",
							"( \r\n",
							"\t[topic] bigint  NULL,\r\n",
							"\t[word_1] NVARCHAR(20)  NULL,\r\n",
							"\t[word_2] NVARCHAR(20)  NULL,\r\n",
							"    [word_3] NVARCHAR(20)  NULL,\r\n",
							"    [word_4] NVARCHAR(20)  NULL,\r\n",
							"    [word_5] NVARCHAR(20)  NULL,\r\n",
							"    [word_6] NVARCHAR(20)  NULL,\r\n",
							"    [word_7] NVARCHAR(20)  NULL,\r\n",
							"    [word_8] NVARCHAR(20)  NULL,\r\n",
							"    [word_9] NVARCHAR(20)  NULL,\r\n",
							"    [word_10] NVARCHAR(20)  NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtopictable = \"SQLPoolTest.\" + topictable\r\n",
							"outputwordtable = \"SQLPoolTest.\" + wordtable"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(spark_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtopictable))"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_words.printSchema"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(df_words.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputwordtable))"
						],
						"outputs": [],
						"execution_count": 45
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TopicModeling_LDA_Sklearn_UsingParms')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolSmall",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d9e51d91-aede-4ea5-a1e6-de341aaa0bb0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolSmall",
						"name": "SparkPoolSmall",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolSmall",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Perform Topic Modeling using Spark Machine Learning Clustering LDA Model\r\n",
							"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.LDA.html"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a variable for the output SQL Pool table name\r\n",
							"Enter a default place holder name which will be changed by the pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"topictable = \"Topic.NATO_Topics\"\r\n",
							"wordtable = \"Words.NATO_Topic_Words\"\r\n",
							"maintable = \"dbo.NATO_Tweets1\"\r\n",
							"\r\n",
							"#topictable = \"Topic.Aithusa_Topics\"\r\n",
							"#wordtable = \"Words.Aithusa_Topic_Words\"\r\n",
							"#maintable = \"dbo.NATO_Tweets1_Aithusa\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"# Read from a query\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\n",
							"df = (spark.read\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\n",
							"                     # Defaults to storage path defined in the runtime configurations\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\n",
							"                     # query from which data will be read\n",
							"                     .option(Constants.QUERY, \"select * from \"+maintable)\n",
							"                     .synapsesql()\n",
							")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.head(10)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sort by created_at and sample if needed"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"#df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.tail(2)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Clean data\r\n",
							"Remove special characters, URLs, hashtags, mentions, emojis, and stopwords"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import re\r\n",
							"from pyspark.sql.functions import udf\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"\r\n",
							"# Define regular expressions for special characters, URLs, hashtags, mentions, and emojis\r\n",
							"special_chars_regex = r'[^\\w\\s]'\r\n",
							"url_regex = r'https?://\\S+'\r\n",
							"mentions_hashtag_regex = r'@\\w+|#\\w+'\r\n",
							"emoji_regex = r'[^\\w\\s\\ufe0f]+'\r\n",
							"extra_spaces_regex = r'\\s{2,}'\r\n",
							"\r\n",
							"def clean_text(text):\r\n",
							"    # Convert text to lowercase\r\n",
							"    text = text.lower()\r\n",
							"    # Remove URLs\r\n",
							"    text = re.sub(url_regex, ' ', text)\r\n",
							"    # Remove mentions and hashtags\r\n",
							"    text = re.sub(mentions_hashtag_regex, ' ', text)\r\n",
							"    # Remove special characters\r\n",
							"    text = re.sub(special_chars_regex, ' ', text)\r\n",
							"    # Remove 'amp'\r\n",
							"    text = re.sub(\"amp\", ' ', text)\r\n",
							"    # Remove emojis\r\n",
							"    text = re.sub(emoji_regex, ' ', text)\r\n",
							"    # Remove empty spaces equal to or bigger than 2 spaces\r\n",
							"    text = re.sub(extra_spaces_regex, ' ', text)\r\n",
							"    # Remove empty spaces\r\n",
							"    text = ' '.join(text.split())\r\n",
							"    return text\r\n",
							"\r\n",
							"# Apply the function to the \"text\" column of the DataFrame\r\n",
							"\r\n",
							"clean_text_udf = udf(clean_text, StringType())\r\n",
							"\r\n",
							"df = df.withColumn(\"cleanText\", clean_text_udf(\"text\"))\r\n",
							"\r\n",
							"#display(df)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.tail(2)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import split\r\n",
							"from pyspark.ml.feature import StopWordsRemover\r\n",
							"\r\n",
							"# split the \"text\" column into an array of words and create a new column \"words\"\r\n",
							"df = df.withColumn(\"words\", split(df[\"cleanText\"], \" \"))\r\n",
							"\r\n",
							"# Remove stopwords from the words array\r\n",
							"remover = StopWordsRemover(inputCol=\"words\", outputCol=\"removed\")\r\n",
							"df_removed = remover.transform(df)\r\n",
							"\r\n",
							"# concatenate the words in the \"words\" column into a single string and create a new column \"text_concat\"\r\n",
							"df_removed = df_removed.withColumn(\"cleanText\", concat_ws(\" \", df_removed[\"removed\"]))\r\n",
							"\r\n",
							"# drop the \"words\" column\r\n",
							"df_removed = df_removed.drop(\"words\")\r\n",
							"df_removed = df_removed.drop(\"removed\")\r\n",
							"\r\n",
							"display(df_removed)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Topic Modeling\r\n",
							"## Sklearn LDA (Latent Dirichlet Allocation)\r\n",
							"## 5 Topics"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from sklearn.decomposition import LatentDirichletAllocation\r\n",
							"from sklearn.feature_extraction.text import CountVectorizer\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"# Convert Spark DataFrame to Pandas DataFrame\r\n",
							"pandas_df = df_removed.toPandas()"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Tokenize and vectorize the text data\r\n",
							"count_vectorizer = CountVectorizer()\r\n",
							"count_data = count_vectorizer.fit_transform(pandas_df[\"cleanText\"])\r\n",
							"\r\n",
							"# Use LatentDirichletAllocation on Pandas DataFrame\r\n",
							"lda = LatentDirichletAllocation(n_components=5, random_state=42)\r\n",
							"lda.fit(count_data)\r\n",
							"lda_output = lda.transform(count_data)\r\n",
							"\r\n",
							"# Convert output to Pandas DataFrame and merge with original data\r\n",
							"lda_output_df = pd.DataFrame(lda_output, columns=[\"topic_0_confidence\", \"topic_1_confidence\",\"topic_2_confidence\",\"topic_3_confidence\",\"topic_4_confidence\"])\r\n",
							"pandas_df = pd.concat([pandas_df, lda_output_df], axis=1)\r\n",
							"\r\n",
							"pandas_df['topic'] = lda_output.argmax(axis=1)\r\n",
							"\r\n",
							"# Convert Pandas DataFrame back to Spark DataFrame\r\n",
							"spark_df = spark.createDataFrame(pandas_df)\r\n",
							"\r\n",
							"# Renaming the \"cleanText\" column to \"cleantext\"\r\n",
							"spark_df = spark_df.withColumnRenamed(\"cleanText\", \"cleantext\")\r\n",
							"\r\n",
							"# Display Spark DataFrame\r\n",
							"display(spark_df)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Top 10 Word for each Topic"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# View the highest probability words for topics\r\n",
							"feature_names = count_vectorizer.get_feature_names_out()\r\n",
							"\r\n",
							"# Get the list of topic-term distributions from the LatentDirichletAllocation object\r\n",
							"topic_term_distributions = lda.components_\r\n",
							"\r\n",
							"# Get the top N most common words for each topic\r\n",
							"N = 10\r\n",
							"top_N_words = [[feature_names[i] for i in topic.argsort()[:-N - 1:-1]] for topic in topic_term_distributions]\r\n",
							"\r\n",
							"print(top_N_words)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Create Pandas Dataframe for the top 10 words for each topic\r\n",
							"df = pd.DataFrame(top_N_words, columns=['word_'+str(i+1) for i in range(len(top_N_words[0]))])\r\n",
							"df.index.name = 'topic'\r\n",
							"\r\n",
							"# reset index and move Topic Number to a column\r\n",
							"df.reset_index(inplace=True)\r\n",
							"df.rename(columns={'index':'topic'}, inplace=True)\r\n",
							"\r\n",
							"# convert pandas dataframe to spark dataframe\r\n",
							"df_words = spark.createDataFrame(df)\r\n",
							"\r\n",
							"# display the spark dataframe\r\n",
							"display(df_words)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark_df.printSchema"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [Topic].[NATO_Topics]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [Topic].[NATO_Topics]\r\n",
							"( \r\n",
							"\t[id] bigint  NULL,\r\n",
							"\t[created_at] DATETIME2(7)  NULL,\r\n",
							"\t[text] NVARCHAR(4000)  NULL,\r\n",
							"\t[lang] nvarchar(10)  NULL,\r\n",
							"\t[retweet_count] INT  NULL,\r\n",
							"\t[reply_count] INT  NULL,\r\n",
							"\t[like_count] INT  NULL,\r\n",
							"\t[quote_count] INT  NULL,\r\n",
							"\t[impression_count] INT  NULL,\r\n",
							"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
							"    [topic_0_confidence] FLOAT NULL,\r\n",
							"    [topic_1_confidence] FLOAT NULL,\r\n",
							"    [topic_2_confidence] FLOAT NULL,\r\n",
							"    [topic_3_confidence] FLOAT NULL,\r\n",
							"    [topic_4_confidence] FLOAT NULL,\r\n",
							"\t[topic] BIGINT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO\r\n",
							"\r\n",
							"CREATE TABLE [Words].[NATO_Topic_Words]\r\n",
							"( \r\n",
							"\t[topic] bigint  NULL,\r\n",
							"\t[word_1] NVARCHAR(20)  NULL,\r\n",
							"\t[word_2] NVARCHAR(20)  NULL,\r\n",
							"    [word_3] NVARCHAR(20)  NULL,\r\n",
							"    [word_4] NVARCHAR(20)  NULL,\r\n",
							"    [word_5] NVARCHAR(20)  NULL,\r\n",
							"    [word_6] NVARCHAR(20)  NULL,\r\n",
							"    [word_7] NVARCHAR(20)  NULL,\r\n",
							"    [word_8] NVARCHAR(20)  NULL,\r\n",
							"    [word_9] NVARCHAR(20)  NULL,\r\n",
							"    [word_10] NVARCHAR(20)  NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create the three-part table name to which data will be written"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"outputtopictable = \"SQLPoolTest.\" + topictable\r\n",
							"outputwordtable = \"SQLPoolTest.\" + wordtable"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(spark_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputtopictable))"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_words.printSchema"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(df_words.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(outputwordtable))"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"maintable"
						],
						"outputs": [],
						"execution_count": 24
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkPoolSmall')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLPoolTest')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}