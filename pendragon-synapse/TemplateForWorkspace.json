{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "pendragon-synapse"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"AzureSqlDatabase1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase1'"
		},
		"AzureSqlDatabase2_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase2'"
		},
		"pendragon-synapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'pendragon-synapse-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:pendragon-synapse.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://pendragon.dfs.core.windows.net/"
		},
		"AzureMLService1_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "57cd2ff8-9306-41d0-9cad-c2052a0a8381"
		},
		"AzureMLService1_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "Spring2023-TeamPendragon"
		},
		"CognitiveService1_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "57cd2ff8-9306-41d0-9cad-c2052a0a8381"
		},
		"PendragonKeyVault_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://pendragonkeys.vault.azure.net/"
		},
		"RestService_Next_Token_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://api.twitter.com"
		},
		"RestService_Twitter_7days_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://api.twitter.com"
		},
		"Twitter_RestService_ Aithusa_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://api.twitter.com"
		},
		"Twitter_RestService_AS_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://api.twitter.com"
		},
		"pendragon-synapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://pendragon.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/GET Loop Next Token RT')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Initial GET Past 7 Days",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete Previous Output Files",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Delete Previous Merged Output files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"paginationRules": {
									"supportRFC5988": "true"
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "RestResourceTwitter7days",
								"type": "DatasetReference",
								"parameters": {
									"query": {
										"value": "@variables('Query')",
										"type": "Expression"
									},
									"tweet_fields": {
										"value": "@variables('Tweet Fields')",
										"type": "Expression"
									},
									"max_results": {
										"value": "@variables('max_results')",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "GET using Next Token",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Initial GET Past 7 Days",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Number of Loops",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0,int(variables('loops')))",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Extract Next Token",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "Get Next Token",
											"type": "DataFlowReference",
											"parameters": {},
											"datasetParameters": {
												"source1": {},
												"source2": {},
												"sink1": {},
												"sink2": {}
											}
										},
										"staging": {},
										"integrationRuntime": {
											"referenceName": "WarmIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"traceLevel": "None",
										"cacheSinks": {
											"firstRowOnly": true
										}
									}
								},
								{
									"name": "GET to Temp",
									"type": "Copy",
									"dependsOn": [
										{
											"activity": "Delete Temp File",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "RestSource",
											"httpRequestTimeout": "00:01:40",
											"requestInterval": "00.00:00:00.010",
											"requestMethod": "GET",
											"paginationRules": {
												"supportRFC5988": "true"
											}
										},
										"sink": {
											"type": "JsonSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "JsonWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "RestResource_Next_Token",
											"type": "DatasetReference",
											"parameters": {
												"query": {
													"value": "@variables('Query')",
													"type": "Expression"
												},
												"tweet_fields": {
													"value": "@variables('Tweet Fields')",
													"type": "Expression"
												},
												"max_results": {
													"value": "@variables('max_results')",
													"type": "Expression"
												},
												"next_token": {
													"value": "@activity('Extract Next Token').output.runStatus.output.sink1.value[0].next_token",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "Twitter_RawJSON_Temp",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								},
								{
									"name": "Delete Temp File",
									"type": "Delete",
									"dependsOn": [
										{
											"activity": "Extract Next Token",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataset": {
											"referenceName": "Twitter_RawJSON_Temp",
											"type": "DatasetReference",
											"parameters": {}
										},
										"enableLogging": false,
										"storeSettings": {
											"type": "AzureBlobFSReadSettings",
											"recursive": true,
											"enablePartitionDiscovery": false
										}
									}
								}
							]
						}
					},
					{
						"name": "Query",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "Query",
							"value": "\"NATO\" lang:en -is:retweet -is:reply"
						}
					},
					{
						"name": "Max Results",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "max_results",
							"value": "100"
						}
					},
					{
						"name": "Tweet Fields",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "Tweet Fields",
							"value": "id,text,created_at,lang,public_metrics"
						}
					},
					{
						"name": "Delete Previous Output Files",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Query",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Max Results",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Tweet Fields",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Raw_Twitter_Outputs",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Number of Loops",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "loops",
							"value": "25"
						}
					},
					{
						"name": "Delete NATO Temp file",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "GET using Next Token",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Merge JSONs and Convert to CSV",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Delete NATO Temp file",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Merge JSON to CSV",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Delete Previous Merged Output files",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Query",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Merged_JSON_folder",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Copy data to Dedicated SQL Pool",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delete All Existing Rows in NATO_Tweets1 Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlPoolSink",
								"allowCopyCommand": true,
								"copyCommandSettings": {}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "id",
											"type": "Int64"
										},
										"sink": {
											"name": "id",
											"type": "Int64"
										}
									},
									{
										"source": {
											"name": "created_at",
											"type": "DateTime"
										},
										"sink": {
											"name": "created_at",
											"type": "DateTime"
										}
									},
									{
										"source": {
											"name": "text",
											"type": "String"
										},
										"sink": {
											"name": "text",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "lang",
											"type": "String"
										},
										"sink": {
											"name": "lang",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "retweet_count",
											"type": "Int32"
										},
										"sink": {
											"name": "retweet_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "reply_count",
											"type": "Int32"
										},
										"sink": {
											"name": "reply_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "like_count",
											"type": "Int32"
										},
										"sink": {
											"name": "like_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "quote_count",
											"type": "Int32"
										},
										"sink": {
											"name": "quote_count",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "impression_count",
											"type": "Int32"
										},
										"sink": {
											"name": "impression_count",
											"type": "Int32"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "Merged_output_csv",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DedicatedSqlPoolTable1",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Delete All Existing Rows in NATO_Tweets1 Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Merge JSONs and Convert to CSV",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[DeleteAllRowsFromNatoTweets1]"
						}
					},
					{
						"name": "Execute NLP_Analysis Pipeline",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Copy data to Dedicated SQL Pool",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "NLP_Analysis",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"Query": {
						"type": "String"
					},
					"max_results": {
						"type": "String"
					},
					"Tweet Fields": {
						"type": "String"
					},
					"Number of Tweets to Pull": {
						"type": "String"
					},
					"loops": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/RestResourceTwitter7days')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_RawJSON_Temp')]",
				"[concat(variables('workspaceId'), '/datasets/Raw_Twitter_Outputs')]",
				"[concat(variables('workspaceId'), '/dataflows/Merge JSON to CSV')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/WarmIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_JSON_folder')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_output_csv')]",
				"[concat(variables('workspaceId'), '/datasets/DedicatedSqlPoolTable1')]",
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]",
				"[concat(variables('workspaceId'), '/pipelines/NLP_Analysis')]",
				"[concat(variables('workspaceId'), '/dataflows/Get Next Token')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Next_Token')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GET Until Next Token')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Query",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "Query",
							"value": "\"NATO\" lang:en -is:retweet -is:reply"
						}
					},
					{
						"name": "Max Results",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "max_results",
							"value": "100"
						}
					},
					{
						"name": "Tweet Fields",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "Tweet Fields",
							"value": "id,text,created_at,lang,public_metrics"
						}
					},
					{
						"name": "Delete Previous Output Files",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Query",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Max Results",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Tweet Fields",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Raw_Aithusa_Outputs",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Delete Previous Merged Output files",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Query",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Merged_Aithusa_JSON_folder",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Until GET Tweets",
						"type": "Until",
						"dependsOn": [
							{
								"activity": "Set variable1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@equals(variables('stopUntil'), 'true')",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Extract Next Token Aithusa",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "Get Next Token for Aithusa",
											"type": "DataFlowReference",
											"parameters": {},
											"datasetParameters": {
												"source1": {},
												"source2": {},
												"sink1": {},
												"sink2": {}
											}
										},
										"staging": {},
										"integrationRuntime": {
											"referenceName": "WarmIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"traceLevel": "None",
										"cacheSinks": {
											"firstRowOnly": true
										}
									}
								},
								{
									"name": "Delete Temp File",
									"type": "Delete",
									"dependsOn": [
										{
											"activity": "Extract Next Token Aithusa",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataset": {
											"referenceName": "json_Aithusa",
											"type": "DatasetReference",
											"parameters": {}
										},
										"enableLogging": false,
										"storeSettings": {
											"type": "AzureBlobFSReadSettings",
											"recursive": true,
											"enablePartitionDiscovery": false
										}
									}
								},
								{
									"name": "If next_token null",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "If specific date with token",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@equals(activity('Extract Next Token Aithusa').output.runStatus.output.sink1.value[0].next_token, '\\n')",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "Set stopUntil 1",
												"type": "SetVariable",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"variableName": "stopUntil",
													"value": "'false'"
												}
											}
										],
										"ifTrueActivities": [
											{
												"name": "Set stopUntil",
												"type": "SetVariable",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"variableName": "stopUntil",
													"value": "'true'"
												}
											}
										]
									}
								},
								{
									"name": "If specific date with token",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "Delete Temp File",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@bool(variables('specific_date'))",
											"type": "Expression"
										},
										"ifFalseActivities": [
											{
												"name": "GET Past 7 Days with token",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "RestSource",
														"httpRequestTimeout": "00:01:40",
														"requestInterval": "00.00:00:00.010",
														"requestMethod": "GET",
														"paginationRules": {
															"supportRFC5988": "true"
														}
													},
													"sink": {
														"type": "JsonSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "JsonWriteSettings"
														}
													},
													"enableStaging": false
												},
												"inputs": [
													{
														"referenceName": "RestResource_Aithusa_7days_NextToken",
														"type": "DatasetReference",
														"parameters": {
															"query": {
																"value": "@variables('Query')",
																"type": "Expression"
															},
															"tweet_fields": {
																"value": "@variables('Tweet Fields')",
																"type": "Expression"
															},
															"max_results": {
																"value": "@variables('max_results')",
																"type": "Expression"
															},
															"next_token": {
																"value": "@activity('Extract Next Token Aithusa').output.runStatus.output.sink1.value[0].next_token",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "json_Aithusa",
														"type": "DatasetReference",
														"parameters": {}
													}
												]
											}
										],
										"ifTrueActivities": [
											{
												"name": "GET with specifc date with token",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "RestSource",
														"httpRequestTimeout": "00:01:40",
														"requestInterval": "00.00:00:00.010",
														"requestMethod": "GET",
														"paginationRules": {
															"supportRFC5988": "true"
														}
													},
													"sink": {
														"type": "JsonSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "JsonWriteSettings"
														}
													},
													"enableStaging": false
												},
												"inputs": [
													{
														"referenceName": "RestResource_Aithusa_withToken",
														"type": "DatasetReference",
														"parameters": {
															"Query": {
																"value": "@variables('Query')",
																"type": "Expression"
															},
															"max_results": {
																"value": "@variables('max_results')",
																"type": "Expression"
															},
															"tweet_fields": {
																"value": "@variables('Tweet Fields')",
																"type": "Expression"
															},
															"start_year": {
																"value": "@variables('start_year')",
																"type": "Expression"
															},
															"start_month": {
																"value": "@variables('start_month')",
																"type": "Expression"
															},
															"start_day": {
																"value": "@variables('start_day')",
																"type": "Expression"
															},
															"start_hour": {
																"value": "@variables('start_hour')",
																"type": "Expression"
															},
															"start_minute": {
																"value": "@variables('start_minute')",
																"type": "Expression"
															},
															"start_second": "00",
															"end_year": {
																"value": "@variables('end_year')",
																"type": "Expression"
															},
															"end_month": {
																"value": "@variables('end_month')",
																"type": "Expression"
															},
															"end_day": {
																"value": "@variables('end_day')",
																"type": "Expression"
															},
															"end_hour": {
																"value": "@variables('end_hour')",
																"type": "Expression"
															},
															"end_minute": {
																"value": "@variables('end_minute')",
																"type": "Expression"
															},
															"end_second": "00",
															"next_token": {
																"value": "@activity('Extract Next Token Aithusa').output.runStatus.output.sink1.value[0].next_token",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "json_Aithusa",
														"type": "DatasetReference",
														"parameters": {}
													}
												]
											}
										]
									}
								},
								{
									"name": "Set stopUntil after fail",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "If specific date with token",
											"dependencyConditions": [
												"Failed"
											]
										},
										{
											"activity": "Extract Next Token Aithusa",
											"dependencyConditions": [
												"Failed"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "stopUntil",
										"value": "'true'"
									}
								}
							],
							"timeout": "0.12:00:00"
						}
					},
					{
						"name": "Set variable1",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "If specific date",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "stopUntil",
							"value": "false"
						}
					},
					{
						"name": "Delete NATO Temp file",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Until GET Tweets",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "json_Aithusa",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Merge JSONs and Convert to CSV",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Delete NATO Temp file",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Merge JSONs and Convert to CSV for Aithusa",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "End year",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_year",
							"value": "2023"
						}
					},
					{
						"name": "End month",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "End year",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_month",
							"value": "03"
						}
					},
					{
						"name": "End day",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "End month",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_day",
							"value": "26"
						}
					},
					{
						"name": "End hour",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "End day",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_hour",
							"value": "00"
						}
					},
					{
						"name": "End minute",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "End hour",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "end_minute",
							"value": "30"
						}
					},
					{
						"name": "Start year",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_year",
							"value": "2023"
						}
					},
					{
						"name": "Start month",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start year",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_month",
							"value": "03"
						}
					},
					{
						"name": "Start day",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start month",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_day",
							"value": "26"
						}
					},
					{
						"name": "Start hour",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start day",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_hour",
							"value": "00"
						}
					},
					{
						"name": "Start minute",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start hour",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "start_minute",
							"value": "00"
						}
					},
					{
						"name": "Use date range",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Start minute",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "End minute",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "specific_date",
							"value": true
						}
					},
					{
						"name": "If specific date",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Delete Previous Merged Output files",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Delete Previous Output Files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@variables('specific_date')",
								"type": "Expression"
							},
							"ifFalseActivities": [
								{
									"name": "Initial GET Past 7 Days",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "RestSource",
											"httpRequestTimeout": "00:01:40",
											"requestInterval": "00.00:00:00.010",
											"requestMethod": "GET",
											"paginationRules": {
												"supportRFC5988": "true"
											}
										},
										"sink": {
											"type": "JsonSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "JsonWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "RestResource_Aithusa_7days",
											"type": "DatasetReference",
											"parameters": {
												"query": {
													"value": "@variables('Query')",
													"type": "Expression"
												},
												"tweet_fields": {
													"value": "@variables('Tweet Fields')",
													"type": "Expression"
												},
												"max_results": {
													"value": "@variables('max_results')",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "json_Aithusa",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							],
							"ifTrueActivities": [
								{
									"name": "Initial GET with specifc date",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "RestSource",
											"httpRequestTimeout": "00:01:40",
											"requestInterval": "00.00:00:00.010",
											"requestMethod": "GET",
											"paginationRules": {
												"supportRFC5988": "true"
											}
										},
										"sink": {
											"type": "JsonSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "JsonWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "RestResource_Aithusa",
											"type": "DatasetReference",
											"parameters": {
												"Query": {
													"value": "@variables('Query')",
													"type": "Expression"
												},
												"max_results": {
													"value": "@variables('max_results')",
													"type": "Expression"
												},
												"tweet_fields": {
													"value": "@variables('Tweet Fields')",
													"type": "Expression"
												},
												"start_year": {
													"value": "@variables('start_year')",
													"type": "Expression"
												},
												"start_month": {
													"value": "@variables('start_month')",
													"type": "Expression"
												},
												"start_day": {
													"value": "@variables('start_day')",
													"type": "Expression"
												},
												"start_hour": {
													"value": "@variables('start_hour')",
													"type": "Expression"
												},
												"start_minute": {
													"value": "@variables('start_minute')",
													"type": "Expression"
												},
												"start_second": "00",
												"end_year": {
													"value": "@variables('end_year')",
													"type": "Expression"
												},
												"end_month": {
													"value": "@variables('end_month')",
													"type": "Expression"
												},
												"end_day": {
													"value": "@variables('end_day')",
													"type": "Expression"
												},
												"end_hour": {
													"value": "@variables('end_hour')",
													"type": "Expression"
												},
												"end_minute": {
													"value": "@variables('end_minute')",
													"type": "Expression"
												},
												"end_second": "00"
											}
										}
									],
									"outputs": [
										{
											"referenceName": "json_Aithusa",
											"type": "DatasetReference",
											"parameters": {}
										}
									]
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"Query": {
						"type": "String"
					},
					"max_results": {
						"type": "String"
					},
					"Tweet Fields": {
						"type": "String"
					},
					"Number of Tweets to Pull": {
						"type": "String"
					},
					"next_token": {
						"type": "String"
					},
					"stopUntil": {
						"type": "String"
					},
					"specific_date": {
						"type": "Boolean"
					},
					"end_year": {
						"type": "String"
					},
					"end_month": {
						"type": "String"
					},
					"end_day": {
						"type": "String"
					},
					"end_hour": {
						"type": "String"
					},
					"end_minute": {
						"type": "String"
					},
					"start_year": {
						"type": "String"
					},
					"start_month": {
						"type": "String"
					},
					"start_day": {
						"type": "String"
					},
					"start_hour": {
						"type": "String"
					},
					"start_minute": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Raw_Aithusa_Outputs')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_Aithusa_JSON_folder')]",
				"[concat(variables('workspaceId'), '/datasets/json_Aithusa')]",
				"[concat(variables('workspaceId'), '/dataflows/Merge JSONs and Convert to CSV for Aithusa')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/WarmIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/dataflows/Get Next Token for Aithusa')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa_7days')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa_7days_NextToken')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa_withToken')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LookUp_LastModified')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get Metadata1",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Individual_NATO_Folder",
								"type": "DatasetReference",
								"parameters": {}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "DelimitedTextReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get Metadata1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get Metadata1').output.childitems",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Get Metadata2",
									"type": "GetMetadata",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataset": {
											"referenceName": "inputFolder_file_Dyn",
											"type": "DatasetReference",
											"parameters": {
												"FileName": {
													"value": "@item().name",
													"type": "Expression"
												}
											}
										},
										"fieldList": [
											"itemName",
											"lastModified"
										],
										"storeSettings": {
											"type": "AzureBlobFSReadSettings",
											"recursive": true,
											"enablePartitionDiscovery": false
										},
										"formatSettings": {
											"type": "DelimitedTextReadSettings"
										}
									}
								},
								{
									"name": "If Condition1",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "Get Metadata2",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@greater(formatDateTime(activity('Get Metadata2').output.lastModified,'yyyyMMddHHmmss'), formatDateTime(variables('PreviousModifiedDate'),'yyyyMMddHHmmss'))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Set variable1",
												"type": "SetVariable",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"variableName": "latestFileName",
													"value": {
														"value": "@activity('Get Metadata2').output.itemName",
														"type": "Expression"
													}
												}
											}
										]
									}
								},
								{
									"name": "Set variable2",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "If Condition1",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "PreviousModifiedDate",
										"value": {
											"value": "@activity('Get Metadata2').output.lastModified",
											"type": "Expression"
										}
									}
								}
							]
						}
					},
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "ForEach1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".csv"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "inputFolder_file_Dyn",
								"type": "DatasetReference",
								"parameters": {
									"FileName": {
										"value": "@variables('latestFileName')",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Lookup_Output",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Copy data1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Find Last ID",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "None",
							"cacheSinks": {
								"firstRowOnly": true
							}
						}
					},
					{
						"name": "Twitter NATO mentions",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Data flow1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"paginationRules": {
									"supportRFC5988": "true"
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "RestResource_Twitter2",
								"type": "DatasetReference",
								"parameters": {
									"Query": "(\"NATO\"OR\"North Atlantic Treaty Organization\"OR%23NATO) lang:en -is:retweet -is:reply",
									"max_results": "100",
									"tweet_fields": "id,text,created_at,lang,public_metrics",
									"until_id": {
										"value": "@activity('Data flow1').output.runStatus.output.sink1.value[0].id\n",
										"type": "Expression"
									},
									"sort_order": "recency"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Twitter_test_rt_Json",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Data flow2",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Twitter NATO mentions",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Twitter_Parse_JSON_to_CSV",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"latestFileName": {
						"type": "String"
					},
					"PreviousModifiedDate": {
						"type": "String",
						"defaultValue": "1990-01-01T05:12:22Z"
					},
					"last_id": {
						"type": "String"
					},
					"Counter": {
						"type": "String"
					},
					"next_token": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Individual_NATO_Folder')]",
				"[concat(variables('workspaceId'), '/datasets/inputFolder_file_Dyn')]",
				"[concat(variables('workspaceId'), '/datasets/Lookup_Output')]",
				"[concat(variables('workspaceId'), '/dataflows/Find Last ID')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/WarmIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Twitter2')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_test_rt_Json')]",
				"[concat(variables('workspaceId'), '/dataflows/Twitter_Parse_JSON_to_CSV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NLP_Analysis')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Delete all from NATO Topics Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[DeleteAllNATOTopics]"
						}
					},
					{
						"name": "Perform Topic Modeling LDA",
						"description": "Outputs to [Topic].[NATO_Topics]",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from NATO Topics Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "TopicModeling_LDA",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolTest",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from NATO Sentiment Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Perform Topic Modeling LDA",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[DeleteAllNatoSentiment]"
						}
					},
					{
						"name": "Perform Sentiment Analysis",
						"description": "Outputs to [Sent].[NATO_Sent]",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from NATO Sentiment Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "CognitiveServices_Sentiment",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolTest",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					},
					{
						"name": "Delete all from NATO Summary Table",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "Perform Sentiment Analysis",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "SQLPoolTest",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[DeleteAllNatoAbsSum]"
						}
					},
					{
						"name": "Perform Summarization",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Delete all from NATO Summary Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "CogntiveServices_Summarization",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SparkPoolTest",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]",
				"[concat(variables('workspaceId'), '/notebooks/TopicModeling_LDA')]",
				"[concat(variables('workspaceId'), '/bigDataPools/SparkPoolTest')]",
				"[concat(variables('workspaceId'), '/notebooks/CognitiveServices_Sentiment')]",
				"[concat(variables('workspaceId'), '/notebooks/CogntiveServices_Summarization')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PauseResumeSQLPool')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "GET List",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools?api-version=2019-06-01-preview')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"resource": "https://management.azure.com/",
								"credential": {
									"referenceName": "PendragonManagedIdentityCredential",
									"type": "CredentialReference"
								},
								"type": "UserAssignedManagedIdentity"
							}
						}
					},
					{
						"name": "Filter_PROD",
						"type": "Filter",
						"dependsOn": [
							{
								"activity": "GET List",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('GET List').output.value",
								"type": "Expression"
							},
							"condition": {
								"value": "@not(endswith(item().name,'prod'))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach_pool",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Filter_PROD",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Filter_PROD').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "CheckState",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',item().name,'?api-version=2019-06-01-preview')",
											"type": "Expression"
										},
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "GET",
										"headers": {},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.azure.com/"
										}
									}
								},
								{
									"name": "State-PauseOrResume",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "CheckState",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@concat(activity('CheckState').output.properties.status,'-',pipeline().parameters.PauseorResume)",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "Paused-Resume",
												"activities": [
													{
														"name": "SQLPoolResume",
														"type": "WebActivity",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"url": {
																"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',activity('CheckState').output.name,'/resume?api-version=2019-06-01-preview')",
																"type": "Expression"
															},
															"connectVia": {
																"referenceName": "AutoResolveIntegrationRuntime",
																"type": "IntegrationRuntimeReference"
															},
															"method": "POST",
															"headers": {},
															"body": "Pause and Resume",
															"authentication": {
																"type": "MSI",
																"resource": "https://management.azure.com/"
															}
														}
													}
												]
											},
											{
												"value": "Online-Pause",
												"activities": [
													{
														"name": "SQLPoolPause",
														"type": "WebActivity",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"url": {
																"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',activity('CheckState').output.name,'/pause?api-version=2019-06-01-preview')",
																"type": "Expression"
															},
															"connectVia": {
																"referenceName": "AutoResolveIntegrationRuntime",
																"type": "IntegrationRuntimeReference"
															},
															"method": "POST",
															"headers": {},
															"body": "Pause and Resume",
															"authentication": {
																"type": "MSI",
																"resource": "https://management.azure.com/"
															}
														}
													}
												]
											}
										]
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"ResourceGroup": {
						"type": "string",
						"defaultValue": "Spring2023-TeamPendragon"
					},
					"SubscriptionID": {
						"type": "string",
						"defaultValue": "57cd2ff8-9306-41d0-9cad-c2052a0a8381"
					},
					"WorkspaceName": {
						"type": "string",
						"defaultValue": "pendragon-synapse"
					},
					"SQLPoolName": {
						"type": "string",
						"defaultValue": "SQLPoolTest"
					},
					"PauseorResume": {
						"type": "string",
						"defaultValue": "Pause"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/credentials/PendragonManagedIdentityCredential')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Run in Sequence')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Execute Pipeline1",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Delete All Look Up Files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Twitter_GET",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Execute Pipeline1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0,899)",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Execute Pipeline2",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "LookUp_LastModified",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {}
									}
								}
							]
						}
					},
					{
						"name": "Delete All Individual Files",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Twitter_Test_RT_DelimitedText1",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Delete All Look Up Files",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Delete All Individual Files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Lookup_Output",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "ForEach1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "Twitter*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings",
									"copyBehavior": "MergeFiles"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".csv"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "Twitter_Test_RT_DelimitedText1",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "Merged_NATO",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Twitter_GET')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_Test_RT_DelimitedText1')]",
				"[concat(variables('workspaceId'), '/datasets/Lookup_Output')]",
				"[concat(variables('workspaceId'), '/datasets/Merged_NATO')]",
				"[concat(variables('workspaceId'), '/pipelines/LookUp_LastModified')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_GET')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Twitter NATO mentions",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"paginationRules": {
									"supportRFC5988": "true"
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "RestResource_Twitter",
								"type": "DatasetReference",
								"parameters": {
									"Query": "(\"NATO\"OR\"North Atlantic Treaty Organization\"OR%23NATO) lang:en -is:retweet -is:reply",
									"max_results": "100",
									"tweet_fields": "id,text,created_at,lang,public_metrics",
									"start_year": "2023",
									"start_month": "03",
									"start_day": "11",
									"start_hour": "20",
									"start_minute": "00",
									"start_second": "01",
									"end_year": "2023",
									"end_month": "03",
									"end_day": "17",
									"end_hour": "20",
									"end_minute": "00",
									"end_second": "01",
									"sort_order": "recency"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "Twitter_test_rt_Json",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Convert JSON to CSV",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Twitter NATO mentions",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Twitter_Parse_JSON_to_CSV",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"integrationRuntime": {
								"referenceName": "WarmIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"filenumber": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/RestResource_Twitter')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_test_rt_Json')]",
				"[concat(variables('workspaceId'), '/dataflows/Twitter_Parse_JSON_to_CSV')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/WarmIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_Loop')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "RestSink",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": 10,
								"requestMethod": "POST",
								"writeBatchSize": 10000,
								"httpCompressionType": "none"
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "AzureSqlTable1",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "RestResource_Aithusa",
								"type": "DatasetReference",
								"parameters": {
									"Query": "NATO",
									"max_results": "10",
									"tweet_fields": "id,text,created_at,lang,public_metrics",
									"start_year": "2023",
									"start_month": "03",
									"start_day": "05",
									"start_hour": "00",
									"start_minute": "00",
									"start_second": "01",
									"end_year": "2023",
									"end_month": "03",
									"end_day": "07",
									"end_hour": "00",
									"end_minute": "00",
									"end_second": "01"
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AzureSqlTable1')]",
				"[concat(variables('workspaceId'), '/datasets/RestResource_Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "Twitter",
					"table": "NATO_tweets_test"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataset')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "pendragon-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Twitter20230305023424-00001.csv",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/pendragon-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DedicatedSqlPoolTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "id",
						"type": "bigint",
						"precision": 19
					},
					{
						"name": "created_at",
						"type": "datetime2",
						"scale": 7
					},
					{
						"name": "text",
						"type": "nvarchar"
					},
					{
						"name": "lang",
						"type": "nvarchar"
					},
					{
						"name": "retweet_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "reply_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "like_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "quote_count",
						"type": "int",
						"precision": 10
					},
					{
						"name": "impression_count",
						"type": "int",
						"precision": 10
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "NATO_Tweets1"
				},
				"sqlPool": {
					"referenceName": "SQLPoolTest",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/SQLPoolTest')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Find_minid_input')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Tweets/Individual_files/Lookup_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Individual_NATO_Folder')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Tweets/Individual_files/NATO_Mentions",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Json1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "pendragon-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "RedditTest1dataset",
						"fileSystem": "pendragon"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/pendragon-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lookup_Output')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Tweets/Individual_files/Lookup_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergedJSON_to_CSV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATO_merged.csv",
						"folderPath": "Raw_Twitter_JSONs/Merged_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergedJSON_to_CSV_Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATO_merged.csv",
						"folderPath": "Twitter_Aithusa/Merged_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_Aithusa_JSON_folder')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Twitter_Aithusa/Merged_Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"lang": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"id": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"created_at": {
										"type": "string"
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_JSON_folder')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Raw_Twitter_JSONs/Merged_Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"lang": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"id": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"created_at": {
										"type": "string"
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_Json')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@concat('Twitter','_NATO_',utcnow(),'.json')",
							"type": "Expression"
						},
						"folderPath": "Raw_Twitter_JSONs/Merged_Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_NATO')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@concat('merged_NATO',utcNow(),'.csv')",
							"type": "Expression"
						},
						"folderPath": "Tweets/Combined_files/NATO_mentions",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merged_output_csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Raw_Twitter_JSONs/Merged_Output",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"rowDelimiter": "\n",
					"escapeChar": "\"",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw_Aithusa_Outputs')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Twitter_Aithusa/Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw_Aithusa_Outputs_withScheme')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Twitter_Aithusa/Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"id": {
										"type": "string"
									},
									"created_at": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"lang": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw_Twitter_Outputs')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Raw_Twitter_JSONs/Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw_Twitter_Outputs1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Raw_Twitter_JSONs/Output",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"id": {
										"type": "string"
									},
									"lang": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"text": {
										"type": "string"
									},
									"created_at": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResourceTwitter7days')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "RestService_Twitter_7days",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"query": {
						"type": "string",
						"defaultValue": "NATO"
					},
					"tweet_fields": {
						"type": "string",
						"defaultValue": "id,text,created_at,lang,public_metrics"
					},
					"max_results": {
						"type": "string",
						"defaultValue": "10"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().query,'&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/RestService_Twitter_7days')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_ Aithusa",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"Query": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"start_year": {
						"type": "string"
					},
					"start_month": {
						"type": "string"
					},
					"start_day": {
						"type": "string"
					},
					"start_hour": {
						"type": "string"
					},
					"start_minute": {
						"type": "string"
					},
					"start_second": {
						"type": "string"
					},
					"end_year": {
						"type": "string"
					},
					"end_month": {
						"type": "string"
					},
					"end_day": {
						"type": "string"
					},
					"end_hour": {
						"type": "string"
					},
					"end_minute": {
						"type": "string"
					},
					"end_second": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().Query,'&start_time=',dataset().start_year,'-',dataset().start_month,'-',dataset().start_day,'T',dataset().start_hour,':',dataset().start_minute,':',dataset().start_second,'.000Z','&end_time=',dataset().end_year,'-',dataset().end_month,'-',dataset().end_day,'T',dataset().end_hour,':',dataset().end_minute,':',dataset().end_second,'.000Z','&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_ Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Aithusa_7days')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_ Aithusa",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"query": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().query,'&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_ Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Aithusa_7days_NextToken')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_ Aithusa",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"query": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					},
					"next_token": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().query,'&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results,'&next_token=',dataset().next_token)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_ Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Aithusa_withToken')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_ Aithusa",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"Query": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"start_year": {
						"type": "string"
					},
					"start_month": {
						"type": "string"
					},
					"start_day": {
						"type": "string"
					},
					"start_hour": {
						"type": "string"
					},
					"start_minute": {
						"type": "string"
					},
					"start_second": {
						"type": "string"
					},
					"end_year": {
						"type": "string"
					},
					"end_month": {
						"type": "string"
					},
					"end_day": {
						"type": "string"
					},
					"end_hour": {
						"type": "string"
					},
					"end_minute": {
						"type": "string"
					},
					"end_second": {
						"type": "string"
					},
					"next_token": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().Query,'&start_time=',dataset().start_year,'-',dataset().start_month,'-',dataset().start_day,'T',dataset().start_hour,':',dataset().start_minute,':',dataset().start_second,'.000Z','&end_time=',dataset().end_year,'-',dataset().end_month,'-',dataset().end_day,'T',dataset().end_hour,':',dataset().end_minute,':',dataset().end_second,'.000Z','&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results,'&next_token=',dataset().next_token)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_ Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Next_Token')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "RestService_Next_Token",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"query": {
						"type": "string",
						"defaultValue": "NATO"
					},
					"tweet_fields": {
						"type": "string",
						"defaultValue": "id,created_at,text,lang,public_metrics"
					},
					"max_results": {
						"type": "string",
						"defaultValue": "10"
					},
					"next_token": {
						"type": "string",
						"defaultValue": "b26v89c19zqg8o3fqka107quvh02v90tb138wylvg69od"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().query,'&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results,'&next_token=',dataset().next_token)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/RestService_Next_Token')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Twitter')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_AS",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"Query": {
						"type": "string",
						"defaultValue": "NATO"
					},
					"max_results": {
						"type": "string",
						"defaultValue": "10"
					},
					"tweet_fields": {
						"type": "string",
						"defaultValue": "id,text,created_at,lang,public_metrics"
					},
					"start_year": {
						"type": "string",
						"defaultValue": "2023"
					},
					"start_month": {
						"type": "string",
						"defaultValue": "03"
					},
					"start_day": {
						"type": "string",
						"defaultValue": "05"
					},
					"start_hour": {
						"type": "string",
						"defaultValue": "00"
					},
					"start_minute": {
						"type": "string",
						"defaultValue": "00"
					},
					"start_second": {
						"type": "string",
						"defaultValue": "01"
					},
					"end_year": {
						"type": "string",
						"defaultValue": "2023"
					},
					"end_month": {
						"type": "string",
						"defaultValue": "03"
					},
					"end_day": {
						"type": "string",
						"defaultValue": "07"
					},
					"end_hour": {
						"type": "string",
						"defaultValue": "00"
					},
					"end_minute": {
						"type": "string",
						"defaultValue": "00"
					},
					"end_second": {
						"type": "string",
						"defaultValue": "01"
					},
					"sort_order": {
						"type": "string",
						"defaultValue": "recency"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().Query,'&start_time=',dataset().start_year,'-',dataset().start_month,'-',dataset().start_day,'T',dataset().start_hour,':',dataset().start_minute,':',dataset().start_second,'.000Z','&end_time=',dataset().end_year,'-',dataset().end_month,'-',dataset().end_day,'T',dataset().end_hour,':',dataset().end_minute,':',dataset().end_second,'.000Z','&tweet.fields=',dataset().tweet_fields,'&max_results=',dataset().max_results,'&sort_order=',dataset().sort_order)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_AS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource_Twitter2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Twitter_RestService_AS",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"Query": {
						"type": "string"
					},
					"max_results": {
						"type": "string"
					},
					"tweet_fields": {
						"type": "string"
					},
					"until_id": {
						"type": "string"
					},
					"sort_order": {
						"type": "string",
						"defaultValue": "recency"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@concat('/2/tweets/search/recent?query=',dataset().Query,'&tweet.fields=',dataset().tweet_fields,'&until_id=',dataset().until_id,'&max_results=',dataset().max_results,'&sort_order=',dataset().sort_order)",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Twitter_RestService_AS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TwitterJson1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "pendragon-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Twitter_Test_DynamicURL",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"results": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"label": {
										"type": "null"
									},
									"name": {
										"type": "string"
									},
									"volume": {
										"type": "integer"
									},
									"retweets": {
										"type": "integer"
									},
									"tweets": {
										"type": "integer"
									},
									"reachEstimate": {
										"type": "integer"
									},
									"impressions": {
										"type": "integer"
									}
								}
							}
						},
						"orderBy": {
							"type": "string"
						},
						"orderDirection": {
							"type": "string"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/pendragon-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "pendragon-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Twitter_Test_RT_CSV",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/pendragon-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_RawJSON_Temp')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATO_Temp",
						"folderPath": "Raw_Twitter_JSONs/Temp",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"id": {
										"type": "string"
									},
									"created_at": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"lang": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_Test_RT_DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "Tweets/Individual_files/NATO_Mentions",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"nullValue": "NA",
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "id",
						"type": "String"
					},
					{
						"name": "created_at",
						"type": "String"
					},
					{
						"name": "text",
						"type": "String"
					},
					{
						"name": "lang",
						"type": "String"
					},
					{
						"name": "retweet_count",
						"type": "String"
					},
					{
						"name": "reply_count",
						"type": "String"
					},
					{
						"name": "like_count",
						"type": "String"
					},
					{
						"name": "quote_count",
						"type": "String"
					},
					{
						"name": "impression_count",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_test_rt_Json')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATOmentions",
						"folderPath": "Raw_Twitter_JSONs",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"created_at": {
										"type": "string"
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"id": {
										"type": "string"
									},
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"text": {
										"type": "string"
									},
									"lang": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/inputFolder_file_Dyn')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"FileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().FileName",
							"type": "Expression"
						},
						"folderPath": "Tweets/Individual_files/NATO_Mentions",
						"fileSystem": "pendragon"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/json_Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "NATO_Temp",
						"folderPath": "Twitter_Aithusa/Temp",
						"fileSystem": "pendragon"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"data": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"public_metrics": {
										"type": "object",
										"properties": {
											"retweet_count": {
												"type": "integer"
											},
											"reply_count": {
												"type": "integer"
											},
											"like_count": {
												"type": "integer"
											},
											"quote_count": {
												"type": "integer"
											},
											"impression_count": {
												"type": "integer"
											}
										}
									},
									"edit_history_tweet_ids": {
										"type": "array",
										"items": {
											"type": "string"
										}
									},
									"lang": {
										"type": "string"
									},
									"created_at": {
										"type": "string"
									},
									"text": {
										"type": "string"
									},
									"id": {
										"type": "string"
									}
								}
							}
						},
						"meta": {
							"type": "object",
							"properties": {
								"newest_id": {
									"type": "string"
								},
								"oldest_id": {
									"type": "string"
								},
								"result_count": {
									"type": "integer"
								},
								"next_token": {
									"type": "string"
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureMLService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMLService",
				"typeProperties": {
					"subscriptionId": "[parameters('AzureMLService1_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureMLService1_properties_typeProperties_resourceGroupName')]",
					"mlWorkspaceName": "pendragon-ml",
					"authentication": "MSI"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "linked pipelines connections",
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase2_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CognitiveService",
				"typeProperties": {
					"subscriptionId": "[parameters('CognitiveService1_properties_typeProperties_subscriptionId')]",
					"resourceGroup": "spring2023-teampendragon",
					"csName": "pendragon-language",
					"csKind": "TextAnalytics",
					"csLocation": "eastus",
					"endPoint": "https://pendragon-language.cognitiveservices.azure.com/",
					"csKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "PendragonKeyVault",
							"type": "LinkedServiceReference"
						},
						"secretName": "LangaugeKey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/PendragonKeyVault')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PendragonKeyVault')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('PendragonKeyVault_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspace1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "89f52e0a-3aeb-4050-9251-54de8e97068e",
					"tenantID": "9e857255-df57-4c47-a0c0-0546460380cb"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspacePendragon')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"tenantID": "9e857255-df57-4c47-a0c0-0546460380cb"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestService_Next_Token')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('RestService_Next_Token_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous",
					"authHeaders": {
						"Authorization": {
							"type": "SecureString",
							"value": "**********"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestService_Twitter_7days')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('RestService_Twitter_7days_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous",
					"authHeaders": {
						"Authorization": {
							"type": "SecureString",
							"value": "**********"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_RestService_ Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('Twitter_RestService_ Aithusa_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous",
					"authHeaders": {
						"Authorization": {
							"type": "SecureString",
							"value": "**********"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_RestService_AS')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('Twitter_RestService_AS_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous",
					"authHeaders": {
						"Authorization": {
							"type": "SecureString",
							"value": "**********"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pendragon-synapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('pendragon-synapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pendragon-synapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('pendragon-synapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WarmIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 10,
							"cleanup": false
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Find Last ID')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Find_minid_input",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregate1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          created_at as string,",
						"          text as string,",
						"          lang as string,",
						"          retweet_count as string,",
						"          reply_count as string,",
						"          like_count as string,",
						"          quote_count as string,",
						"          impression_count as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     purgeFiles: true) ~> source1",
						"source1 aggregate(id = min(id)) ~> aggregate1",
						"aggregate1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Find_minid_input')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get Next Token for Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "json_Aithusa",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "json_Aithusa",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "Raw_Aithusa_Outputs",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), edit_history_tweet_ids as string[], lang as string, created_at as string, text as string, id as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source(output(",
						"          data as (public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), edit_history_tweet_ids as string[], lang as string, created_at as string, text as string, id as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source2",
						"source1 derive(next_token = meta.next_token) ~> derivedColumn1",
						"derivedColumn1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1,",
						"     mapColumn(",
						"          next_token",
						"     )) ~> sink1",
						"source2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     filePattern:(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.json')),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2) ~> sink2"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/json_Aithusa')]",
				"[concat(variables('workspaceId'), '/datasets/Raw_Aithusa_Outputs')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get Next Token')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference"
							},
							"name": "source1"
						},
						{
							"dataset": {
								"referenceName": "Twitter_RawJSON_Temp",
								"type": "DatasetReference"
							},
							"name": "source2"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "Raw_Twitter_Outputs",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (edit_history_tweet_ids as string[], id as string, created_at as string, text as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source(output(",
						"          data as (edit_history_tweet_ids as string[], id as string, created_at as string, text as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source2",
						"source1 derive(next_token = meta.next_token) ~> derivedColumn1",
						"derivedColumn1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1,",
						"     mapColumn(",
						"          next_token",
						"     )) ~> sink1",
						"source2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     filePattern:(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.json')),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     saveOrder: 2) ~> sink2"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Twitter_RawJSON_Temp')]",
				"[concat(variables('workspaceId'), '/datasets/Raw_Twitter_Outputs')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Get next token variable')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "json_Aithusa",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (edit_history_tweet_ids as string[], id as string, created_at as string, text as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source1 derive(next_token = meta.next_token) ~> derivedColumn1",
						"derivedColumn1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1,",
						"     mapColumn(",
						"          next_token",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/json_Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merge JSON to CSV')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Raw_Twitter_Outputs1",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "MergedJSON_to_CSV",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "sort1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (id as string, lang as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), edit_history_tweet_ids as string[], text as string, created_at as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'arrayOfDocuments') ~> source1",
						"source1 foldDown(unroll(data, data),",
						"     mapColumn(",
						"          data,",
						"          meta",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 derive(id = data.id,",
						"          created_at = data.created_at,",
						"          text = replace(replace(replace(data.text, '\\n\\n', ' '), '\\n', ' '),'\\\"',''),",
						"          lang = data.lang,",
						"          retweet_count = data.public_metrics.retweet_count,",
						"          reply_count = data.public_metrics.reply_count,",
						"          like_count = data.public_metrics.like_count,",
						"          quote_count = data.public_metrics.quote_count,",
						"          impression_count = data.public_metrics.impression_count) ~> derivedColumn1",
						"derivedColumn1 sort(desc(id, false)) ~> sort1",
						"sort1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          created_at as string,",
						"          text as string,",
						"          lang as string,",
						"          retweet_count as string,",
						"          reply_count as string,",
						"          like_count as string,",
						"          quote_count as string,",
						"          impression_count as string",
						"     ),",
						"     partitionFileNames:[(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.csv'))],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          created_at,",
						"          text,",
						"          lang,",
						"          retweet_count,",
						"          reply_count,",
						"          like_count,",
						"          quote_count,",
						"          impression_count",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Raw_Twitter_Outputs1')]",
				"[concat(variables('workspaceId'), '/datasets/MergedJSON_to_CSV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Merge JSONs and Convert to CSV for Aithusa')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Raw_Aithusa_Outputs_withScheme",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "MergedJSON_to_CSV_Aithusa",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "sort1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (edit_history_tweet_ids as string[], id as string, created_at as string, text as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'arrayOfDocuments') ~> source1",
						"source1 foldDown(unroll(data, data),",
						"     mapColumn(",
						"          data,",
						"          meta",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 derive(id = data.id,",
						"          created_at = data.created_at,",
						"          text = replace(replace(replace(data.text, '\\n\\n', ' '), '\\n', ' '),'\\\"',''),",
						"          lang = data.lang,",
						"          retweet_count = data.public_metrics.retweet_count,",
						"          reply_count = data.public_metrics.reply_count,",
						"          like_count = data.public_metrics.like_count,",
						"          quote_count = data.public_metrics.quote_count,",
						"          impression_count = data.public_metrics.impression_count) ~> derivedColumn1",
						"derivedColumn1 sort(desc(id, false)) ~> sort1",
						"sort1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          created_at as string,",
						"          text as string,",
						"          lang as string,",
						"          retweet_count as string,",
						"          reply_count as string,",
						"          like_count as string,",
						"          quote_count as string,",
						"          impression_count as string",
						"     ),",
						"     partitionFileNames:[(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.csv'))],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          created_at,",
						"          text,",
						"          lang,",
						"          retweet_count,",
						"          reply_count,",
						"          like_count,",
						"          quote_count,",
						"          impression_count",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Raw_Aithusa_Outputs_withScheme')]",
				"[concat(variables('workspaceId'), '/datasets/MergedJSON_to_CSV_Aithusa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Twitter_Parse_JSON_to_CSV')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Twitter_test_rt_Json",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Twitter_Test_RT_DelimitedText1",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "sort1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          data as (created_at as string, edit_history_tweet_ids as string[], id as string, public_metrics as (retweet_count as integer, reply_count as integer, like_count as integer, quote_count as integer, impression_count as integer), text as string, lang as string)[],",
						"          meta as (newest_id as string, oldest_id as string, result_count as integer, next_token as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source1 foldDown(unroll(data, data),",
						"     mapColumn(",
						"          data,",
						"          meta",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 derive(id = data.id,",
						"          created_at = data.created_at,",
						"          text = replace(replace(replace(data.text, '\\n\\n', ' '), '\\n', ' '),'\\\"',''),",
						"          lang = data.lang,",
						"          retweet_count = data.public_metrics.retweet_count,",
						"          reply_count = data.public_metrics.reply_count,",
						"          like_count = data.public_metrics.like_count,",
						"          quote_count = data.public_metrics.quote_count,",
						"          impression_count = data.public_metrics.impression_count) ~> derivedColumn1",
						"derivedColumn1 sort(asc(id, true),",
						"     partitionLevel: true) ~> sort1",
						"sort1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          created_at as string,",
						"          text as string,",
						"          lang as string,",
						"          retweet_count as string,",
						"          reply_count as string,",
						"          like_count as string,",
						"          quote_count as string,",
						"          impression_count as string",
						"     ),",
						"     filePattern:(concat('Twitter',toString(currentTimestamp(),'yyyyMMddHHmmss'),'.csv')),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          created_at,",
						"          text,",
						"          lang,",
						"          retweet_count,",
						"          reply_count,",
						"          like_count,",
						"          quote_count,",
						"          impression_count",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Twitter_test_rt_Json')]",
				"[concat(variables('workspaceId'), '/datasets/Twitter_Test_RT_DelimitedText1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PendragonManagedIdentityCredential')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {
					"resourceId": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourcegroups/Spring2023-TeamPendragon/providers/Microsoft.ManagedIdentity/userAssignedIdentities/PendragonManagedIdentity"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Abstract_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [sum].[NATO_Abstractive_Sum]\n(\n    [abstractive_summary] NVARCHAR(4000) NULL,\n    [topic] SMALLINT NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_NER_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [NER].[NATO_NER]\n(\n    [entity_text] NVARCHAR(100) NULL,\n    [entity_category] NVARCHAR(100) NULL,\n    [topic] INT NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Sentiment_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [Sent].[NATO_Sentiment]\n(\n    [id] bigint  NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[cleantext] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n    [topic] SMALLINT NULL,\n    [topic_0_confidence] REAL NULL,\n    [topic_1_confidence] REAL NULL,\n    [topic_2_confidence] REAL NULL,\n    [topic_3_confidence] REAL NULL,\n    [topic_4_confidence] REAL NULL,\n    [sentiment] NVARCHAR(20) NULL,\n    [negative_score] REAL NULL,\n    [positive_score] REAL NULL,\n    [neutral_score] REAL NULL,\n    [mixed_score] REAL NULL\n)\n\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Topic_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "CREATE TABLE [Topic].[NATO_Topics]\n( \n\t[id] bigint  NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[cleantext] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL,\n    [topic] SMALLINT NULL,\n    [topic_0_confidence] REAL NULL,\n    [topic_1_confidence] REAL NULL,\n    [topic_2_confidence] REAL NULL,\n    [topic_3_confidence] REAL NULL,\n    [topic_4_confidence] REAL NULL\n)\n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_NATO_Tweets1_Table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Create_Tables"
				},
				"content": {
					"query": "SET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[NATO_Tweets1]\n( \n\t[id] bigint  NOT NULL,\n\t[created_at] DATETIME2(7)  NULL,\n\t[text] NVARCHAR(4000)  NULL,\n\t[lang] nvarchar(10)  NULL,\n\t[retweet_count] INT  NULL,\n\t[reply_count] INT  NULL,\n\t[like_count] INT  NULL,\n\t[quote_count] INT  NULL,\n\t[impression_count] INT  NULL\n)\nWITH\n(\n\tDISTRIBUTION = HASH ( [id] ),\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_Timeline')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Analytics"
				},
				"content": {
					"query": "SELECT DATEADD(second, (DATEPART(second, created_at) / 10) * 10, DATEADD(minute, DATEDIFF(minute, 0, created_at), 0)) AS IntervalStart,\n       COUNT(*) AS Count\nFROM [dbo].[NATO_Tweets1]\nGROUP BY DATEADD(second, (DATEPART(second, created_at) / 10) * 10, DATEADD(minute, DATEDIFF(minute, 0, created_at), 0))\nORDER BY IntervalStart;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeleteAllFromNATONER_Procedure')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SQL_Procedures"
				},
				"content": {
					"query": "CREATE PROCEDURE DeleteAllNatoNer\nAS\nBEGIN\n    DELETE FROM [NER].[NATO_NER];\nEND;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeleteAllFromNATOSentiment_Procedure')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SQL_Procedures"
				},
				"content": {
					"query": "CREATE PROCEDURE DeleteAllNatoSentiment\nAS\nBEGIN\n    DELETE FROM [Sent].[NATO_Sentiment];\nEND;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeleteAllFromNATOSummary_Procedure')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SQL_Procedures"
				},
				"content": {
					"query": "CREATE PROCEDURE DeleteAllNatoAbsSum\nAS\nBEGIN\n    DELETE FROM [sum].[NATO_Abstractive_Sum];\nEND;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeleteAllFromNATOTopics_Procedure')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SQL_Procedures"
				},
				"content": {
					"query": "CREATE PROCEDURE DeleteAllNATOTopics\nAS\nBEGIN\n    DELETE FROM [Topic].[NATO_Topics];\nEND;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeleteAllProcedure')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SQL_Procedures"
				},
				"content": {
					"query": "CREATE PROCEDURE DeleteAllRowsFromNatoTweets1\nAS\nBEGIN\n    DELETE FROM [dbo].[NATO_Tweets1];\nEND;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Explore"
				},
				"content": {
					"query": "SELECT * FROM [sum].[NATO_Abstractive_Sum];",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Other"
				},
				"content": {
					"query": "CREATE PROCEDURE tweetsClean\nAS\nBEGIN\n     FROM [dbo].[NATO_Tweets1];\nEND;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Other"
				},
				"content": {
					"query": "DROP TABLE [dbo].[NATO_Tweets1]\nGO\n\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[NATO_Tweets1]\n( \n\t[id] [bigint]  NOT NULL,\n\t[created_at] [datetime2](7)  NULL,\n\t[text] [nvarchar](4000)  NULL,\n\t[lang] [nvarchar](10)  NULL,\n\t[retweet_count] [int]  NULL,\n\t[reply_count] [int]  NULL,\n\t[like_count] [int]  NULL,\n\t[quote_count] [int]  NULL,\n\t[impression_count] [int]  NULL,\n\t[cleanText] [nvarchar](4000)\n)\nWITH\n(\n\tDISTRIBUTION = HASH ( [id] ),\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Other"
				},
				"content": {
					"query": "CREATE PROC [dbo].[cleanTweets]\n\nAS\nBEGIN\nupdate dbo.NATO_Tweets1 set cleanText=text;\nUPDATE dbo.NATO_Tweets1 \nSET text = CASE \n\n             WHEN CHARINDEX('https', text) > 0 \n             THEN LEFT(text, CHARINDEX('https', text) - 1) \n             ELSE text \n           END;\n           delete from dbo.NATO_Tweets1 where text='';\nEND\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPoolTest",
						"poolName": "SQLPoolTest"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Cleaning_test')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Cleaning"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b814756b-6cfd-4750-9e3d-ff92393d0e23"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Install bertopic\n",
							"!pip install bertopic"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from bertopic import BERTopic\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data processing\n",
							"import pandas as pd\n",
							"import numpy as np\n",
							"# Text preprocessiong\n",
							"import nltk\n",
							"nltk.download('stopwords')\n",
							"nltk.download('omw-1.4')\n",
							"nltk.download('wordnet')\n",
							"wn = nltk.WordNetLemmatizer()\n",
							"# Topic model\n",
							"from bertopic import BERTopic\n",
							"# Dimension reduction\n",
							"from umap import UMAP"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install transformers[torch]\n",
							"from transformers import pipeline;\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
							"data = [\"I love you\", \"I hate you\"]\n",
							"\n",
							"sentiment_pipeline(data)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#%%pyspark\n",
							"\n",
							"df = spark.read.load('abfss://pendragon@pendragon.dfs.core.windows.net/Raw_Twitter_JSONs/Merged_Output/Twitter20230325213227.csv', format='csv') ## Ifheaderexistsuncommentlinebelow ##, header=True )\n",
							"\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.mount( \n",
							"    \"abfss://pendragon@pendragon.dfs.core.windows.net\", \n",
							"    \"/test\", \n",
							"    {\"linkedService\":\"mygen2account\"} "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\n",
							"import com.microsoft.spark.sqlanalytics\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\n",
							"from pyspark.sql.functions import col\n",
							"\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\n",
							"\n",
							"# Read from a query\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\n",
							"df = (spark.read\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\n",
							"                     # Defaults to storage path defined in the runtime configurations\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\n",
							"                     # query from which data will be read\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\n",
							"                     .synapsesql())"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#regex cleaning\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CleansDataFromFolder')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ReadWrite"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9a20c3d9-e847-4523-9b56-873570d32737"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"!pip install azure-storage-file-datalake"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# azureml-core of version 1.0.72 or higher is required\r\n",
							"# azureml-dataprep[pandas] of version 1.1.34 or higher is required\r\n",
							"from azureml.core import Workspace, Dataset\r\n",
							"\r\n",
							"subscription_id = '57cd2ff8-9306-41d0-9cad-c2052a0a8381'\r\n",
							"resource_group = 'Spring2023-TeamPendragon'\r\n",
							"workspace_name = 'pendragon-ml'\r\n",
							"\r\n",
							"workspace = Workspace(subscription_id, resource_group, workspace_name)\r\n",
							"\r\n",
							"dataset = Dataset.get_by_name(workspace, name='NATO_Tweets')\r\n",
							"df = dataset.to_pandas_dataframe()\r\n",
							"df\r\n",
							""
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Remove @, #, links, and emoticons"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# remove @s\r\n",
							"df['text'] = df['text'].str.replace(r'@\\w+\\s*', '')\r\n",
							"\r\n",
							"# remove hashtags\r\n",
							"df['text'] = df['text'].str.replace(r'#\\w+\\s*', '')\r\n",
							"\r\n",
							"# Remove links\r\n",
							"import re\r\n",
							"df['text'] = df['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\r\n",
							"\r\n",
							"# drop rows where the 'text' column is an empty string\r\n",
							"df = df.drop(df[df['text'] == ''].index)\r\n",
							"\r\n",
							"# drop rows where the 'lang' column is not 'en'\r\n",
							"df = df.drop(df[df['lang'] != 'en'].index)\r\n",
							"\r\n",
							"# print the resulting dataframe\r\n",
							"#df.head(10)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import re\r\n",
							"\r\n",
							"# define a regex pattern to match emoticons\r\n",
							"emoticon_pattern = re.compile(\"[\"\r\n",
							"                              u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
							"                              u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
							"                              u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
							"                              u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
							"                              \"]+\", flags=re.UNICODE)\r\n",
							"\r\n",
							"# apply the regex pattern to the 'text' column to remove emoticons\r\n",
							"df['text'] = df['text'].str.replace(emoticon_pattern, '')\r\n",
							"\r\n",
							"# print the modified DataFrame\r\n",
							"print(df['text'])"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Add a period to the end of text if one doesn't exist"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# check if each string in 'text' column ends with a period\r\n",
							"no_period_mask = ~df['text'].str.endswith('.')\r\n",
							"\r\n",
							"# add period to the end of the strings that don't end with a period\r\n",
							"df.loc[no_period_mask, 'text'] += '.'\r\n",
							"\r\n",
							"# print the modified DataFrame\r\n",
							"#print(df['text'])"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sort the DataFrame by the 'created_at' column\r\n",
							"df = df.sort_values(by='created_at')\r\n",
							"#df.head(10)"
						],
						"outputs": [],
						"execution_count": 23
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c5d59bc6-a182-4bb3-af65-237182a2ad5f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Sentiment Analysis, Entity Extraction, Key Phrase Extraction, Named Entity Recognition"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"sorted_df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"\r\n",
							"# select the first 20 records\r\n",
							"first_20 = sorted_df.limit(20)\r\n",
							"first_20.show()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import regexp_replace\r\n",
							"\r\n",
							"# Define the regular expression pattern to match words starting with @, &, or https\r\n",
							"regex = r'@\\w+|&\\w+|https\\w+|#\\w+'\r\n",
							"regex = r'\\b[@#&]\\w+|\\bhttps\\w*'\r\n",
							"\r\n",
							"# Apply the regexp_replace function to the text column and create a new column with the filtered text\r\n",
							"df_filtered = first_20.withColumn(\"text\", regexp_replace(col(\"text\"), regex, ''))\r\n",
							"\r\n",
							"# Show the filtered DataFrame\r\n",
							"df_filtered.select(\"text\").show()\r\n",
							"\r\n",
							"first_20 = df_filtered"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(dir(synapse.ml.cognitive))"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sentiment"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"You can send up to 125,000 characters across all documents contained in the asynchronous request. Each tweet is a maximum length of 4,000 characters.\r\n",
							"125,000/4,000 = 31 Tweets"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\r\n",
							"from pyspark.sql.functions import col\r\n",
							"import pyspark.sql.functions as F\r\n",
							"import collections\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 5\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = first_20.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of equal to the batch_size\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = first_20.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"output = []\r\n",
							"\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(batch_size)\r\n",
							"    sent = (TextSentiment()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setOutputCol(\"sentiment\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    sent_batch_output = sent.transform(batch.select(\"text\"))\r\n",
							"    #display(sent_batch_output)\r\n",
							"\r\n",
							"    # Extract the 'sentiment' column and convert it to JSON\r\n",
							"    json_output = sent_batch_output.select(\"sentiment\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'sentiment' field\r\n",
							"    sentiments = [json.loads(x)[\"sentiment\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'sentiment' field for each row\r\n",
							"    for sentiment in sentiments:\r\n",
							"        #print(sentiment)\r\n",
							"        output.append(sentiment['document']['sentiment'])\r\n",
							"\r\n",
							"# Print all sentiments\r\n",
							"print(output)\r\n",
							"\r\n",
							"# Count the occurrences of each value in the array\r\n",
							"counts = collections.Counter(output)\r\n",
							"\r\n",
							"# Get the most common value and its count\r\n",
							"most_common = counts.most_common(1)[0]\r\n",
							"most_common_value = most_common[0]\r\n",
							"most_common_count = most_common[1]\r\n",
							"\r\n",
							"#print(\"Most common value:\", most_common_value)\r\n",
							"#print(\"Count:\", most_common_count)\r\n",
							"\r\n",
							"confidence_scores = {}\r\n",
							"\r\n",
							"total_values = len(output)\r\n",
							"\r\n",
							"for value, count in counts.items():\r\n",
							"    confidence_scores[value] = count / total_values\r\n",
							"\r\n",
							"#print(\"Confidence scores:\", confidence_scores)\r\n",
							"sentiment_output = [{\"Sentiment\": most_common_value},{\"Count\": most_common_count},{\"Confidence Scores\":confidence_scores}]\r\n",
							"print(sentiment_output)\r\n",
							"\r\n",
							"# Add to DataFrame\r\n",
							"\r\n",
							"# Define the schema of the DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"Sentiment\", StringType(), True),\r\n",
							"    StructField(\"Count\", IntegerType(), True),\r\n",
							"    StructField(\"Confidence Score Negative\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Positive\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Neutral\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Mixed\", FloatType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"sentiment_output_df = spark.createDataFrame([(most_common_value, most_common_count, confidence_scores['negative'], confidence_scores['positive'], confidence_scores['neutral'], confidence_scores['mixed'])], schema)\r\n",
							"\r\n",
							"display(sentiment_output_df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dir(pyspark.sql.types)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"Sentiment\",  StringType(), True),\r\n",
							"    StructField(\"Count\", IntegerType(), True),\r\n",
							"    StructField(\"Confidence Score Negative\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Positive\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Neutral\", FloatType(), True),\r\n",
							"    StructField(\"Confidence Score Mixed\", FloatType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"sentiment_output_df = spark.createDataFrame([(most_common_value, most_common_count, confidence_scores['negative'], confidence_scores['positive'], confidence_scores['neutral'], confidence_scores['mixed'])], schema)\r\n",
							"\r\n",
							"display(sentiment_output_df)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Load results to the SQL Pool NATO_Sentiment table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(sentiment_output_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(\"SQLPoolTest.Sent.NATO_Sentiment\"))"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Entity Extraction"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"import pyspark.sql.functions as F\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 5\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = first_20.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = first_20.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"# define the schema for the output DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"entity_name\", StringType(), True),\r\n",
							"    StructField(\"entity_url\", StringType(), True)\r\n",
							"    ])\r\n",
							"\r\n",
							"# create an empty DataFrame with the defined schema\r\n",
							"output_df = spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(batch_size)\r\n",
							"    entity = (EntityDetector()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setLanguage(\"en\")\r\n",
							"    .setOutputCol(\"replies\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    ent_batch_output = entity.transform(batch.select(\"text\"))\r\n",
							"    #display(ent_batch_output)\r\n",
							"\r\n",
							"    # Extract the 'replies' column and convert it to JSON\r\n",
							"    json_output = ent_batch_output.select(\"replies\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'replies' field\r\n",
							"    ents = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'replies' field for each row\r\n",
							"    for each in ents:\r\n",
							"        #print(each)\r\n",
							"\r\n",
							"        # create a PySpark DataFrame from the JSON string\r\n",
							"        json_string = each\r\n",
							"        df = spark.read.json(sc.parallelize([json_string]))\r\n",
							"\r\n",
							"        # extract the 'entities' array from the 'document' column\r\n",
							"        df = df.selectExpr('explode(document.entities) as entities')\r\n",
							"\r\n",
							"        # select the 'text' and 'category' fields from the exploded 'entities' array\r\n",
							"        df = df.select('entities.name', 'entities.url')\r\n",
							"\r\n",
							"        # rename the columns\r\n",
							"        df = df.withColumnRenamed('text', 'entity_name')\r\n",
							"        df = df.withColumnRenamed('category', 'entity_url')\r\n",
							"\r\n",
							"        # show the resulting DataFrame\r\n",
							"        #df.show()\r\n",
							"\r\n",
							"        # Append the batch results dataframe to the main output dataframe\r\n",
							"        output_df = output_df.union(df)\r\n",
							"display(output_df)"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Key Phrase Extractor"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"keyPhrase = (KeyPhraseExtractor()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setLanguageCol(\"lang\")\r\n",
							"    .setOutputCol(\"replies\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"keyphrase_output = keyPhrase.transform(first_20.select(\"text\"))\r\n",
							"display(keyPhrase.transform(first_20))"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Convert output to a JSON and collect the results into one array"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"\r\n",
							"# Extract the 'sentiment' column and convert it to JSON\r\n",
							"json_output = keyphrase_output.select(\"replies\").toJSON().collect()\r\n",
							"\r\n",
							"# Deserialize the JSON and extract the 'sentiment' field\r\n",
							"keyphrases = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
							"\r\n",
							"kp_output = []\r\n",
							"\r\n",
							"# Print the 'sentiment' field for each row\r\n",
							"for each in keyphrases:\r\n",
							"    #print(each)\r\n",
							"    for keyphrase in each['document']['keyPhrases']:\r\n",
							"        kp_output.append(keyphrase)\r\n",
							"\r\n",
							"print(kp_output)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Create a dataframe for the key phrases and their count"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Create a PySpark DataFrame from the array\r\n",
							"kp_df = spark.createDataFrame([(i, 1) for i in kp_output], ['text', 'count'])\r\n",
							"\r\n",
							"# Group by 'text' column and sum the 'count' column to get the count of each unique text\r\n",
							"kp_df = kp_df.groupBy('text').agg({'count': 'sum'})\r\n",
							"\r\n",
							"# Sort the DataFrame by the 'count' column in descending order\r\n",
							"kp_df = kp_df.sort(col('sum(count)').desc())\r\n",
							"\r\n",
							"# Show the resulting DataFrame\r\n",
							"display(kp_df)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"import pyspark.sql.functions as F\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 5\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = first_20.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = first_20.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"# define the schema for the output DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"entity_name\", StringType(), True),\r\n",
							"    StructField(\"entity_url\", StringType(), True)\r\n",
							"    ])\r\n",
							"\r\n",
							"# create an empty DataFrame with the defined schema\r\n",
							"output_df = spark.createDataFrame([], schema)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Named Entity Recognition"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"import pyspark.sql.functions as F\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = first_20.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
							"num_batches = int(num_rows / 5) + (num_rows % 5 > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = first_20.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"# define the schema for the output DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"entity_text\", StringType(), True),\r\n",
							"    StructField(\"entity_category\", StringType(), True)\r\n",
							"    ])\r\n",
							"\r\n",
							"# create an empty DataFrame with the defined schema\r\n",
							"output_df = spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(5)\r\n",
							"    ner = (NER()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setLanguageCol(\"lang\")\r\n",
							"    .setOutputCol(\"replies\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    ner_batch_output = ner.transform(batch)\r\n",
							"    #display(ner_batch_output)\r\n",
							"\r\n",
							"    # Extract the 'replies' column and convert it to JSON\r\n",
							"    json_output = ner_batch_output.select(\"replies\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'replies' field\r\n",
							"    nes = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'replies' field for each row\r\n",
							"    for each in nes:\r\n",
							"        #print(each)\r\n",
							"\r\n",
							"        # create a PySpark DataFrame from the JSON string\r\n",
							"        json_string = each\r\n",
							"        df = spark.read.json(sc.parallelize([json_string]))\r\n",
							"\r\n",
							"        # extract the 'entities' array from the 'document' column\r\n",
							"        df = df.selectExpr('explode(document.entities) as entities')\r\n",
							"\r\n",
							"        # select the 'text' and 'category' fields from the exploded 'entities' array\r\n",
							"        df = df.select('entities.text', 'entities.category')\r\n",
							"\r\n",
							"        # rename the columns\r\n",
							"        df = df.withColumnRenamed('text', 'entity_text')\r\n",
							"        df = df.withColumnRenamed('category', 'entity_category')\r\n",
							"\r\n",
							"        # show the resulting DataFrame\r\n",
							"        #df.show()\r\n",
							"\r\n",
							"        # Append the batch results dataframe to the main output dataframe\r\n",
							"        output_df = output_df.union(df)\r\n",
							"display(output_df)"
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices_NamedEntityRecog')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ce1c3f6d-d355-4789-9593-bd47ff0329da"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def QueryDataSQLPool (query):\r\n",
							"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"    # Read from a query\r\n",
							"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"    df = (spark.read\r\n",
							"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                        # Defaults to storage path defined in the runtime configurations\r\n",
							"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                        # query from which data will be read\r\n",
							"                        .option(Constants.QUERY, query)\r\n",
							"                        .synapsesql()\r\n",
							"    )\r\n",
							"    return(df)"
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"#df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create function to run NER and output dataframe"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import *\r\n",
							"import pyspark.sql.functions as F\r\n",
							"from pyspark.sql.functions import lit"
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"def NamedEntityRecognition(dataframe, batchsize, topic):\r\n",
							"    if dataframe.count() > topic:\r\n",
							"\r\n",
							"        # Batch size\r\n",
							"        batch_size = batchsize\r\n",
							"\r\n",
							"        # First, count the total number of rows in the DataFrame\r\n",
							"        num_rows = dataframe.count()\r\n",
							"\r\n",
							"        # Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
							"        num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"        # Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"        df_batches = dataframe.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"        # define the schema for the output DataFrame\r\n",
							"        schema = StructType([\r\n",
							"            StructField(\"entity_text\", StringType(), True),\r\n",
							"            StructField(\"entity_category\", StringType(), True),\r\n",
							"            StructField(\"topic\", IntegerType(),True)\r\n",
							"            ])\r\n",
							"\r\n",
							"        # create an empty DataFrame with the defined schema\r\n",
							"        output_df = spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"        entities = []\r\n",
							"        categories = []\r\n",
							"\r\n",
							"        # Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"        for i in range(num_batches):\r\n",
							"            batch = df_batches[i].limit(batch_size)\r\n",
							"            ner = (NER()\r\n",
							"            .setLinkedService(linked_service_name)\r\n",
							"            .setTextCol(\"cleantext\")\r\n",
							"            .setLanguageCol(\"lang\")\r\n",
							"            .setOutputCol(\"replies\")\r\n",
							"            .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"            ner_batch_output = ner.transform(batch)\r\n",
							"            #display(ner_batch_output)\r\n",
							"\r\n",
							"            # Extract the 'replies' column and convert it to JSON\r\n",
							"            json_output = ner_batch_output.select(\"replies\").toJSON().collect()\r\n",
							"\r\n",
							"            # Deserialize the JSON and extract the 'replies' field\r\n",
							"            nes = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
							"\r\n",
							"            # Print the 'replies' field for each row\r\n",
							"            for each in nes:\r\n",
							"                #print(each)\r\n",
							"                # convert the dictionary to a JSON string\r\n",
							"                json_str = json.dumps(each)\r\n",
							"\r\n",
							"                # parse the JSON string\r\n",
							"                json_data = json.loads(json_str)\r\n",
							"\r\n",
							"                # extract all the 'text' values from the JSON and put them into an array\r\n",
							"                entities = entities + [entity['text'] for entity in json_data['document']['entities']]\r\n",
							"                categories = categories + [entity['category'] for entity in json_data['document']['entities']]\r\n",
							"\r\n",
							"\r\n",
							"        # create rows\r\n",
							"        rows = [Row(entity_text=entities[i],\r\n",
							"                    entity_category=categories[i],\r\n",
							"                    topic = topic\r\n",
							"                    )\r\n",
							"                for i in range(len(entities))]\r\n",
							"\r\n",
							"        # create batch dataframe\r\n",
							"        ner_df = spark.createDataFrame(rows, schema)\r\n",
							"        return(ner_df)"
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Topic 0 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic0 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 0\")\r\n",
							"df_topic0.count()"
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic0_NER = NamedEntityRecognition(dataframe = df_topic0, batchsize = 5, topic = 0)"
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 1 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic1 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 1\")\r\n",
							"df_topic1.count()"
						],
						"outputs": [],
						"execution_count": 93
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"topic1_NER = NamedEntityRecognition(dataframe = df_topic1, batchsize = 5, topic = 1)"
						],
						"outputs": [],
						"execution_count": 94
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 2 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic2 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 2\")\r\n",
							"df_topic2.count()"
						],
						"outputs": [],
						"execution_count": 95
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic2_NER = NamedEntityRecognition(dataframe = df_topic2, batchsize = 5, topic = 2)"
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 3 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic3 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 3\")\r\n",
							"df_topic3.count()"
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic3_NER = NamedEntityRecognition(dataframe = df_topic3, batchsize = 5, topic = 3)"
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 4 NER"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic4 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 4\")\r\n",
							"df_topic4.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic4_NER = NamedEntityRecognition(dataframe = df_topic4, batchsize = 5, topic = 4)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Union all Topic NER dataframes together"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# union the dataframes\r\n",
							"NameEntity_df = topic0_NER.union(topic1_NER).union(topic2_NER).union(topic3_NER).union(topic4_NER)\r\n",
							"display(NameEntity_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"NameEntity_df = topic0_NER.union(topic1_NER).union(topic2_NER)"
						],
						"outputs": [],
						"execution_count": 100
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(NameEntity_df)"
						],
						"outputs": [],
						"execution_count": 101
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [NER].[NATO_NER]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [NER].[NATO_NER]\r\n",
							"(\r\n",
							"    [entity_text] NVARCHAR(100) NULL,\r\n",
							"    [entity_category] NVARCHAR(100) NULL,\r\n",
							"    [topic] INT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(NameEntity_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(\"SQLPoolTest.NER.NATO_NER\"))"
						],
						"outputs": [],
						"execution_count": 102
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices_Sentiment')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2aa1ead5-facb-477f-864e-41e15ea33d8b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from Topic.NATO_Topics\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sort by created_at and sample if needed for testing"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"#df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Import libraries"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Assign Linked Cognitive Service"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Perform Sentiment Analysis in Batches"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import col\r\n",
							"import pyspark.sql.functions as F\r\n",
							"import collections\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 30\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = df.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of equal to the batch_size\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = df.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"from pyspark.sql.types import *\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"id\",  LongType(), True),\r\n",
							"    StructField(\"created_at\", TimestampType(), True),\r\n",
							"    StructField(\"text\", StringType(), True),\r\n",
							"    StructField(\"cleantext\", StringType(), True),\r\n",
							"    StructField(\"lang\", StringType(), True),\r\n",
							"    StructField(\"retweet_count\", IntegerType(), True),\r\n",
							"    StructField(\"reply_count\", IntegerType(), True),\r\n",
							"    StructField(\"like_count\", IntegerType(), True),\r\n",
							"    StructField(\"quote_count\", IntegerType(), True),\r\n",
							"    StructField(\"impression_count\", IntegerType(), True),\r\n",
							"    StructField(\"topic\", ShortType(), True),\r\n",
							"    StructField(\"topic_0_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_1_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_2_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_3_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_4_confidence\", FloatType(), True),\r\n",
							"    StructField(\"sentiment\", StringType(), True),\r\n",
							"    StructField(\"negative_score\", FloatType(), True),\r\n",
							"    StructField(\"positive_score\", FloatType(), True),\r\n",
							"    StructField(\"neutral_score\", FloatType(), True),\r\n",
							"    StructField(\"mixed_score\", FloatType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"# create a DataFrame\r\n",
							"sentiment_output_df =  spark.createDataFrame([], schema)\r\n",
							"\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(batch_size)\r\n",
							"    sent = (TextSentiment()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setTextCol(\"cleantext\")\r\n",
							"    .setOutputCol(\"sentiment\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    sent_batch_output = sent.transform(batch)\r\n",
							"    #display(sent_batch_output)\r\n",
							"\r\n",
							"    # Create arrays for each original column values\r\n",
							"    id_values = []\r\n",
							"    created_at_values = []\r\n",
							"    text_values = []\r\n",
							"    cleantext_values = []\r\n",
							"    lang_values = []\r\n",
							"    retweet_count_values = []\r\n",
							"    reply_count_values = []\r\n",
							"    like_count_values = []\r\n",
							"    quote_count_values = []\r\n",
							"    impression_count_values = []\r\n",
							"    topicgroup_values = []\r\n",
							"    topic0confidence_values = []\r\n",
							"    topic1confidence_values = []\r\n",
							"    topic2confidence_values = []\r\n",
							"    topic3confidence_values = []\r\n",
							"    topic4confidence_values = []\r\n",
							"\r\n",
							"\r\n",
							"    # collect all rows of the batch DataFrame\r\n",
							"    rows = batch.collect()\r\n",
							"\r\n",
							"    # iterate over rows and extract values from each column\r\n",
							"    for row in rows:\r\n",
							"        id_values.append(row[\"id\"])\r\n",
							"        created_at_values.append(row[\"created_at\"])\r\n",
							"        text_values.append(row[\"text\"])\r\n",
							"        cleantext_values.append(row['cleantext'])\r\n",
							"        lang_values.append(row[\"lang\"])\r\n",
							"        retweet_count_values.append(row[\"retweet_count\"])\r\n",
							"        reply_count_values.append(row[\"reply_count\"])\r\n",
							"        like_count_values.append(row[\"like_count\"])\r\n",
							"        quote_count_values.append(row[\"quote_count\"])\r\n",
							"        impression_count_values.append(row[\"impression_count\"])\r\n",
							"        topicgroup_values.append(row['topic'])\r\n",
							"        topic0confidence_values.append(row['topic_0_confidence'])\r\n",
							"        topic1confidence_values.append(row['topic_1_confidence'])\r\n",
							"        topic2confidence_values.append(row['topic_2_confidence'])\r\n",
							"        topic3confidence_values.append(row['topic_3_confidence'])\r\n",
							"        topic4confidence_values.append(row['topic_4_confidence'])\r\n",
							"\r\n",
							"    # Create arrays for new column values\r\n",
							"    batch_sentiments = []\r\n",
							"    batch_negative_scores = []\r\n",
							"    batch_positive_scores = []\r\n",
							"    batch_neutral_scores = []\r\n",
							"    batch_mixed_scores = []\r\n",
							"\r\n",
							"    # Extract the 'sentiment' column and convert it to JSON\r\n",
							"    json_output = sent_batch_output.select(\"sentiment\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'sentiment' field\r\n",
							"    sentiments = [json.loads(x)[\"sentiment\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'sentiment' field for each row\r\n",
							"    for sentiment in sentiments:\r\n",
							"        #print(sentiment)\r\n",
							"        batch_sentiments.append(sentiment['document']['sentiment'])\r\n",
							"        batch_negative_scores.append(sentiment['document']['confidenceScores'].get(\"negative\", 0.0))\r\n",
							"        batch_positive_scores.append(sentiment['document']['confidenceScores'].get(\"positive\", 0.0))\r\n",
							"        batch_neutral_scores.append(sentiment['document']['confidenceScores'].get(\"neutral\", 0.0))\r\n",
							"        batch_mixed_scores.append(sentiment['document']['confidenceScores'].get(\"mixed\", 0.0))\r\n",
							"    \r\n",
							"    # create rows\r\n",
							"    rows = [Row(id=id_values[i],\r\n",
							"                created_at=created_at_values[i],\r\n",
							"                text=text_values[i],\r\n",
							"                cleanText = cleantext_values[i],\r\n",
							"                lang=lang_values[i],\r\n",
							"                retweet_count = retweet_count_values[i],\r\n",
							"                reply_count=reply_count_values[i],\r\n",
							"                like_count=like_count_values[i],\r\n",
							"                quote_count=quote_count_values[i],\r\n",
							"                impression_count = impression_count_values[i],\r\n",
							"                topic = topicgroup_values[i],\r\n",
							"                topic_0_confidence = topic0confidence_values[i],\r\n",
							"                topic_1_confidence = topic1confidence_values[i],\r\n",
							"                topic_2_confidence = topic2confidence_values[i],\r\n",
							"                topic_3_confidence = topic3confidence_values[i],\r\n",
							"                topic_4_confidence = topic4confidence_values[i],\r\n",
							"                sentiment = batch_sentiments[i],\r\n",
							"                negative_score = batch_negative_scores[i],\r\n",
							"                positive_score = batch_positive_scores[i],\r\n",
							"                neutral_score = batch_neutral_scores[i],\r\n",
							"                mixed_score = batch_mixed_scores[i] )\r\n",
							"            for i in range(len(id_values))]\r\n",
							"\r\n",
							"    # create batch dataframe\r\n",
							"    batch_df = spark.createDataFrame(rows, schema)\r\n",
							"\r\n",
							"    # Append the batch results dataframe to the main output dataframe\r\n",
							"    sentiment_output_df = sentiment_output_df.union(batch_df)\r\n",
							"\r\n",
							"#display(sentiment_output_df)"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [Sent].[NATO_Sentiment]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [Sent].[NATO_Sentiment]\r\n",
							"(\r\n",
							"    [id] bigint  NULL,\r\n",
							"\t[created_at] DATETIME2(7)  NULL,\r\n",
							"\t[text] NVARCHAR(4000)  NULL,\r\n",
							"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
							"\t[lang] nvarchar(10)  NULL,\r\n",
							"\t[retweet_count] INT  NULL,\r\n",
							"\t[reply_count] INT  NULL,\r\n",
							"\t[like_count] INT  NULL,\r\n",
							"\t[quote_count] INT  NULL,\r\n",
							"\t[impression_count] INT  NULL,\r\n",
							"    [topic] SMALLINT NULL,\r\n",
							"    [topic_0_confidence] REAL NULL,\r\n",
							"    [topic_1_confidence] REAL NULL,\r\n",
							"    [topic_2_confidence] REAL NULL,\r\n",
							"    [topic_3_confidence] REAL NULL,\r\n",
							"    [topic_4_confidence] REAL NULL,\r\n",
							"    [sentiment] NVARCHAR(20) NULL,\r\n",
							"    [negative_score] REAL NULL,\r\n",
							"    [positive_score] REAL NULL,\r\n",
							"    [neutral_score] REAL NULL,\r\n",
							"    [mixed_score] REAL NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(sentiment_output_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(\"SQLPoolTest.Sent.NATO_Sentiment\"))"
						],
						"outputs": [],
						"execution_count": 34
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CogntiveService_KeyPhrases')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8b0715db-c182-41d7-bc85-fd18f6eac939"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from Topic.NATO_Topics WHERE topic = 0\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 180
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 181
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import regexp_replace\r\n",
							"\r\n",
							"# Define the regular expression pattern to match words starting with @, &, or https\r\n",
							"regex = r'@\\w+|&\\w+|#\\w+'\r\n",
							"\r\n",
							"# Apply the regexp_replace function to the text column and create a new column with the filtered text\r\n",
							"df = df.withColumn(\"text\", regexp_replace(col(\"text\"), regex, ''))\r\n",
							"df = df.withColumn('text', regexp_replace('text', 'https\\\\S+', ''))\r\n",
							"\r\n",
							"# Show the filtered DataFrame\r\n",
							"df.select(\"text\").show()"
						],
						"outputs": [],
						"execution_count": 183
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 184
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 185
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import required libraries\r\n",
							"import json\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"import pyspark.sql.functions as F\r\n",
							"\r\n",
							"# Batch size\r\n",
							"batch_size = 100\r\n",
							"\r\n",
							"# First, count the total number of rows in the DataFrame\r\n",
							"num_rows = df.count()\r\n",
							"\r\n",
							"# Next, calculate the number of batches needed to split the DataFrame into batches of 5\r\n",
							"num_batches = int(num_rows / batch_size) + (num_rows % batch_size > 0)\r\n",
							"\r\n",
							"# Then, use randomSplit to split the DataFrame into smaller DataFrames of equal size\r\n",
							"df_batches = df.randomSplit([1.0]*num_batches, seed=42)\r\n",
							"\r\n",
							"# define the schema for the output DataFrame\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"entity_text\", StringType(), True),\r\n",
							"    StructField(\"entity_category\", StringType(), True)\r\n",
							"    ])\r\n",
							"\r\n",
							"kp_output = []\r\n",
							"\r\n",
							"# create an empty DataFrame with the defined schema\r\n",
							"output_df = spark.createDataFrame([], schema)\r\n",
							"# Finally, use limit to limit the number of rows in each DataFrame to 5\r\n",
							"for i in range(num_batches):\r\n",
							"    batch = df_batches[i].limit(batch_size)\r\n",
							"    keyPhrase = (KeyPhraseExtractor()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setLanguageCol(\"lang\")\r\n",
							"    .setOutputCol(\"replies\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"    kp_batch_output = keyPhrase.transform(batch)\r\n",
							"    #display(ner_batch_output)\r\n",
							"\r\n",
							"    # Extract the 'replies' column and convert it to JSON\r\n",
							"    json_output = kp_batch_output.select(\"replies\").toJSON().collect()\r\n",
							"\r\n",
							"    # Deserialize the JSON and extract the 'sentiment' field\r\n",
							"    keyphrases = [json.loads(x)[\"replies\"] for x in json_output]\r\n",
							"\r\n",
							"    # Print the 'sentiment' field for each row\r\n",
							"    for each in keyphrases:\r\n",
							"        #print(each)\r\n",
							"        for keyphrase in each['document']['keyPhrases']:\r\n",
							"            kp_output.append(keyphrase)\r\n",
							"\r\n",
							"print(kp_output)"
						],
						"outputs": [],
						"execution_count": 186
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Create a PySpark DataFrame from the array\r\n",
							"kp_df = spark.createDataFrame([(i, 1) for i in kp_output], ['text', 'count'])\r\n",
							"\r\n",
							"# Group by 'text' column and sum the 'count' column to get the count of each unique text\r\n",
							"kp_df = kp_df.groupBy('text').agg({'count': 'sum'})\r\n",
							"\r\n",
							"# Sort the DataFrame by the 'count' column in descending order\r\n",
							"kp_df = kp_df.sort(col('sum(count)').desc())\r\n",
							"\r\n",
							"# Show the resulting DataFrame\r\n",
							"display(kp_df)"
						],
						"outputs": [],
						"execution_count": 187
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CogntiveServices_Summarization')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "112g",
					"driverCores": 16,
					"executorMemory": "112g",
					"executorCores": 16,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "894761f5-5ff5-4316-b624-185feba4a6cf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Cognitive Services Extractive Summary for inputs into OpenAI for Abstractive Summary \r\n",
							"## For Each topic\r\n",
							"**Possibly will fail if number of tweets in a topic group is less than 30 tweets**\r\n",
							"<hr>"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Install necessary packags and create functions"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install azure-ai-textanalytics==5.3.0b1"
						],
						"outputs": [],
						"execution_count": 355
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"!pip install openai"
						],
						"outputs": [],
						"execution_count": 356
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *\r\n",
							"import re\r\n",
							"from azure.ai.textanalytics import TextAnalyticsClient\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"from azure.core.credentials import AzureKeyCredential\r\n",
							"from azure.ai.textanalytics import (TextAnalyticsClient,ExtractSummaryAction) \r\n",
							"import openai\r\n",
							"import time"
						],
						"outputs": [],
						"execution_count": 357
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to pull data from the Dedicated SQL Pool\r\n",
							"def QueryDataSQLPool (query):\r\n",
							"    # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"    # Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"    spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"    # Read from a query\r\n",
							"    # Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"    df = (spark.read\r\n",
							"                        # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                        # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                        .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                        # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                        # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                        .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                        # Defaults to storage path defined in the runtime configurations\r\n",
							"                        .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                        # query from which data will be read\r\n",
							"                        .option(Constants.QUERY, query)\r\n",
							"                        .synapsesql()\r\n",
							"    )\r\n",
							"    return(df)"
						],
						"outputs": [],
						"execution_count": 358
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Cognitive Services credentials\r\n",
							"key = \"a3028b5f7e7a462d8a3381e49b9c976d\"\r\n",
							"endpoint = \"https://pendragon-language.cognitiveservices.azure.com/\""
						],
						"outputs": [],
						"execution_count": 359
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enter Azure OpenAI API credentials \r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://pendragonopenai.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"7c4b192d51f64c09a2ca680590ccae3f\""
						],
						"outputs": [],
						"execution_count": 360
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define a UDF (user-defined function) to apply the regular expressions and stop words removal to each row of the dataframe\r\n",
							"def clean_text(text):\r\n",
							"    # Remove URLs, hashtags, mentions, and emojis using regular expressions\r\n",
							"    text = re.sub(url_regex, '', text)\r\n",
							"    text = re.sub(hashtag_regex, '', text)\r\n",
							"    text = re.sub(mention_regex, '', text)\r\n",
							"    text = re.sub(emoji_regex, '', text)\r\n",
							"\r\n",
							"    # Remove special characters\r\n",
							"    text = re.sub(special_chars_regex, '', text)\r\n",
							"\r\n",
							"    return text"
						],
						"outputs": [],
						"execution_count": 362
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define regular expressions for special characters, URLs, hashtags, mentions, and emojis\r\n",
							"special_chars_regex = r'[^\\w\\s]'\r\n",
							"url_regex = r'https?://\\S+'\r\n",
							"hashtag_regex = r'#\\w+'\r\n",
							"mention_regex = r'@\\w+'\r\n",
							"emoji_regex = r'[^\\w\\s\\ufe0f]+'\r\n",
							"\r\n",
							"# Define the UDF as a Spark SQL function\r\n",
							"clean_text_udf = udf(clean_text, StringType())"
						],
						"outputs": [],
						"execution_count": 363
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import nltk\r\n",
							"nltk.download('punkt')\r\n",
							"import re\r\n",
							"from nltk.corpus import stopwords\r\n",
							"nltk.download('stopwords')\r\n",
							"\r\n",
							"# Define a function to clean the text\r\n",
							"def clean_text1(text):\r\n",
							"    # Convert text to lowercase\r\n",
							"    text = text.lower()\r\n",
							"    # Remove URLs\r\n",
							"    text = re.sub(r'http\\S+', '', text)\r\n",
							"    # Remove mentions and hashtags\r\n",
							"    text = re.sub(r'@\\w+|#\\w+', '', text)\r\n",
							"    # Remove special characters\r\n",
							"    text = re.sub(r'[^\\w\\s]', '', text)\r\n",
							"    # Tokenize the text\r\n",
							"    tokens = nltk.word_tokenize(text)\r\n",
							"    # Remove stop words\r\n",
							"    stopwords_list = stopwords.words('english')\r\n",
							"    tokens = [token for token in tokens if token not in stopwords_list]\r\n",
							"    # Remove any remaining noise\r\n",
							"    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\r\n",
							"    return ' '.join(tokens)\r\n",
							"\r\n",
							"# Define a UDF to apply the clean_text function\r\n",
							"clean_text1_udf = udf(clean_text1, StringType())"
						],
						"outputs": [],
						"execution_count": 364
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to split input to Cogntive Services Extractive Summarization into batches of 125000 characters max\r\n",
							"def split_into_batches(input_string, max_chars_per_batch):\r\n",
							"    current_batch = \"\"\r\n",
							"    result = []\r\n",
							"    for c in input_string:\r\n",
							"        if len(current_batch) + 1 > max_chars_per_batch:\r\n",
							"            result.append(current_batch)\r\n",
							"            current_batch = c\r\n",
							"        else:\r\n",
							"            current_batch += c\r\n",
							"    if current_batch:\r\n",
							"        result.append(current_batch)\r\n",
							"    return result"
						],
						"outputs": [],
						"execution_count": 366
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create function to authenticate the client using your key and endpoint \r\n",
							"def authenticate_client():\r\n",
							"    ta_credential = AzureKeyCredential(key)\r\n",
							"    text_analytics_client = TextAnalyticsClient(\r\n",
							"            endpoint=endpoint, \r\n",
							"            credential=ta_credential)\r\n",
							"    return text_analytics_client\r\n",
							"\r\n",
							"client = authenticate_client()"
						],
						"outputs": [],
						"execution_count": 367
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Creat function to apply Exractive Summarization\r\n",
							"def sample_extractive_summarization(client,doc):\r\n",
							"\r\n",
							"    document = doc\r\n",
							"\r\n",
							"    poller = client.begin_analyze_actions(\r\n",
							"        document,\r\n",
							"        actions=[\r\n",
							"            ExtractSummaryAction(max_sentence_count=10)\r\n",
							"        ],\r\n",
							"    )\r\n",
							"    document_results = poller.result()\r\n",
							"    for result in document_results:\r\n",
							"        extract_summary_result = result[0]  # first document, first result\r\n",
							"        #if extract_summary_result.is_error:\r\n",
							"        #    print(\"...Is an error with code '{}' and message '{}'\".format(\r\n",
							"        #        extract_summary_result.code, extract_summary_result.message\r\n",
							"        #    ))\r\n",
							"        #else:\r\n",
							"            #print(\"Summary extracted: \\n{}\".format(\r\n",
							"            #    \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\r\n",
							"            #)\r\n",
							"    return(\" \".join([sentence.text for sentence in extract_summary_result.sentences]))"
						],
						"outputs": [],
						"execution_count": 368
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define dataframe schema\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"topic\", ShortType(), True),\r\n",
							"    StructField(\"abstractive_summary\", StringType(), True)\r\n",
							"    ])"
						],
						"outputs": [],
						"execution_count": 369
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Cognitive Services Extractive to OpenAI Abstractive Summarization"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Abstractive_Summarization(dataframe, topic):\r\n",
							"    # Apply the UDF to the 'text' column of the dataframe and store the result in a new column named 'cleanText'\r\n",
							"    df = dataframe.withColumn('cleanText', clean_text_udf('text'))\r\n",
							"    # Apply the UDF1 to a DataFrame column\r\n",
							"    df = df.withColumn('cleanText', clean_text1_udf(df.cleanText))\r\n",
							"    \r\n",
							"    # Remove empty spaces equal to or bigger than 2 spaces\r\n",
							"    df = df.withColumn('text_trimmed', regexp_replace(df['cleanText'], '\\s{2,}', ' '))\r\n",
							"    # Remove empty spaces before and after text.\r\n",
							"    df = df.withColumn('text_trimmed', trim(df['text_trimmed']))\r\n",
							"    text_col = df.select('text_trimmed')\r\n",
							"    # Collect all tweets into a list\r\n",
							"    text_list = text_col.rdd.map(lambda row: row[0]).collect()\r\n",
							"    # Join all tweets into a single document\r\n",
							"    result = \" \".join([text + \".\" for text in text_list])\r\n",
							"    document = [result]\r\n",
							"    # Extract the string that contains all tweets\r\n",
							"    input_string = document[0]\r\n",
							"\r\n",
							"    # Split the string into batches of 125000 characters maximum\r\n",
							"    max_chars_per_batch = 125000\r\n",
							"    result = split_into_batches(input_string, max_chars_per_batch)\r\n",
							"    print('Number of batches',len(result))\r\n",
							"    for batch in result:\r\n",
							"        print('Characters in batch', len(batch))\r\n",
							"\r\n",
							"    # For each batch create an extractive summary of 10 sentences\r\n",
							"    ex_summaries = []\r\n",
							"\r\n",
							"    for each in result:\r\n",
							"        #print('Batch', [each])\r\n",
							"        ex_summary = sample_extractive_summarization(client,[each])\r\n",
							"        ex_summaries.append([ex_summary])\r\n",
							"\r\n",
							"    print('All Extractive Summaries', ex_summaries) \r\n",
							"\r\n",
							"    ab_summaries = []\r\n",
							"\r\n",
							"    retries = 3\r\n",
							"    delay = 300\r\n",
							"\r\n",
							"    for i in range(retries):\r\n",
							"        try:\r\n",
							"            # Create a list of abstractive summaries for each extractive summary\r\n",
							"            for each in ex_summaries:\r\n",
							"                print(each)\r\n",
							"                print(each[0])\r\n",
							"                print(type(each[0]))\r\n",
							"                p = 'Provide a summary of the text below that captures its main idea.\\n\\n' + each[0] \r\n",
							"                print(p)\r\n",
							"\r\n",
							"                response = openai.Completion.create(\r\n",
							"                engine=\"SummaryDivinci003\",\r\n",
							"                prompt = p,\r\n",
							"                temperature=0.3,\r\n",
							"                max_tokens=250,\r\n",
							"                top_p=1,\r\n",
							"                frequency_penalty=0,\r\n",
							"                presence_penalty=0,\r\n",
							"                best_of=1,\r\n",
							"                stop=None)\r\n",
							"\r\n",
							"                print(response['choices'][0]['text'])\r\n",
							"\r\n",
							"                ab_summaries.append(response['choices'][0]['text'])\r\n",
							"                if len(ex_summaries) > 1:\r\n",
							"                    time.sleep(10)\r\n",
							"            print(ab_summaries)\r\n",
							"            print(\"Code succeeded!\")\r\n",
							"            break # Break out of the loop if code succeeds\r\n",
							"        except:\r\n",
							"            time.sleep(delay)\r\n",
							"            print('Failed')\r\n",
							"            #raise Exception(f\"Failed after {retries} retries\")\r\n",
							"            ab_summaries = []\r\n",
							"\r\n",
							"    else:\r\n",
							"        raise Exception(f\"Failed after {retries} retries\")\r\n",
							"\r\n",
							"    # Wait 10 seconds\r\n",
							"    time.sleep(10)\r\n",
							"\r\n",
							"    # Create one abstractive summary using the collection of abstractive summaries created above\r\n",
							"    p = 'Provide a summary of the text below that captures the main idea. ' + ''.join(ab_summaries)\r\n",
							"\r\n",
							"    response = openai.Completion.create(\r\n",
							"    engine=\"SummaryDivinci003\",\r\n",
							"    prompt = p,\r\n",
							"    temperature=0.3,\r\n",
							"    max_tokens=250,\r\n",
							"    top_p=1,\r\n",
							"    frequency_penalty=0,\r\n",
							"    presence_penalty=0,\r\n",
							"    best_of=1,\r\n",
							"    stop=None)\r\n",
							" \r\n",
							"    final_ab_summarization = response['choices'][0]['text']\r\n",
							"    #print(response['choices'][0]['text'])\r\n",
							"    print('-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\r\n",
							"    print('Abstractive Summary: ', final_ab_summarization)\r\n",
							"    print('Character Length:', len(final_ab_summarization))\r\n",
							"\r\n",
							"    # Put the meta abstractive summary into a data frame and return the data frame object \r\n",
							"    return(spark.createDataFrame([(topic, final_ab_summarization)], schema))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 373
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 0 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic0 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 0\")\r\n",
							"df_topic0.count()"
						],
						"outputs": [],
						"execution_count": 371
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import nltk\r\n",
							"nltk.download('punkt')\r\n",
							"topic0_summary = Abstractive_Summarization(dataframe = df_topic0, topic=0)\r\n",
							"display(topic0_summary)"
						],
						"outputs": [],
						"execution_count": 374
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"time.sleep(180)"
						],
						"outputs": [],
						"execution_count": 375
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 1 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic1 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 1\")\r\n",
							"df_topic1.count()"
						],
						"outputs": [],
						"execution_count": 376
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import nltk\r\n",
							"nltk.download('punkt')\r\n",
							"topic1_summary = Abstractive_Summarization(dataframe = df_topic1, topic=1)\r\n",
							"display(topic1_summary)"
						],
						"outputs": [],
						"execution_count": 377
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"time.sleep(180)"
						],
						"outputs": [],
						"execution_count": 378
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic2 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 2\")\r\n",
							"df_topic2.count()"
						],
						"outputs": [],
						"execution_count": 379
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import nltk\r\n",
							"nltk.download('punkt')\r\n",
							"topic2_summary = Abstractive_Summarization(dataframe = df_topic2, topic=2)\r\n",
							"display(topic2_summary)"
						],
						"outputs": [],
						"execution_count": 381
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"time.sleep(180)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Topic 3 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic3 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 3\")\r\n",
							"df_topic3.count()"
						],
						"outputs": [],
						"execution_count": 382
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import nltk\r\n",
							"nltk.download('punkt')\r\n",
							"topic3_summary = Abstractive_Summarization(dataframe = df_topic3, topic=3)\r\n",
							"display(topic3_summary)"
						],
						"outputs": [],
						"execution_count": 383
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"time.sleep(180)"
						],
						"outputs": [],
						"execution_count": 384
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Topic 4 Summary"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_topic4 = QueryDataSQLPool(\"select * from Sent.NATO_Sentiment where topic = 4\")\r\n",
							"df_topic4.count()"
						],
						"outputs": [],
						"execution_count": 385
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import nltk\r\n",
							"nltk.download('punkt')\r\n",
							"topic4_summary = Abstractive_Summarization(dataframe = df_topic4, topic=4)\r\n",
							"display(topic4_summary)"
						],
						"outputs": [],
						"execution_count": 387
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Union dataframes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# union the dataframes\r\n",
							"Topic_Summary_df = topic0_summary.union(topic1_summary).union(topic2_summary).union(topic3_summary).union(topic4_summary)\r\n",
							"display(Topic_Summary_df)"
						],
						"outputs": [],
						"execution_count": 388
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [sum].[NATO_Abstractive_Sum]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [sum].[NATO_Abstractive_Sum]\r\n",
							"(\r\n",
							"    [abstractive_summary] NVARCHAR(4000) NULL,\r\n",
							"    [topic] SMALLINT NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(Topic_Summary_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(\"SQLPoolTest.sum.NATO_Abstractive_Sum\"))"
						],
						"outputs": [],
						"execution_count": 389
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OpenAI')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "OpenAI"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "78fc0f2f-9b10-4a78-a38e-4427ed14eba7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# sort dataframe by date column in ascending order\r\n",
							"sorted_df = df.orderBy(col(\"impression_count\").desc())\r\n",
							"\r\n",
							"# show sorted dataframe\r\n",
							"sorted_df.show()"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# select the first 20 records\r\n",
							"first_20 = sorted_df.limit(20)\r\n",
							"first_20.show()"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import collect_list, concat_ws\r\n",
							"\r\n",
							"# concatenate strings in \"text\" column into a single string\r\n",
							"concatenated_string = first_20.agg(concat_ws(\"\", collect_list(\"text\"))).collect()[0][0]\r\n",
							"\r\n",
							"print(concatenated_string)"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark import SparkContext\r\n",
							"sc = SparkContext.getOrCreate()\r\n",
							"\r\n",
							"input_string = concatenated_string\r\n",
							"\r\n",
							"def remove_emoji(text):\r\n",
							"    return text.encode('ascii', 'ignore').decode('ascii')\r\n",
							"result_string = remove_emoji(input_string)\r\n",
							"\r\n",
							"words_rdd = sc.parallelize(result_string.split())\r\n",
							"filtered_words_rdd = words_rdd.filter(lambda word: not word.startswith(\"#\"))\r\n",
							"result_string = \" \".join(filtered_words_rdd.collect())\r\n",
							"\r\n",
							"words_rdd = sc.parallelize(result_string.split())\r\n",
							"filtered_words_rdd = words_rdd.filter(lambda word: not word.startswith(\"@\"))\r\n",
							"result_string = \" \".join(filtered_words_rdd.collect())\r\n",
							"\r\n",
							"words_rdd = sc.parallelize(result_string.split())\r\n",
							"filtered_words_rdd = words_rdd.filter(lambda word: not word.startswith(\"&\"))\r\n",
							"result_string = \" \".join(filtered_words_rdd.collect())\r\n",
							"\r\n",
							"words_rdd = sc.parallelize(result_string.split())\r\n",
							"filtered_words_rdd = words_rdd.filter(lambda word: not word.startswith(\"https\"))\r\n",
							"result_string = \" \".join(filtered_words_rdd.collect())\r\n",
							"\r\n",
							"print(result_string)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#!pip install openai\r\n",
							"import openai\r\n",
							"\r\n",
							"\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\""
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p = 'Provide a summary of the text below that captures the main idea. ' + result_string"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Note: The openai-python library support for Azure OpenAI is in preview.\r\n",
							"import os\r\n",
							"import openai\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\"\r\n",
							"\r\n",
							"# query = input(\"Enter your query in natural language: \")\r\n",
							"\r\n",
							"response = openai.Completion.create(\r\n",
							"  engine=\"SumTest\",\r\n",
							"  prompt = p,\r\n",
							"  temperature=0.3,\r\n",
							"  max_tokens=250,\r\n",
							"  top_p=1,\r\n",
							"  frequency_penalty=0,\r\n",
							"  presence_penalty=0,\r\n",
							"  best_of=1,\r\n",
							"  stop=None)\r\n",
							"\r\n",
							"response"
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p = 'Perform key phrase extraction on the following text. ' + result_string"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Note: The openai-python library support for Azure OpenAI is in preview.\r\n",
							"import os\r\n",
							"import openai\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\"\r\n",
							"\r\n",
							"# query = input(\"Enter your query in natural language: \")\r\n",
							"\r\n",
							"response = openai.Completion.create(\r\n",
							"  engine=\"SumTest\",\r\n",
							"  prompt = p,\r\n",
							"  temperature=0.3,\r\n",
							"  max_tokens=250,\r\n",
							"  top_p=1,\r\n",
							"  frequency_penalty=0,\r\n",
							"  presence_penalty=0,\r\n",
							"  best_of=1,\r\n",
							"  stop=None)\r\n",
							"\r\n",
							"response"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p = 'Perform Named Entity Recognition on the following text. ' + result_string"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Note: The openai-python library support for Azure OpenAI is in preview.\r\n",
							"import os\r\n",
							"import openai\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\"\r\n",
							"\r\n",
							"# query = input(\"Enter your query in natural language: \")\r\n",
							"\r\n",
							"response = openai.Completion.create(\r\n",
							"  engine=\"SumTest\",\r\n",
							"  prompt = p,\r\n",
							"  temperature=0.3,\r\n",
							"  max_tokens=250,\r\n",
							"  top_p=1,\r\n",
							"  frequency_penalty=0,\r\n",
							"  presence_penalty=0,\r\n",
							"  best_of=1,\r\n",
							"  stop=None)\r\n",
							"\r\n",
							"response"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p = 'What is the overall sentiment of the following text as a percentage. ' + result_string"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Note: The openai-python library support for Azure OpenAI is in preview.\r\n",
							"import os\r\n",
							"import openai\r\n",
							"openai.api_type = \"azure\"\r\n",
							"openai.api_base = \"https://querystructureddata.openai.azure.com/\"\r\n",
							"openai.api_version = \"2022-12-01\"\r\n",
							"openai.api_key = \"c607bf2bba454302b492ba369efe54f6\"\r\n",
							"\r\n",
							"# query = input(\"Enter your query in natural language: \")\r\n",
							"\r\n",
							"response = openai.Completion.create(\r\n",
							"  engine=\"SumTest\",\r\n",
							"  prompt = p,\r\n",
							"  temperature=0.3,\r\n",
							"  max_tokens=250,\r\n",
							"  top_p=1,\r\n",
							"  frequency_penalty=0,\r\n",
							"  presence_penalty=0,\r\n",
							"  best_of=1,\r\n",
							"  stop=None)\r\n",
							"\r\n",
							"response"
						],
						"outputs": [],
						"execution_count": 65
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ReadWriteSQLPool')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ReadWrite"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2aa6efde-df82-4393-ae30-814b3a7a34c5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Read from a query using Azure AD based authentication\r\n",
							"\r\n",
							"https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export?tabs=python%2Cpython1%2Cscala2%2Cscala3%2Cscala4%2Cscala5"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"dfToReadFromQueryAsOption = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")\r\n",
							"\r\n",
							"dfToReadFromQueryAsArgument = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .synapsesql(\"select * from dbo.NATO_Tweets1\")\r\n",
							")\r\n",
							"\r\n",
							"# Show contents of the dataframe\r\n",
							"dfToReadFromQueryAsOption.show()\r\n",
							"dfToReadFromQueryAsArgument.show()"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = dfToReadFromQueryAsArgument"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Text Analytics\r\n",
							"https://learn.microsoft.com/en-us/azure/synapse-analytics/machine-learning/tutorial-text-analytics-use-mmlspark"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from pyspark.sql.functions import col"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service_name = \"CognitiveService1\""
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a dataframe that's tied to it's column names\r\n",
							"df = spark.createDataFrame([\r\n",
							"  (\"Hello World\",),\r\n",
							"  (\"Bonjour tout le monde\",),\r\n",
							"  (\"La carretera estaba atascada. Haba mucho trfico el da de ayer.\",),\r\n",
							"  (\"\",),\r\n",
							"  (\"\",),\r\n",
							"  (\":) :( :D\",)\r\n",
							"], [\"text\",])\r\n",
							"\r\n",
							"# Run the Text Analytics service with options\r\n",
							"language = (LanguageDetector()\r\n",
							"    .setLinkedService(linked_service_name)\r\n",
							"    .setTextCol(\"text\")\r\n",
							"    .setOutputCol(\"language\")\r\n",
							"    .setErrorCol(\"error\"))\r\n",
							"\r\n",
							"# Show the results of your text query in a table format\r\n",
							"display(language.transform(df))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Remove special characters"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import regexp_replace\r\n",
							"\r\n",
							"# remove special characters from text column\r\n",
							"df = df.withColumn(\"text\", regexp_replace(df[\"text\"], \"[^a-zA-Z0-9\\\\s]\", \"\"))\r\n",
							"\r\n",
							"df_text = df.select('text')\r\n",
							"\r\n",
							"# show resulting dataframe\r\n",
							"df_text.show()"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Write back to SQLPoolTest dbo.NATO_Tweets1 table using Azure AD based authentication"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(\"SQLPoolTest.dbo.NATO_Tweets1\"))"
						],
						"outputs": [],
						"execution_count": 44
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TopicModeling_LDA')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "NLP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPoolTest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "112g",
					"driverCores": 16,
					"executorMemory": "112g",
					"executorCores": 16,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c68dfa68-0548-436d-91af-a54cbcaa9c84"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/57cd2ff8-9306-41d0-9cad-c2052a0a8381/resourceGroups/Spring2023-TeamPendragon/providers/Microsoft.Synapse/workspaces/pendragon-synapse/bigDataPools/SparkPoolTest",
						"name": "SparkPoolTest",
						"type": "Spark",
						"endpoint": "https://pendragon-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolTest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 16,
						"memory": 112
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SQLPoolTest\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"df = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SQLPoolTest\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							"                     # Defaults to storage path defined in the runtime configurations\r\n",
							"                     .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							"                     # query from which data will be read\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.NATO_Tweets1\")\r\n",
							"                     .synapsesql()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Sort by created_at and sample if needed"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Select sample size for testing\r\n",
							"\r\n",
							"# Sort ascending by 'created_at'\r\n",
							"from pyspark.sql.functions import desc\r\n",
							"\r\n",
							"df = df.orderBy(desc(\"created_at\"))\r\n",
							"\r\n",
							"# select the first 100 records\r\n",
							"#df = df.limit(100)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Clean data\r\n",
							"Remove special characters, URLs, hashtags, mentions, emojis, and stopwords"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"import re\r\n",
							"from pyspark.ml.feature import StopWordsRemover\r\n",
							"from pyspark.sql.functions import udf\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"\r\n",
							"# Define regular expressions for special characters, URLs, hashtags, mentions, and emojis\r\n",
							"special_chars_regex = r'[^\\w\\s]'\r\n",
							"url_regex = r'https?://\\S+'\r\n",
							"hashtag_regex = r'#\\w+'\r\n",
							"mention_regex = r'@\\w+'\r\n",
							"emoji_regex = r'[^\\w\\s\\ufe0f]+'\r\n",
							"\r\n",
							"# Define a list of stop words\r\n",
							"stop_words = StopWordsRemover().getStopWords()\r\n",
							"\r\n",
							"# Define a UDF (user-defined function) to apply the regular expressions and stop words removal to each row of the dataframe\r\n",
							"def clean_text(text):\r\n",
							"    # Remove URLs, hashtags, mentions, and emojis using regular expressions\r\n",
							"    text = re.sub(url_regex, '', text)\r\n",
							"    text = re.sub(hashtag_regex, '', text)\r\n",
							"    text = re.sub(mention_regex, '', text)\r\n",
							"    text = re.sub(emoji_regex, '', text)\r\n",
							"\r\n",
							"    # Remove special characters and convert to lowercase\r\n",
							"    text = re.sub(special_chars_regex, '', text).lower()\r\n",
							"\r\n",
							"    # Remove stop words\r\n",
							"    words = text.split()\r\n",
							"    words = [w for w in words if w not in stop_words]\r\n",
							"    text = ' '.join(words)\r\n",
							"\r\n",
							"    return text\r\n",
							"\r\n",
							"# Define the UDF as a Spark SQL function\r\n",
							"from pyspark.sql.functions import udf\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"clean_text_udf = udf(clean_text, StringType())\r\n",
							"\r\n",
							"# Apply the UDF to the 'text' column of the dataframe and store the result in a new column named 'cleanText'\r\n",
							"df = df.withColumn('cleanText', clean_text_udf('text'))\r\n",
							"#display(df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Perform Topic Modeling\r\n",
							"## Pyspark ML Clustering LDA (Latent Dirichlet Allocation)\r\n",
							"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.LDA.html\r\n",
							"## 5 Topics"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.ml.feature import Tokenizer, CountVectorizer\r\n",
							"from pyspark.ml.clustering import LDA\r\n",
							"\r\n",
							"# tokenize text\r\n",
							"tokenizer = Tokenizer(inputCol='cleanText', outputCol='tokens')\r\n",
							"df_tokens = tokenizer.transform(df)\r\n",
							"\r\n",
							"# create document-term matrix\r\n",
							"vectorizer = CountVectorizer(inputCol='tokens', outputCol='features')\r\n",
							"model = vectorizer.fit(df_tokens)\r\n",
							"df_features = model.transform(df_tokens)\r\n",
							"\r\n",
							"# fit LDA model\r\n",
							"num_topics = 5\r\n",
							"max_iterations = 10\r\n",
							"lda = LDA(k=num_topics, maxIter=max_iterations)\r\n",
							"lda_model = lda.fit(df_features)\r\n",
							"\r\n",
							"# get topic distribution for each document\r\n",
							"df_topics = lda_model.transform(df_features).select('id', 'topicDistribution')\r\n",
							"\r\n",
							"# join with original DataFrame\r\n",
							"df_result = df.join(df_topics, 'id')\r\n",
							"\r\n",
							"#display(df_result)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Parse topicDistribution output column for the topic group and confidence values"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"import numpy as np\r\n",
							"\r\n",
							"# Extract the 'replies' column and convert it to JSON\r\n",
							"json_output = df_result.select(\"topicDistribution\").toJSON().collect()\r\n",
							"\r\n",
							"# Deserialize the JSON and extract the 'replies' field\r\n",
							"topics = [json.loads(x)[\"topicDistribution\"] for x in json_output]\r\n",
							"\r\n",
							"topicgroup = []\r\n",
							"topic0confidence = []\r\n",
							"topic1confidence = []\r\n",
							"topic2confidence = []\r\n",
							"topic3confidence = []\r\n",
							"topic4confidence = []\r\n",
							"\r\n",
							"# Print the 'replies' field for each row\r\n",
							"for topic in topics:\r\n",
							"    #print(topic)\r\n",
							"    \r\n",
							"    # convert JSON array to numpy array\r\n",
							"    arr = np.array(topic['values'])\r\n",
							"    topic0confidence.append(arr[0])\r\n",
							"    topic1confidence.append(arr[1])\r\n",
							"    topic2confidence.append(arr[2])\r\n",
							"    topic3confidence.append(arr[3])\r\n",
							"    topic4confidence.append(arr[4])\r\n",
							"\r\n",
							"    # find index of maximum value\r\n",
							"    idx = np.argmax(arr)\r\n",
							"    #print(idx)\r\n",
							"    topicgroup.append(idx)\r\n",
							"\r\n",
							"#print(topicgroup)\r\n",
							"#print(topic0confidence)\r\n",
							"#print(topic1confidence)\r\n",
							"#print(topic2confidence)\r\n",
							"#print(topic3confidence)\r\n",
							"#print(topic4confidence)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Collect the original dataframe column values"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# assume df is the DataFrame to extract values from\r\n",
							"id_values = []\r\n",
							"created_at_values = []\r\n",
							"text_values = []\r\n",
							"cleantext_values = []\r\n",
							"lang_values = []\r\n",
							"retweet_count_values = []\r\n",
							"reply_count_values = []\r\n",
							"like_count_values = []\r\n",
							"quote_count_values = []\r\n",
							"impression_count_values = []\r\n",
							"\r\n",
							"# collect all rows of the DataFrame\r\n",
							"rows = df.collect()\r\n",
							"\r\n",
							"# iterate over rows and extract values from each column\r\n",
							"for row in rows:\r\n",
							"    id_values.append(row[\"id\"])\r\n",
							"    created_at_values.append(row[\"created_at\"])\r\n",
							"    text_values.append(row[\"text\"])\r\n",
							"    lang_values.append(row[\"lang\"])\r\n",
							"    retweet_count_values.append(row[\"retweet_count\"])\r\n",
							"    reply_count_values.append(row[\"reply_count\"])\r\n",
							"    like_count_values.append(row[\"like_count\"])\r\n",
							"    quote_count_values.append(row[\"quote_count\"])\r\n",
							"    impression_count_values.append(row[\"impression_count\"])\r\n",
							"    cleantext_values.append(row['cleanText'])"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Convert new values to proper int of float data types"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert nump.int64 to int and numpy.float64 to float\r\n",
							"topicgroup = [int(i) for i in topicgroup]\r\n",
							"topic0confidence = [float(i) for i in topic0confidence]\r\n",
							"topic1confidence = [float(i) for i in topic1confidence]\r\n",
							"topic2confidence = [float(i) for i in topic2confidence]\r\n",
							"topic3confidence = [float(i) for i in topic3confidence]\r\n",
							"topic4confidence = [float(i) for i in topic4confidence]"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create a final output dataframe by mapping the values to their proper schema "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"id\",  LongType(), True),\r\n",
							"    StructField(\"created_at\", TimestampType(), True),\r\n",
							"    StructField(\"text\", StringType(), True),\r\n",
							"    StructField(\"cleantext\", StringType(), True),\r\n",
							"    StructField(\"lang\", StringType(), True),\r\n",
							"    StructField(\"retweet_count\", IntegerType(), True),\r\n",
							"    StructField(\"reply_count\", IntegerType(), True),\r\n",
							"    StructField(\"like_count\", IntegerType(), True),\r\n",
							"    StructField(\"quote_count\", IntegerType(), True),\r\n",
							"    StructField(\"impression_count\", IntegerType(), True),\r\n",
							"    StructField(\"topic\", ShortType(), True),\r\n",
							"    StructField(\"topic_0_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_1_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_2_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_3_confidence\", FloatType(), True),\r\n",
							"    StructField(\"topic_4_confidence\", FloatType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"# create rows\r\n",
							"rows = [Row(id=id_values[i],\r\n",
							"            created_at=created_at_values[i],\r\n",
							"            text=text_values[i],\r\n",
							"            cleanText = cleantext_values[i],\r\n",
							"            lang=lang_values[i],\r\n",
							"            retweet_count = retweet_count_values[i],\r\n",
							"            reply_count=reply_count_values[i],\r\n",
							"            like_count=like_count_values[i],\r\n",
							"            quote_count=quote_count_values[i],\r\n",
							"            impression_count = impression_count_values[i],\r\n",
							"            topic = topicgroup[i],\r\n",
							"            topic_0_confidence = topic0confidence[i],\r\n",
							"            topic_1_confidence = topic1confidence[i],\r\n",
							"            topic_2_confidence = topic2confidence[i],\r\n",
							"            topic_3_confidence = topic3confidence[i],\r\n",
							"            topic_4_confidence = topic4confidence[i])\r\n",
							"        for i in range(len(id_values))]\r\n",
							"\r\n",
							"# create a DataFrame\r\n",
							"final_topic_df = spark.createDataFrame(rows, schema)\r\n",
							"\r\n",
							"# show the DataFrame\r\n",
							"#display(final_topic_df)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write Spark Dataframe to Dedicated SQL Pool Table [Topic].[NATO_Topics]\r\n",
							"# First, create the table in the Dedicated SQL Pool 'SQLPoolTest' using the code below\r\n",
							"~~~\r\n",
							"CREATE TABLE [Topic].[NATO_Topics]\r\n",
							"( \r\n",
							"\t[id] bigint  NULL,\r\n",
							"\t[created_at] DATETIME2(7)  NULL,\r\n",
							"\t[text] NVARCHAR(4000)  NULL,\r\n",
							"\t[cleantext] NVARCHAR(4000)  NULL,\r\n",
							"\t[lang] nvarchar(10)  NULL,\r\n",
							"\t[retweet_count] INT  NULL,\r\n",
							"\t[reply_count] INT  NULL,\r\n",
							"\t[like_count] INT  NULL,\r\n",
							"\t[quote_count] INT  NULL,\r\n",
							"\t[impression_count] INT  NULL,\r\n",
							"    [topic] SMALLINT NULL,\r\n",
							"    [topic_0_confidence] REAL NULL,\r\n",
							"    [topic_1_confidence] REAL NULL,\r\n",
							"    [topic_2_confidence] REAL NULL,\r\n",
							"    [topic_3_confidence] REAL NULL,\r\n",
							"    [topic_4_confidence] REAL NULL\r\n",
							")\r\n",
							"\r\n",
							"GO"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write using AAD Auth to internal table\r\n",
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"\r\n",
							"# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
							"# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
							"(final_topic_df.write\r\n",
							" # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							" # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							" .option(Constants.SERVER, \"pendragon-synapse.sql.azuresynapse.net\")\r\n",
							" # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
							" .option(Constants.TEMP_FOLDER, \"abfss://pendragon@pendragon.dfs.core.windows.net/NotebookStaging\")\r\n",
							" # Choose a save mode that is apt for your use case.\r\n",
							" # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
							" # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
							" .mode(\"overwrite\")\r\n",
							" # Required parameter - Three-part table name to which data will be written\r\n",
							" .synapsesql(\"SQLPoolTest.Topic.NATO_Topics\"))"
						],
						"outputs": [],
						"execution_count": 17
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLPoolTest')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkPoolTest')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Large",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}